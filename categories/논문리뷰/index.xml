<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>논문리뷰 on 이경민</title><link>https://gyeongmin.kr/categories/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/</link><description>Recent content in 논문리뷰 on 이경민</description><generator>Hugo -- gohugo.io</generator><language>ko</language><copyright>Gyeongmin Lee</copyright><lastBuildDate>Wed, 27 Dec 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://gyeongmin.kr/categories/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/index.xml" rel="self" type="application/rss+xml"/><item><title>[논문리뷰] PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation</title><link>https://gyeongmin.kr/p/paper-review-panda/</link><pubDate>Wed, 27 Dec 2023 00:00:00 +0000</pubDate><guid>https://gyeongmin.kr/p/paper-review-panda/</guid><description>&lt;img src="https://gyeongmin.kr/images/paper-review.png" alt="Featured image of post [논문리뷰] PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation" />&lt;h1 id="panda-adapting-pretrained-features-for-anomaly-detection-and-segmentation">PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation&lt;/h1>
&lt;blockquote>
&lt;p>&lt;a class="link" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Reiss_PANDA_Adapting_Pretrained_Features_for_Anomaly_Detection_and_Segmentation_CVPR_2021_paper.pdf" target="_blank" rel="noopener"
>논문 PDF&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;h3 id="anomaly-detection">Anomaly Detection&lt;/h3>
&lt;p>&lt;strong>Anomaly&lt;/strong>는 정상의 범주에서 벗어나 있는 모든 것들을 의미한다.&lt;/p>
&lt;p>주어진 데이터셋에서 Anomaly들을 탐지하는 것을 &lt;strong>Anomaly Detection&lt;/strong> (이상치 탐지)라 한다. 주로 정상 데이터셋만으로 학습을 진행하며, 주어진 이미지를 Normal / Anomaly로 구분해야 하기에 One-Class Classification (OCC) 라고도 부른다.&lt;/p>
&lt;p>본 논문에서는 Anomaly를 검출하는 세 분야를 다음과 같이 정의한다.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Anomaly Detection&lt;/strong>: 훈련 과정에서 오직 정상 이미지만 사용하는 경우&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Anomaly Segmentation&lt;/strong>: 정상 이미지가 주어진 상태에서 Anomaly가 포함된 모든 픽셀을 탐지하는 경우&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Outlier Exposure&lt;/strong>: 이상 현상을 시뮬레이션하는 데에 외부 데이터셋 사용이 가능한 경우&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>최근 대부분의 이상 탐지 방법들은 제한된 정상 훈련 데이터셋을 사용하여 데이터의 feature를 학습하며, 이것에 의존하고 있다.&lt;/p>
&lt;h3 id="catastrophic-collapse">Catastrophic Collapse&lt;/h3>
&lt;p>특히 Multi-Class Calssifiaction보다 One-Class Classification 분야에서 전이 학습에 대한 연구가 부족한 상황이다. 이미지 전이 학습 분야에서는 &lt;strong>Catastrophic Collapse&lt;/strong>가 잘 발생하기 때문이다. Catastrophic Collapse는 Normal과 Anomaly 데이터가 잘 구분되지 않고, feature 공간에서 동일한 지점에 매핑되는 현상을 말한다. 이 현상은 데이터의 패턴을 학습하지 못하거나 학습 과정에서 Overfitting에 빠지는 것에 기인하기 때문에, Early Stopping을 위해 Epoch를 잘 조절한다면 이를 방지할 수 있다.&lt;/p>
&lt;p>본 논문에서는 Catastrophic Collapse를 방지하기 위해 Epoch 수에 의존적이지 않은 &lt;strong>early stopping variant&lt;/strong> 방법과 continual learning에서 영감을 받은 &lt;strong>Elastic Regularization&lt;/strong> 기법을 제안하는데, 이는 뒤에서 자세히 설명하겠다.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;h3 id="3-stage-framework">3-Stage Framework&lt;/h3>
&lt;p>일반적인 프레임워크는 세 단계로 구성된다. 정상 훈련 데이터셋 $D_{train} = \{ x_1, x_2, \dots, x_N \} $이 주어졌다고 가정하자.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Initial feature extractor&lt;/strong>&lt;/p>
&lt;p>Initial feature extractor인 $\psi_0$는 pre-train을 통해 얻을 수 있으며, 손실 함수는 $L_{pretrain}$이다. auxiliary task (보조 작업)은 외부 데이터셋을 통한 pre-train이나 self-supervised learning일 수 있다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Feature Adaptation&lt;/strong>&lt;/p>
&lt;p>auxiliary task나 데이터셋을 통해 학습된 feature는 anomaly score를 매기기 전에 적응이 필요할 수 있다. 이는 훈련 데이터셋을 통한 fine-tuning으로, 적응된 feature extractor는 $\psi$로 표기한다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Anomaly Scoring&lt;/strong>&lt;/p>
&lt;p>Feature를 적응시킨 후, 훈련 데이터셋 샘플들의 특성 $\psi(x_1), \psi(x_2), \dots, \psi(x_N)$ 을 추출한다. 그 다음, Anomaly score를 구하는 함수를 학습하는 과정을 진행한다. 일반적으로 scoring function은 테스트 샘플 $\psi(x)$ 주변의 정상 데이터 밀도를 측정하며 밀도가 낮은 지역에 높은 anomaly score를 할당한다.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="deep-nearest-neighbours-dn2">Deep Nearest Neighbours (DN2)&lt;/h3>
&lt;p>&lt;a class="link" href="https://arxiv.org/pdf/2002.10445.pdf" target="_blank" rel="noopener"
>&amp;ldquo;Deep Nearest Neighbor Anomaly Detection&amp;rdquo;&lt;/a>에서 제안된 방법으로, &lt;strong>DN2&lt;/strong>에서는 ImageNet 데이터셋에서 pre-trained된 &lt;strong>ResNet&lt;/strong>을 사용해 feature를 추출하고, &lt;strong>kNN&lt;/strong>을 적용해 Normal과 Anomaly를 구분한다. (Normal과 Anomaly 이미지 간의 평균 거리를 Anomaly Score로 사용한다)&lt;/p>
&lt;h3 id="semantic-pyramid-anomaly-detection-spade">Semantic Pyramid Anomaly Detection (SPADE)&lt;/h3>
&lt;p>&lt;a class="link" href="https://arxiv.org/pdf/2005.02357.pdf" target="_blank" rel="noopener"
>&amp;ldquo;Sub-Image Anomaly Detection with Deep Pyramid Correspondences&amp;rdquo;&lt;/a>에서 제안된 방법으로, ImageNet으로 사전 훈련된 ResNet을 사용하여 모든 이미지에 대한 픽셀별 특성을 추출한다. 또한 Feature Pyramid를 이용하여 다양한 수준의 feature를 동시에 추출하고 concat해 사용한다. DN2와는 달리 SPADE에서는 Anomaly Segmentation도 가능하다.&lt;/p>
&lt;h3 id="deep-support-vector-data-description-deepsvdd">Deep Support Vector Data Description (DeepSVDD)&lt;/h3>
&lt;p>&lt;a class="link" href="https://proceedings.mlr.press/v80/ruff18a/ruff18a.pdf" target="_blank" rel="noopener"
>&amp;ldquo;Deep One-Class Classification&amp;rdquo;&lt;/a>에서 제안된 방법으로, 데이터의 정상적인 패턴을 학습하여 정상 범주에서 크게 벗어난 데이터 포인트를 Anomaly로 식별하는 것이다. CNN을 사용해 이미지 데이터를 저차원의 feature 공간으로 매핑하고, 데이터 포인트가 중심에서 얼마나 떨어져 있는지를 측정하여 Anomaly 여부를 판단한다.&lt;/p>
&lt;h3 id="joint-optimization-jo">Joint Optimization (JO)&lt;/h3>
&lt;p>&lt;a class="link" href="https://arxiv.org/pdf/1801.05365.pdf" target="_blank" rel="noopener"
>&amp;ldquo;Learning Deep Features for One-Class Classification&amp;rdquo;&lt;/a>에서 제안된 방법으로, ImageNet 데이터셋에서 객체 분류를 위해 Pretrained된 Feature Extractor를 사용한다. 오분류에 대한 패널티가 없는 경우 trivial한 솔루션을 학습할 수 있기에, compactness loss와 classification loss를 동시에 최적화한다. 이 방법은 메모리를 많이 요구하며, 두 작업을 함께 훈련하는 경우 Anomaly Detection 작업의 정확도가 떨어질 수 있다는 한계점을 가진다.&lt;/p>
&lt;h2 id="method">Method&lt;/h2>
&lt;p>SVDD와 JO와 유사하게 compactness loss를 사용하여 pre-trained된 feature의 분포를 anomaly detection 작업에 적응시킨다. 하지만 구조를 제한하거나 외부 데이터를 사용하지 않고, 직접적으로 Catastrophic Collapse를 다룬다.&lt;/p>
&lt;p>compactness loss의 최적 솔루션이 Collapse로 이어질 수 있다. Collapse가 일어나면 모든 입력을 같은 지점으로 매핑하게 되어, 더이상 구분이 불가능해진다.&lt;/p>
&lt;p>본 논문에서는 Catastrophic Collapse를 방지하기 위해 아래 3가지 방법을 제시하였다.&lt;/p>
&lt;h3 id="simple-early-stopping-panda-early">Simple Early Stopping (PANDA-Early)&lt;/h3>
&lt;p>collapse가 일어나기 전에, &lt;strong>특정 Epoch마다 Early Stopping&lt;/strong>을 진행하는 방법이다. 가장 단순하며 강력한 방법이지만, Hyperparameter의 설정이 필요하다는 단점이 있다. 예를 들어, 15 Epoch마다 early stopping을 진행하는 것이다.&lt;/p>
&lt;h3 id="sample-wise-early-stopping-panda-ses">Sample-Wise Early Stopping (PANDA-SES)&lt;/h3>
&lt;p>&lt;strong>sample 단위로 early stopping&lt;/strong>여부를 결정하는 방법이다. Anomaly 샘플과 중심 간의 거리가 멀고, 정상 샘플과 중심 간의 거리가 짧을 때 anomaly detection의 정확도가 상관관계가 있다. 이를 위해 훈련 과정 중 특정 Epoch마다 (예를 들어 5 Epoch 마다) 평균 거리를 저장하고, 정규화를 거치고 평균 거리의 최대 비율을 anomaly detection score로 사용한다.&lt;/p>
&lt;h3 id="elastic-regularization-panda-ewc">Elastic Regularization (PANDA-EWC)&lt;/h3>
&lt;p>Continual Learning은 adaptive regularization을 이용하는 방법이다.&lt;/p>
&lt;p>continual learning에서 영감을 받아 Elastic Weight Consolidation(EWC)을 사용한 방법이다. Continual Learning이란 이전에 학습한 것을 까먹지 않고 새로운 것을 학습하는 것이다. 여기서 보조 작업에 대해 사전 훈련을 진행하기 위해 100개의 미니배치를 사용한다. 이 과정에서 신경망의 모든 가중치 파라미터에 대한 Fisher 정보 행렬 $F$의 대각선을 계산한다. 이는 사전 훈련 단계가 끝난 후 한 번만 수행한다. 각 가중치 파라미터의 Fisher 행렬 값은 사전 훈련 데이터셋을 통해 주어진 식으로 계산된다.&lt;/p>
&lt;p>연구팀은 Fisher 정보 행렬의 대각선 요소를 사용하여 네트워크의 각 가중치가 사전 훈련된 상태($\psi_0$)에서 미세 조정된 상태($\psi^*$)로 변화하는 거리의 제곱을 가중치로 삼는다. 이는 가중치 함수의 손실 풍경 곡률을 측정하는 방법으로 볼 수 있으며, 값이 크면 곡률이 높고 가중치가 비탄력적임을 의미한다.&lt;/p>
&lt;p>이러한 정규화 방법은 $\lambda$라는 하이퍼파라미터로 가중치가 부여된 compactness loss와 함께 사용된다. 본 연구에서는 $\lambda = 10^4$를 사용했다. 이를 통해 최종적인 손실 함수를 정의하고, 이는 사전 훈련된 가중치와 미세 조정된 가중치 간의 차이를 기반으로 한다. 이 접근법은 네트워크가 새로운 작업을 학습하면서도 이전에 학습한 작업에 대한 정보를 유지할 수 있도록 돕는다.&lt;/p>
&lt;h3 id="anomaly-scoring">Anomaly Scoring&lt;/h3>
&lt;p>전통적인 Anomaly Detection와 같이, Anomaly Score는 밀도 추정을 통해 구할 수 있다. 따라서 본 연구에서도 kNN을 사용해 구현한다.&lt;/p>
&lt;h3 id="outlier-exposure">Outlier Exposure&lt;/h3>
&lt;p>Outlier Exposure는 이미지 이상 탐지 작업을 확장한 것으로, Normal 데이터보다 Anomaly에 더 유사한 Auxiliary 데이터셋 $D_{OE}$가 있다고 가정하고, 데이터셋을 Normal과 Abnormal로 분류한다. 이는 Linear Classification Layer $w$와 Feature $\psi$를 이용해 Logistic Regression Loss를 계산하는 방식으로 이루어진다.&lt;/p>
&lt;h2 id="expreiments">Expreiments&lt;/h2>
&lt;h3 id="dataset">Dataset&lt;/h3>
&lt;p>&lt;img src="https://gyeongmin.kr/p/paper-review-panda/image-1.png"
width="828"
height="383"
srcset="https://gyeongmin.kr/p/paper-review-panda/image-1_hu8fe9d0bb7d2841f520322010f3770529_538886_480x0_resize_box_3.png 480w, https://gyeongmin.kr/p/paper-review-panda/image-1_hu8fe9d0bb7d2841f520322010f3770529_538886_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="왼쪽부터 시계 방향으로 CIFAR10, CIFAR100, Fashion MNIST, DogsVsCats, WBC, DIOR, Oxford Flowers, MVTec"
class="gallery-image"
data-flex-grow="216"
data-flex-basis="518px"
>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Dataset&lt;/th>
&lt;th>$N_{classes}$&lt;/th>
&lt;th>$N_{train}$&lt;/th>
&lt;th>$N_{test}$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>CIFAR10&lt;/td>
&lt;td>10&lt;/td>
&lt;td>5,000&lt;/td>
&lt;td>10,000&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Fashion MNIST&lt;/td>
&lt;td>10&lt;/td>
&lt;td>6,000&lt;/td>
&lt;td>10,000&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CIFAR100&lt;/td>
&lt;td>20&lt;/td>
&lt;td>2,500&lt;/td>
&lt;td>10,000&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Flowers&lt;/td>
&lt;td>102&lt;/td>
&lt;td>10&lt;/td>
&lt;td>7,169&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Birds&lt;/td>
&lt;td>200&lt;/td>
&lt;td>30&lt;/td>
&lt;td>5,794&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CatsVsDogs&lt;/td>
&lt;td>2&lt;/td>
&lt;td>10,000&lt;/td>
&lt;td>5,000&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MVTec&lt;/td>
&lt;td>15&lt;/td>
&lt;td>242&lt;/td>
&lt;td>1,725&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>WBC&lt;/td>
&lt;td>4&lt;/td>
&lt;td>59&lt;/td>
&lt;td>62&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DIOR&lt;/td>
&lt;td>19&lt;/td>
&lt;td>649&lt;/td>
&lt;td>9,243&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>여기서 MVTec을 제외한 데이터셋은 정상 데이터를 한 개의 클래스로 구성하고, 나머지 클래스는 비정상으로 구성한다. 예를 들어, 고양이 이미지는 정상이고 강아지나 토끼 등 다른 이미지는 전부 비정상으로 구성된다.&lt;/p>
&lt;p>MVTec 데이터셋은 Anomaly Detection을 위해 제작된 데이터셋으로, 클래스마다 정상과 비정상 이미지가 라벨링되어 있다.&lt;/p>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>&lt;img src="https://gyeongmin.kr/p/paper-review-panda/image-2.png"
width="1664"
height="435"
srcset="https://gyeongmin.kr/p/paper-review-panda/image-2_huf70542624251d7a6a11badc98e4b0885_133792_480x0_resize_box_3.png 480w, https://gyeongmin.kr/p/paper-review-panda/image-2_huf70542624251d7a6a11badc98e4b0885_133792_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="Anomaly detection performance (Average ROC AUC %)"
class="gallery-image"
data-flex-grow="382"
data-flex-basis="918px"
>&lt;/p>
&lt;p>&lt;img src="https://gyeongmin.kr/p/paper-review-panda/image-3.png"
width="1042"
height="385"
srcset="https://gyeongmin.kr/p/paper-review-panda/image-3_huc19b42b9821d23f121b64dda0f10e1cd_84052_480x0_resize_box_3.png 480w, https://gyeongmin.kr/p/paper-review-panda/image-3_huc19b42b9821d23f121b64dda0f10e1cd_84052_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="Pretrained feature performance on various small datasets (Average ROC AUC %)"
class="gallery-image"
data-flex-grow="270"
data-flex-basis="649px"
>&lt;/p>
&lt;p>위 두 결과를 보았을 때, Self-Supervised 방법보다 DN2와 PANDA 방법이 훨씬 높은 성능을 보이는 것을 확인할 수 있다.&lt;/p>
&lt;p>&lt;img src="https://gyeongmin.kr/p/paper-review-panda/image-4.png"
width="1085"
height="111"
srcset="https://gyeongmin.kr/p/paper-review-panda/image-4_hu4ce90baf9558792fb2b91e9a4c4c6d70_30823_480x0_resize_box_3.png 480w, https://gyeongmin.kr/p/paper-review-panda/image-4_hu4ce90baf9558792fb2b91e9a4c4c6d70_30823_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="Comparison of anomaly segmentation methods (pixel-level ROCAUC and PRO %)"
class="gallery-image"
data-flex-grow="977"
data-flex-basis="2345px"
>&lt;/p>
&lt;p>기존의 Anomaly Segmentation 방법들보다 SPADE 방법이 매우 높은 성능을 보이는 것을 확인할 수 있다.&lt;/p>
&lt;p>&lt;img src="https://gyeongmin.kr/p/paper-review-panda/image-5.png"
width="492"
height="228"
srcset="https://gyeongmin.kr/p/paper-review-panda/image-5_hud409dcc8912bd9d9524595ad3a9d5b84_40928_480x0_resize_box_3.png 480w, https://gyeongmin.kr/p/paper-review-panda/image-5_hud409dcc8912bd9d9524595ad3a9d5b84_40928_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="A comparison of different feature adaptation methods (Avg. ROC AUC %)"
class="gallery-image"
data-flex-grow="215"
data-flex-basis="517px"
>&lt;/p>
&lt;p>위 표를 보면, 모든 데이터셋에서 PANDA 방법들이 JO 방법보다 높은 성능을 보이는 것을 확인할 수 있다.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>본 논문에서는 Anomaly Detection과 Anomaly Aegmentation을 위한 간단한 baseline을 제안하였으며, 이 방법은 현재 SOTA 방법들을 능가하며, 한계점을 해결했다. 또한 pre-trained된 Feature을 적응시키고 Catastrophic Collapse를 완화하는 방법을 제안하였다.&lt;/p>
&lt;p>하지만 이 연구의 주요 한계점은 pre-trained된 강력한 Feature Extractor가 필요하다는 것이다.&lt;/p></description></item></channel></rss>