[{"content":"Visual Attention and Background Subtraction With Adaptive Weight for Hyperspectral Anomaly Detection 논문 링크\nAbstract 하이퍼스펙트럴 타겟 탐지에서 이상 탐지(AD)는 지상 물체 스펙트럼에 대한 사전 지식이 필요하지 않기 때문에 특히 중요하다. 그러나 하이퍼스펙트럴 이상 탐지에서는 하이퍼스펙트럴 이미지(HSI)의 중요한 특징을 활용하고 노이즈의 영향을 줄이는 것이 어려워 탐지 성능이 크게 제한된다. 본 논문에서는 시각적 주의 모델과 적응 가중치를 사용한 배경 제거를 통해 하이퍼스펙트럴 이상 탐지를 구현하는 전략을 제안한다. 밴드 선택 방법을 통해 가장 판별력이 높은 밴드를 선택하여 이후 처리의 입력 이미지로 사용한다. 그런 다음, 하이퍼스펙트럴 이상 탐지에 하이퍼스펙트럴 시각적 주의 모델을 최초로 도입하여 입력 이미지의 중요한 특징 맵을 추출한다. 더 나아가, 곡률 필터를 통해 중요한 특징 맵에서 배경과 노이즈를 줄일 수 있는 배경 제거 과정을 개발하여 초기 이상 영역 맵을 얻는다. 마지막으로, 스펙트럼 정보를 결합하여 적응 가중치 맵을 초기 이상 영역 맵에 적용하여 배경을 더욱 억제한다. 실험에서는 제안된 방법을 합성 및 실제 HSI에서의 최신 7가지 방법과 비교하였다. 가장 중요한 점은 실험 결과 제안된 방법이 효과적이며 대체 방법보다 더 나은 성능을 보인다는 것이다. 우리는 이 방법이 하이퍼스펙트럴 이상 탐지에 시각적 처리 방법의 새로운 길을 열 수 있을 것이라고 믿는다.\nIntroduction 이 논문은 3차원 Hyperspectral Image(HSI)를 활용한 anomaly detection(AD) 방법에 대해 설명하고 있다. HSI는 공간적 및 spectral 정보를 포함하고 있어 다양한 분야에서 널리 사용된다. 이 논문에서는 주로 unsupervised anomaly detection 방법을 다루고 있으며, 특히 Reed-Xiaoli (RX) 방법의 한계와 이를 개선하기 위한 여러 변형 기법을 논의하고 있다. RX 방법은 global 정보를 사용하기 때문에 anomaly와 noise에 취약하다는 문제가 있다.\n이를 해결하기 위해, 지역적 정보, sparse representation, low-rank matrix decomposition 등을 활용한 다양한 방법들이 제안되었다. 이러한 방법들은 noise와 anomaly의 영향을 줄이고, detection의 정확도를 높이기 위해 고안되었다.\n이 논문에서는 인간의 시각적 모델을 기반으로 한 새로운 HSI anomaly detection 방법을 제안하고 있다. 이 방법은 visual attention mechanism과 background subtraction을 결합하여 anomaly detection을 수행한다. 이 과정에서 선택된 spectral bands에 대해 visual attention mechanism을 적용하여 salient feature map을 추출하고, background subtraction과 adaptive weight map을 통해 최종 anomaly detection 결과를 도출한다.\n이 논문은 기존 방법들보다 뛰어난 성능을 보여주며, 특히 Visual Attention Mechanism(HVAM)을 anomaly detection에 도입하여 그 효과를 입증하고 있다. 주요 기여로는 HVAM의 도입, curvature filter를 이용한 background subtraction 방법, 그리고 spectral 정보를 활용한 adaptive weight의 적용이 있다.\nProposed Method 이 논문에서는 Hyperspectral Image(HSI)에서 anomaly detection(AD)을 위한 새로운 방법을 제안하고 있다. 이 방법은 Visual Attention Model(VAM)과 Background Subtraction을 결합하고, Adaptive Weight을 사용하는 접근법을 채택하고 있다.\n차원 축소: Optimal Clustering Framework(OCF) band selection 방법을 사용하여 HSI의 차원을 축소하고, 가장 대표적인 밴드를 후속 처리의 입력 이미지로 사용한다.\nSalient Feature Map 추출: HSI에서 anomaly target 영역을 포함하는 salient feature map을 추출하기 위해 HVAM(Hyperspectral Visual Attention Model)을 처음으로 도입하였다.\nBackground Subtraction: TVCF(Total Variation Curvature Filter)를 이용하여 Salient Feature Map에서 초기 anomaly 영역 지도를 추출하고, 동시에 background와 noise를 억제한다.\nAdaptive Weight Map: 원래 HSI의 spectral angular distance를 활용하여 adaptive weight map을 계산한 후, 이를 초기 anomaly 영역 지도와 결합하여 최종 anomaly detection 결과를 도출한다.\n이 방법은 기존의 여러 첨단 방법들과 비교했을 때 뛰어난 성능을 보였으며, 특히 background 억제와 noise 제거에 효과적이다.\nExperiments Results 이 논문에서는 제안된 Visual Attention and Background Subtraction with Adaptive Weight (VABS) 방법의 성능을 검증하기 위해 다양한 실험을 수행하였다. 실험에는 합성 데이터셋과 두 개의 실제 Hyperspectral Image(HSI) 데이터셋이 사용되었으며, 제안된 방법을 기존의 7가지 최신 anomaly detection(AD) 방법과 비교하였다.\n합성 데이터셋: RX, LRX, CBAD 등의 방법은 타겟 탐지에 실패하거나 일부만 탐지했다. 반면에 VABS 방법은 배경을 억제하고 타겟을 성공적으로 탐지했다. 제안된 방법은 다른 방법들보다 높은 ROC 곡선과 최대 AUC 값을 기록하며 우수한 성능을 보였다.\nSan Diego 데이터셋: 두 개의 실험 장면에서 VABS 방법은 배경을 잘 억제하고 타겟을 성공적으로 분리했다. 특히, San Diego scene-1과 San Diego scene-2에서 AUC 값이 다른 방법들보다 높았다.\nAirport 데이터셋: RX와 CBAD 방법은 타겟을 명확히 탐지하지 못했으며, 다른 방법들은 배경에 영향을 받았다. 반면에 VABS 방법은 타겟을 더 명확히 탐지하였으며, 다른 방법들보다 높은 AUC 값을 기록했다.\n또한, VABS 방법은 Gaussian noise, salt and pepper noise, 그리고 Poisson noise와 같은 노이즈 억제 능력을 검증하기 위해 추가 실험을 수행했으며, 노이즈 추가 후에도 여전히 만족스러운 탐지 성능을 보였다. 마지막으로, initial anomaly area map과 weight map의 결합을 통해 최종 탐지 결과의 성능이 향상됨을 확인하였다.\n이 결과들은 제안된 VABS 방법이 기존 방법들에 비해 우수한 성능을 보임을 입증하며, 특히 배경 억제와 노이즈 억제 측면에서 뛰어난 성능을 나타낸다.\nConclusion 이 논문에서는 새로운 hyperspectral anomaly detection(AD) 방법을 제안하고 있다. 제안된 방법은 OCF band selection을 통해 차원 축소를 수행하고, HVAM을 도입해 anomaly 영역을 포함한 salient feature map을 추출한다. 이후 TVCF를 활용한 background subtraction을 통해 초기 anomaly 영역을 식별하고, spectral angular distance를 이용해 adaptive weight map을 계산하여 최종 anomaly detection 결과를 얻는다. 실험 결과, 제안된 방법은 기존의 방법들에 비해 우수한 성능을 보였다. 다만, anomaly pixels이 subpixels인 경우는 고려되지 않았으며, 향후 연구에서 이 문제와 adaptive 파라미터 설정을 다룰 예정이다.\n","date":"2024-07-27T00:00:00Z","image":"https://gyeongmin.kr/images/paper-review.png","permalink":"https://gyeongmin.kr/p/paper-review-visual-attention-ad/","title":"[논문리뷰] Visual Attention and Background Subtraction With Adaptive Weight for Hyperspectral Anomaly Detection"},{"content":"Hyperspectral Anomaly Detection via Background and Potential Anomaly Dictionaries Construction 논문 링크\nAbstract 이 논문에서는 배경 사전과 잠재적 이상치 사전이라는 두 가지 잘 설계된 사전을 기반으로 한 새로운 초분광 이미지 이상 탐지 방법을 제안한다. 이상치를 효과적으로 탐지하고 노이즈의 영향을 제거하기 위해, 원본 이미지를 배경, 이상치, 노이즈의 세 가지 구성 요소로 분해한다. 이와 같이, 이상 탐지 작업을 행렬 분해 문제로 간주한다. 배경의 동질성과 이상치의 희소성을 고려하여, 저랭크 및 희소 제약을 모델에 적용한다. 그런 다음, 배경 사전과 잠재적 이상치 사전은 배경 및 이상치의 사전 지식을 사용하여 구성한다. 배경 사전의 경우, 과완비 사전에서 자주 사용되는 원자가 배경이 될 가능성이 높다고 가정하여, 공동 희소 표현(Joint Sparse Representation, JSR) 기반의 사전 선택 전략을 제안한다. 장면에 숨겨진 이상치의 사전 정보를 최대한 활용하기 위해 잠재적 이상치 사전을 구성한다. JSR 모델 내의 지역 영역에서 계산된 잔차를 사용하여 픽셀의 이상 수준이라는 기준을 정의한다. 그런 다음, 노이즈와 배경의 영향을 완화하기 위해 가중치 항과 결합한다. 실험 결과, 잠재적 이상치 및 배경 사전 구축에 기반한 제안된 이상 탐지 방법이 최신 방법들과 비교하여 우수한 성능을 달성할 수 있음을 보여준다.\nIntroduction 초분광영상(Hyperspectral image, HSI)은 넓은 범위와 높은 해상도의 스펙트럼을 가지고 있는 이미지이다. HSI 기반의 Anomaly Detection은 다양한 분야에서 연구되어 왔다.\nSupervised 기반의 물체 검출과 달리 Anomaly Detection은 목표물에 대한 사전 정보를 알지 못한 상태에서 이루어지기 때문에, 기존 방법들은 배경 정보를 최대한 활용하려고 노력했다. local region의 중심에 있는 배경 픽셀은 다른 픽셀들의 조합으로 표현될 수 있지만 anomalous한 픽셀은 그렇지 않다는 가정을 바탕으로 JSR (Joint Sparse Representation)의 특성을 활용하는 AD 방법이 제안되었다.\nHSI는 일반적으로 유사한 스펙트럼 특성을 균질하게 가지고 있기 때문에, 이 구조는 subspace로 표현될 수 있다. 일반적으로 PCS나 RPCA가 사용되고, 더 나은 multisubspace 학습을 위해 LRR(Low-Rank Representation) 방법이 제안되었다. HSI 데이터의 경우, LRR 기법은 분류와 Denoising에 사용되었으며, 최근에는 Anomaly Detection Problem을 모델링하는 데에도 활용되었다. background는 low-rank를 가지고 있고, anomaly는 sparse한 속성을 가지고 있다고 가정하여 RPCA 모델을 확장하여 데이터를 background, anomaly, noise로 분해했고 mahalanobis 거리를 이용하여 AD를 진행했다.\n본 논문에서는 background와 anomaly information을 잘 모델링하기 위해, LRSR(Low-Rank and Sparse Representation)을 활용한 새로운 HSI Anomaly Detection을 제안한다. HSI의 homogeneity 때문에 background는 low-rank 속성을 가지고, anomaly는 sparsity한 속성을 가지도록 제약된다. JSR 모델은 clutter와 anomaly의 영향을 받지 않는 순수한 배경을 묘사하는 데에 사용된다.\nProposed Method Background, Anomaly, and Noise Decomposition Model Background를 $BZ$, Anomaly를 $A$, Noise를 $E$라고 했을 때, HSI 이미지 $X$는 아래와 같이 나타낼 수 있다.\n$$ X = BZ + A + E $$\nHSI에서 local region은 대부분 homogeneity하기 때문에 배경은 low-rank의 성질을 가진다. Anomaly는 sparse한 속성을 부여하여 모델링한다. Noise는 Sparse한 Noise와 Gaussian Random Noise 두 종류가 존재하는데, 이를 표현하기 위해 Noise를 모델링하는 데에 $l_{2,1}$ norm을 사용한다.\n최적화 문제는 다음과 같이 정의된다. $\\beta$와 $\\lambda$는 각 성분 간의 균형을 맞추기 위한 계수이다.\n$$ min_{Z,A,E} \\text{rank}(Z) + \\beta{\\Vert A \\Vert_l} + \\lambda{\\Vert E \\Vert _{2,1}} \\quad \\text{s.t} \\ X = BZ + A + E $$\nBackground Dictionary Construction local region 내의 픽셀들은 공통된 구조를 공유할 수 있으므로, 공동 희소 표현(JSR) 모델을 사용하여 지역 내의 픽셀들을 몇 가지 공통된 원자의 선형 결합으로 표현한다.\nregion을 클러스터링하여 비슷한 구조를 가진 픽셀 그룹으로 나누고, 각 클러스터에서 자주 사용되는 원자를 선택하여 Background Dictionary를 구축한다. $U$는 지역 내의 픽셀 행렬, $V$는 Overcomplete Dictoinary, $\\psi$는 표현 계수, $R$은 잔차(residual)다.\n$$ \\min |\\psi|_{\\text{row},0} \\quad \\text{s.t} \\ U = V\\psi + R $$\nBackground Dictionary의 원자는 JSR 모델에서 자주 사용되는 원자들로 선택되며, 선택 빈도는 정규화된 계수의 합으로 정의된다.\n$$ P_c = \\frac{1}{\\gamma} \\sum_{i=1}^{n_c} \\sum_{j=1}^{L} |\\psi_{i,j}| $$\nPotential Anomaly Dictionary Construction Potential Anomaly Dictionary는 JSR 모델을 사용하여 큰 잔차를 가지는 픽셀들을 선택하여 구성한다. 이 잔차가 큰 픽셀들은 이상일 가능성이 높다고 간주된다.\n각 픽셀의 AL (Anomalous Level)은 지역 기반의 잔차 평균을 계산하여 결정됩니다.\n$$ AL = \\frac{1}{\\chi} \\left[ R^1_{\\text{mean}}, R^2_{\\text{mean}}, \\dots, R^K_{\\text{mean}} \\right] $$\nAnomaly Weight는 이러한 이상 수준과 선택 빈도, 계수의 절대값을 고려하여 선택되며, 이는 다음과 같은 수식으로 표현된다. $F_c$는 선택된 원소의 빈도수를 반영한다.\n$$ AW_c = \\frac{P_c}{F_c} $$\nExperiments Results 제안된 PAB-DC 방법의 성능을 평가하기 위해 5개의 실제 hyperspectral 이미지 데이터 세트를 사용하여 실험을 수행하였다. 이 데이터 세트들은 각기 다른 특성을 가지며, 항공 이미지로 구성되어 있다. PAB-DC 방법은 Global-RX, Local-RX, CRD, LRSR 등 최신 방법들과 성능을 비교하였다. 실험 결과, PAB-DC 방법은 대부분의 데이터 세트에서 다른 방법들에 비해 우수한 성능을 보였으며, 이는 ROC 곡선과 AUC 값을 통해 정량적으로 입증되었다.\n또한, potential anomaly dictionary의 효과를 검증하기 위해 이 dictionary를 사용한 경우와 그렇지 않은 경우를 비교하였다. 그 결과, potential anomaly dictionary를 활용한 PAB-DC 방법이 더 높은 탐지 성능을 보였으며, 이는 이 dictionary가 이상 탐지에 중요한 역할을 한다는 것을 시사한다. 마지막으로, 매개변수 분석을 통해 윈도우 크기, 클러스터 수, $\\beta$와 $\\lambda$ 값, 배경 사전과 potential anomaly dictionary의 원자 수 등의 매개변수들이 탐지 성능에 미치는 영향을 평가하였다. 이러한 분석 결과, 제안된 PAB-DC 방법은 비교적 안정적인 성능을 보였으며, 다양한 매개변수 설정에서도 우수한 탐지 성능을 유지함을 확인하였다. 종합적으로, PAB-DC 방법이 기존의 방법들보다 hyperspectral 이미지에서의 이상 탐지 성능이 뛰어남을 실험을 통해 입증하였다.\nConclusion 이 논문에서는 LRSR 전략을 활용한 Background 및 potential anomaly dictionary 기반의 새로운 HSI 이상 탐지 방법(PAB-DC)을 제시하였다. PAB-DC 방법이 hyperspectral 이미지에서 anomaly detection을 효과적으로 수행할 수 있음을 강조하였다. 이 방법은 low-rank와 sparse representation(LRSR) 전략을 사용하여 데이터를 배경, anomaly, 그리고 noise로 분해하며, 배경 사전과 potential anomaly dictionary를 활용하여 성능을 크게 향상시켰다. 기존의 방법들이 주로 배경 정보에만 의존한 것과 달리, PAB-DC는 anomaly 정보를 적극적으로 활용하여 다른 최신 방법들보다 뛰어난 성능을 보였다. 이를 통해 PAB-DC가 hyperspectral 이미지 anomaly detection에서 강력한 도구임을 입증하였다.\n","date":"2024-07-17T00:00:00Z","image":"https://gyeongmin.kr/images/paper-review.png","permalink":"https://gyeongmin.kr/p/paper-review-hyperspectral-ad/","title":"[논문리뷰] Hyperspectral Anomaly Detection via Background and Potential Anomaly Dictionaries Construction"},{"content":"\u003c!DOCTYPE html\u003e Bug Report Table Bug Report Example Preview Reset Fill Current Date/Time Copy Table to Clipboard Bug ID Overview Bug Name (Title) Bug Reporter Date/Time Submitted Prioritization Bug Severity Blocker Critical Major Normal Minor Trivial Bug Priority Top High Medium Low None Assigned to Details Steps to reproduce Expected Result Observed Result What actions is this bug blocking? Environment Platform OS Branch Commit # Note ","date":"2024-07-01T00:00:00Z","permalink":"https://gyeongmin.kr/p/bug-report/","title":"[Util] Bug Report Generator"},{"content":"ordered_set이란? g++에서 추가된 자료구조로, std::set과 유사하지만, 아래 두 연산을 $\\mathcal{O}(\\log N)$만에 수행할 수 있다.\norder_of_key(k) : k보다 작은 원소의 개수\nfind_by_order(k) : 오름차순으로 정렬했을 때 k번째 원소 (zero-based)\n집합 내부에서 원소의 추가/삭제가 빈번하게 이루어지며, 수시로 k번째 수를 찾아야하는 경우에 유용한 자료구조이다.\n1 2 3 4 5 6 7 #include \u0026lt;ext/pb_ds/assoc_container.hpp\u0026gt; #include \u0026lt;ext/pb_ds/tree_policy.hpp\u0026gt; using namespace std; using namespace __gnu_pbds; typedef tree\u0026lt;int, null_type, less\u0026lt;\u0026gt;, rb_tree_tag, tree_order_statistics_node_update\u0026gt; ordered_set; ordered_set os; 다만, 삭제하는 연산은 아래와 같이 따로 구현해주어야 한다.\n1 2 3 4 5 void erase(ordered_set \u0026amp;os, int value) { size_t index = os.order_of_key(value); auto iter = os.find_by_order(index); os.erase(iter); } ordered_set 사용 예제 order_of_key의 경우 value를 리턴하고, find_by_order의 경우 iterator를 리턴한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 #include \u0026lt;iostream\u0026gt; #include \u0026lt;ext/pb_ds/assoc_container.hpp\u0026gt; #include \u0026lt;ext/pb_ds/tree_policy.hpp\u0026gt; using namespace std; using namespace __gnu_pbds; typedef tree\u0026lt;int, null_type, less\u0026lt;\u0026gt;, rb_tree_tag, tree_order_statistics_node_update\u0026gt; ordered_set; void erase(ordered_set \u0026amp;os, int value) { size_t index = os.order_of_key(value); auto iter = os.find_by_order(index); os.erase(iter); } int main() { ordered_set os; os.insert(5); os.insert(4); os.insert(3); os.insert(3); os.insert(2); os.insert(1); for (int i : os) cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#39; \u0026#39;; // 1 2 3 4 5 cout \u0026lt;\u0026lt; os.order_of_key(2) \u0026lt;\u0026lt; endl; // 1 cout \u0026lt;\u0026lt; *os.find_by_order(2) \u0026lt;\u0026lt; endl; // 3 cout \u0026lt;\u0026lt; os.size() \u0026lt;\u0026lt; endl; // 5 erase(os, 2); cout \u0026lt;\u0026lt; os.size() \u0026lt;\u0026lt; endl; // 4 for (int i : os) cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#39; \u0026#39;; // 1 3 4 5 cout \u0026lt;\u0026lt; os.order_of_key(2) \u0026lt;\u0026lt; endl; // 1 cout \u0026lt;\u0026lt; *os.find_by_order(2) \u0026lt;\u0026lt; endl; // 4 } ordered_multi_set 사용 예제 중복 값을 허용한다.\nless\u0026lt;\u0026gt; 대신 less_equal\u0026lt;\u0026gt;를 넣어주면 된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 #include \u0026lt;iostream\u0026gt; #include \u0026lt;ext/pb_ds/assoc_container.hpp\u0026gt; #include \u0026lt;ext/pb_ds/tree_policy.hpp\u0026gt; using namespace std; using namespace __gnu_pbds; typedef tree\u0026lt;int, null_type, less_equal\u0026lt;\u0026gt;, rb_tree_tag, tree_order_statistics_node_update\u0026gt; ordered_multi_set; void erase(ordered_multi_set \u0026amp;oms, int value) { size_t index = oms.order_of_key(value); auto iter = oms.find_by_order(index); oms.erase(iter); } int main() { ordered_multi_set oms; oms.insert(5); oms.insert(4); oms.insert(3); oms.insert(3); oms.insert(2); oms.insert(1); for (int i : oms) cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#39; \u0026#39;; // 1 2 3 3 4 5 cout \u0026lt;\u0026lt; oms.order_of_key(2) \u0026lt;\u0026lt; endl; // 1 cout \u0026lt;\u0026lt; *oms.find_by_order(2) \u0026lt;\u0026lt; endl; // 3 cout \u0026lt;\u0026lt; oms.size() \u0026lt;\u0026lt; endl; // 6 erase(oms, 2); cout \u0026lt;\u0026lt; oms.size() \u0026lt;\u0026lt; endl; // 5 for (int i : oms) cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#39; \u0026#39;; // 1 3 3 4 5 cout \u0026lt;\u0026lt; oms.order_of_key(2) \u0026lt;\u0026lt; endl; // 1 cout \u0026lt;\u0026lt; *oms.find_by_order(2) \u0026lt;\u0026lt; endl; // 3 } BOJ 1572 중앙값 https://www.acmicpc.net/problem/1572\n세그트리/이분탐색 문제이지만, PBDS를 사용하면 쉽게 풀 수 있다.\n문제 풀이 온도 값이 1초마다 추가되며, 최근 K초 까지 온도의 중앙값을 다 더한 값을 구하는 문제이다.\n매 초마다 ordered_multi_set에 값을 추가해 준다.\nk초 이전에는 값 입력만 받고, k초부터는 중앙값을 구해준다. find_by_order()는 zero-based임에 유의하자.\n우리는 최근 k초까지의 값에만 관심 있기 때문에, k초 이후에는 k초 이전의 값은 삭제해 준다.\n중복값이 발생하므로 less_equal\u0026lt;\u0026gt;를 사용해 주자. 정답은 $2^{63}-1$ 이하이므로, int범위를 초과한다.\n","date":"2024-04-10T00:00:00Z","image":"https://gyeongmin.kr/images/algorithm.png","permalink":"https://gyeongmin.kr/p/pbds/","title":"C++ STL PBDS (Policy based data structures)"},{"content":"Pre-trained Model 사전 학습된 모델(Pre-trained Model)이란 대규모 데이터세트로 학습된 딥러닝 모델로, 이미 학습이 완료된 모델을 의미한다. 모델 자체를 현재 시스템에 적용하거나 사전 학습된 임베딩 벡터를 이용해 모델을 구성할 수 있다. Pre-trained 모델을 사용하면 처음부터 훈련시키는 게 아니므로 학습에 필요한 시간이 대폭 감소하며, 이미 다양한 작업에서 검증된 모델이기 때문에 안정적이고 우수한 성능을 기대할 수 있다.\nBackbone 백본(Backbone)이란 입력 데이터에서 특징을 추출해 최종 분류기에 전달하는 딥러닝 모델이나 그 일부를 의미한다. 백본 네트워크는 입력 데이터에서 특징을 추출하므로 노이즈와 불필요한 특성을 제거하고 가장 중요한 특징을 추출할 수 있다. 이렇게 추출된 특징을 활용해 새로운 모델이나 기능의 입력으로 사용한다.\n포즈 추정 모델이나 이미지 분할 모델을 만들 땐, 객체를 검출하는 컨볼루젼 신경망의 특징값을 가져와 최종 레이어를 바꾸는 등의 방식으로 적용할 수 있다.\n백본을 쓴다고 성능이 급격하게 좋아지지는 않으며, 사전 학습된 백본은 쉽게 오버피팅될 수 있다. 미세 조정이나 전이 학습을 적용해 오버피팅을 피해야 한다.\nTransfer Learning 전이 학습(Transfer Learning)이란 어떤 작업을 수행하기 위해 이미 사전 학습된 모델을 재사용해 새로운 작업이나 관련 도메인의 성능을 향상시킬 수 있는 기술을 의미한다. 전이 학습은 대규모 데이터세트에서 사전 학습된 모델을 다른 작은 데이터세트로 미세 조정해 활용한다.\n소스 도메인에서 학습한 지식을 재사용함으로써 전이 학습된 모델이 더 적은 데이터와 학습 시간으로 더 높은 성능을 낼 수 있다. 또한 대규모 데이터세트에서 사전 학습된 모델을 활용하므로 과대적합 문제를 최소화할 수 있다.\nUpstream과 Downstream 전이 학습의 구조는 업스트림(Upstream) 모델과 다운스트림(Downstream) 모델로 나뉜다.\n업스트림 모델: 대규모 특정 도메인의 데이터세트에서 학습한 모델이다. 이 모델은 기본적인 특징을 학습하며, 새로운 작업에 필요한 지식을 제공한다. 다운스트림 모델: 업스트림 모델에서 학습한 지식을 활용하여 새로운 작업이나 도메인의 데이터세트에서 학습하는 모델이다. 다운스트림 모델은 소규모 데이터세트에서 모델의 성능을 높이기 위해 미세 조정된다. Inductive Transfer Learning 귀납적 전이 학습(Inductive Transfer Learning)은 기존의 모델이 학습한 지식을 새로운 작업에 적용하여 성능을 개선하는 방법이다.\n자기주도적 학습: 레이블이 없는 대규모 데이터에서 특징을 학습하고, 소량의 레이블 데이터를 이용해 미세 조정을 진행한다. 다중 작업 학습: 소스 도메인과 타깃 도메인의 데이터를 기반으로 여러 작업을 동시에 학습하는 방법이다. 이 방식은 작업 간 상호작용을 통해 모델의 일반화를 돕는다. Transductive Transfer Learning 변환적 전이 학습(Transductive Transfer Learning)은 소스 도메인과 타깃 도메인이 유사하지만 완전히 동일하지 않은 경우에 사용된다.\n도메인 적응: 두 도메인의 특징 분포 차이를 줄이는 방식으로 모델을 학습한다. 표본 선택 편향/공변량 이동: 소스와 타깃 도메인의 데이터 분포 차이를 조정하여 전이를 진행한다. Unsupervised Transfer Learning 비지도 전이 학습(Unsupervised Transfer Learning)은 소스와 타깃 도메인 모두에서 레이블이 없는 데이터를 사용하는 방법이다. GAN이나 군집화(Clustering) 기법을 통해 타깃 도메인에서의 성능을 개선한다.\nZero-shot Transfer Learning 제로-샷 전이 학습은 사전 학습된 모델을 새로운 도메인에서도 바로 사용할 수 있도록 설계하는 방법이다. 학습하지 않은 데이터에도 일반화된 성능을 발휘하며, 데이터가 부족한 상황에서 유용하다.\nOne-shot Transfer Learning 원-샷 전이 학습은 클래스당 하나의 샘플만으로 모델을 학습하여 새로운 데이터를 분류하는 기법이다. 서포트 셋(Support Set)과 쿼리 셋(Query Set)을 활용해 분류를 진행하며, 적은 데이터로 높은 정확도를 달성할 수 있다.\nFeature Extraction 특징 추출(Feature Extraction)은 전이 학습에서 사전 학습된 모델의 계층을 활용하여 타깃 도메인의 데이터를 처리하는 방식이다. 이 방법은 소스 도메인과 타깃 도메인이 유사한 경우에 주로 사용된다.\n특징 추출 과정에서 사전 학습된 모델의 합성곱 계층(Convolutional Layers)과 같은 주요 계층은 동결(Freeze) 하여 학습하지 않는다. 대신, 이 계층에서 추출한 특징들을 기반으로 분류기(Classifier)만 재구성하고 학습한다.\nFine-tuning 미세 조정은 사전 학습된 모델의 일부 계층 또는 전체 계층을 타깃 도메인의 데이터에 맞게 학습시키는 방식이다. 이 방법은 소스 도메인과 타깃 도메인이 유사하지 않거나, 타깃 도메인의 데이터세트가 충분히 크지 않은 경우에 사용된다.\n미세 조정에서는 특정 계층을 선택적으로 동결하거나, 동결을 해제하여 학습을 진행한다. 예를 들어, 하위 계층은 일반적으로 저수준 특징(예: 선, 모서리)을 학습하므로 그대로 사용하고, 상위 계층만 재학습하는 경우가 많다. 반면, 소스와 타깃 도메인의 차이가 크다면 전체 계층을 학습시키는 경우도 있다.\n","date":"2024-03-10T00:00:00Z","image":"https://gyeongmin.kr/images/pytorch-transformer-nlp-computer-vision.png","permalink":"https://gyeongmin.kr/p/pre-trained-model/","title":"Pre-trained 모델"},{"content":"데이터 증강 데이터 증강(Data Augmentation)이란 데이터가 가진 고유한 특징을 유지한 채 변형하거나 노이즈를 추가해 데이터세트의 크기를 늘리는 방법이다. 데이터 증강은 모델의 과대적합을 줄이고 일반화 능력을 향상시킬 수 있다.\n너무 많은 변형이나 노이즈를 추가한다면 기존 데이터가 가진 특징이 파괴될 수 있으므로 주의해야 한다.\n텍스트 데이터 삽입 및 삭제 삽입은 의미 없는 문자나 단어, 또는 문장 의미에 영향을 끼치지 않는 수식어 등을 추가하는 방법이다. 임의의 단어나 문자를 기존 텍스트에 덧붙여 사용한다. 삭제는 삽입과 반대로 임의의 단어나 문자를 삭 제해 데이터의 특징을 유지하는 방법이다.\nContextualWordEmbsAug 클래스는 BERT 모델을 활용해 단어를 삽입하는 기능을 제공한다. action으로는 insert, substitute, swap, delete가 가능하다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import nlpaug.augmenter.word as naw texts = [ \u0026#34;Those who can imagine anything, can create the impossible.\u0026#34;, \u0026#34;We can only see a short distance ahead, but we can see plenty there that needs to be done.\u0026#34;, \u0026#34;If a machine is expected to be infallible, it cannot also be intelligent.\u0026#34;, ] aug = naw.ContextualWordEmbsAug(model_path=\u0026#34;bert-base-uncased\u0026#34;, action=\u0026#34;insert\u0026#34;) augmented_texts = aug.augment(texts) for text, augmented in zip(texts, augmented_texts): print(f\u0026#34;src : {text}\u0026#34;) print(f\u0026#34;dst : {augmented}\u0026#34;) print(\u0026#34;------------------\u0026#34;) 1 2 3 4 5 6 7 8 src: Those who can imagine anything, can create the impossible. dst: those scientists who can simply imagine seemingly anything, can create precisely the impossible. ------------------ src : We can only see a short distance ahead, but we can see plenty there that needs to be done. dst : we probably can still only see a short distance ahead, but we can nonetheless see about plenty from there that just needs to be properly done. ------------------ src : If a machine is expected to be infallible, it cannot also be intelligent. dst : if a logic machine is expected either to necessarily be infallible, subsequently it cannot also be highly intelligent. 교체 및 대체 교체는 단어나 문자의 위치를 교환하는 방법이다. ‘문제점을 찾지 말고 해결책을 찾으라’라는 문장에서 교체를 적용한다면 \u0026lsquo;해결책을 찾으라 문제점을 찾지 말고\u0026rsquo;로 변경될 수 있다. 교체는 무의미하거나 의미상 잘못된 문장을 생성할 수 있으므로 데이터의 특성에 따라 주의해 사용해야 한다.\n대체는 단어나 문자를 임의의 단어나 문자로 바꾸거나 동의어로 변경하는 방법을 의미한다. ‘사과’라는 단어를 ‘바나나’와 같이 유사한 단어로 변경하거나 ‘해’를 ‘태양으로 바꿔 뜻이 같은 말로 바꾸는 작업이 다. 단어나 문장을 대체하면 다른 증강 방법보다 비교적 데이터의 정합성(Consistency)이 어긋나지 않아 효율적으로 데이터를 증강할 수 있다. 하지만 조사를 바꿔주진 않는다.\nRandomWordAug 클래스를 통해 무작위로 단어를 교체할 수 있다. action으로는 insert, substitute, swap, delete, crop이 가능하다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import nlpaug.augmenter.word as naw texts = [ \u0026#34;Those who can imagine anything, can create the impossible.\u0026#34;, \u0026#34;We can only see a short distance ahead, but we can see plenty there that needs to be done.\u0026#34;, \u0026#34;If a machine is expected to be infallible, it cannot also be intelligent.\u0026#34;, ] aug = naw.RandomWordAug(action=\u0026#34;swap\u0026#34;) augmented_texts = aug.augment(texts) for text, augmented in zip(texts, augmented_texts): print(f\u0026#34;src : {text}\u0026#34;) print(f\u0026#34;dst : {augmented}\u0026#34;) print(\u0026#34;------------------\u0026#34;) 1 2 3 4 5 6 7 8 9 src : Those who can imagine anything, can create the impossible. dst : Those who can imagine can anything create, the. impossible ------------------ src : We can only see a short distance ahead, but we can see plenty there that needs to be done. dst : We see can only a short distance but ahead, can we see plenty that there needs to done be. ------------------ src : If a machine is expected to be infallible, it cannot also be intelligent. dst : A if is machine to expected be infallible, cannot also it be intelligent. ------------------ 모델을 활용해 대체하는 경우 ContextualWordEmbsAug 클래스를 사용하거나, SynonymAug 클래스로 워드넷(WordNet) 데이터베이스나 의역 데이터베이스(The Paraphrase Database, PPDB)를 활용해 단어를 대체해 데이터를 증강할 수도 있다.\n단어 집합을 미리 선언하고 그 중 하나로 대체하고 싶은 경우, ReservedAug를 사용할 수도 있다.\n역번역 역번역(Back-translation)이란 입력 텍스트를 특정 언어로 번역한 다음 다시 본래의 언어로 번역하는 방법을 의미한다. 예를 들어 영어를 한국어로 번역한 다음 번역된 텍스트를 다시 영어로 번역하는 과정을 의미한다. 원래의 언어로 번역하는 과정에서 원래 텍스트와 유사한 텍스트가 생성되므로 패러프레이징(Paraphrasing)21 효과를 얻을 수 있다.\n역번역은 번역 모델의 성능에 크게 좌우되기에, 모델의 성능을 평가하는 데 사용되기도 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import nlpaug.augmenter.word as naw texts = [ \u0026#34;Those who can imagine anything, can create the impossible.\u0026#34;, \u0026#34;We can only see a short distance ahead, but we can see plenty there that needs to be done.\u0026#34;, \u0026#34;If a machine is expected to be infallible, it cannot also be intelligent.\u0026#34;, ] back_translation = naw.BackTranslationAug( from_model_name=\u0026#34;facebook/wmt19-en-de\u0026#34;, to_model_name=\u0026#34;facebook/wmt19-de-en\u0026#34; ) augmented_texts = back_translation.augment(texts) for text, augmented in zip(texts, augmented_texts): print(f\u0026#34;src : {text}\u0026#34;) print(f\u0026#34;dst : {augmented}\u0026#34;) print(\u0026#34;------------------\u0026#34;) 1 2 3 4 5 6 7 8 9 src : Those who can imagine anything, can create the impossible. dst : Anyone who can imagine anything can achieve the impossible. ------------------ src : We can only see a short distance ahead, but we can see plenty there that needs to be done. dst : We can only look a little ahead, but we can see a lot there that needs to be done. ------------------ src : If a machine is expected to be infallible, it cannot also be intelligent. dst : If a machine is expected to be infallible, it cannot be intelligent. ------------------ 이미지 데이터 1 2 3 4 5 6 transform = transforms.Compose( [ transforms.Resize(size=(512, 512)), transforms.ToTensor() ] ) 이미지 데이터는 토치비전의 transforms 모듈을 이용하여 증강할 수 있다. 텐서화 클래스(transforms.ToTensor)는 PIL.Image 형식을 Tensor 형식으로 변환한다. 텐서화 클래스는 [0~255] 범위의 픽셀값을 [0.0~1.0] 사이의 값으로 최대 최소 정규화를 수행한다. 또한 입력 데이터의 형태를 [채널, 높이, 너비] 형태로 변환한다.\n회전 및 대칭 학습 이미지를 회전하거나 대칭한다면 변형된 이미지가 들어오더라도 더 강건한 모델을 구축할 수 있으며 일반화된 성능을 끌어낼 수 있다.\n1 2 3 4 5 6 7 transform = transforms.Compose( [ transforms.RandomRotation(degrees=30, expand=False, center=None), transforms.RandomHorizontalFlip(p=0.5), transforms.RandomVerticalFlip(p=0.5) ] ) 위 코드는 이미지를 ±30° 사이로 회전시키면서, 수평 대칭과 수직 대칭을 50% 확률로 적용하는 예제이다. expand=True이면 확장되어 여백이 생기지 않는다. 중심점을 입력하지 않으면 좌측 상단을 기준으로 회전한다.\n자르기 및 패딩 OD(Object Detection)과 같은 모델을 구성할 때, 학습 데이터의 크기가 일정하지 않거나 주요한 객체가 일부 영역에만 작게 존재할 수 있다. 이러한 경우 불필요한 부분을 자르거나, 패딩을 주어 크기를 맞출 수 있다.\n1 2 3 4 5 6 transform = transforms.Compose( [ transforms.RandomCrop(size=(512, 512)), transforms.Pad(padding=50, fill=(127, 127, 255), padding_mode=\u0026#34;constant\u0026#34;) ] ) padding_mode가 constant면 fill=(127, 127, 255)로 테두리가 생성된다. reflect나 symmetric이라면 입력한 RGB는 무시되며, 이미지의 픽셀값을 이용하여 생성한다. RandomCrop에도 자를 때 발생하는 여백 공간에 대한 패딩을 줄 수 있다.\n크기 조정 이미지 처리 모델 학습을 위해, 학습 데이터에 사용되는 이미지의 크기는 모두 일정해야 한다.\n1 2 3 4 5 transform = transforms.Compose( [ transforms.Resize(size=(512, 512)) ] ) size를 정수로 입력하는 경우, 높이나 너비 중 더 작은 값에 비율을 맞추어 크기가 수정된다.\n변형 아핀 변환(Affine Transformation)이나 원근(Perspective Transformation) 변환과 같은 기하학적 변환을 사용한다.\n1 2 3 4 5 6 7 8 transform = transforms.Compose( [ transforms.RandomAffine( degrees=15, translate=(0.2, 0.2), scale=(0.8, 1.2), shear=15 ) ] ) 아핀 변환은 각도(degrees), 이동(translate), 척도(scale), 전단(shear)을 입력해 이미지를 변형한다.\n색상 변환 이미지 데이터의 특징은 픽셀값의 분포나 패턴에 크게 좌우되는데, 앞선 변형들은 색상을 변경하진 않는다. 특정 색상에 편향되지 않도록 정규화하면 모델을 더 일반화시킬 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 transform = transforms.Compose( [ transforms.ColorJitter( brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3 ), transforms.ToTensor(), transforms.Normalize( mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225] ), transforms.ToPILImage() ] ) 노이즈 특정 픽셀값에 편향되지 않도록, 임의의 노이즈를 추가하는 것은 좋은 방법이다. 학습에 사용되지 않더라도, 테스트 데이터에 노이즈를 주어 Robustness를 평가하는 데 사용하기도 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class IaaTransforms: def __init__(self): self.seq = iaa.Sequential([ iaa.SaltAndPepper(p=(0.03, 0.07)), iaa.Rain(speed=(0.3, 0.7)) ]) def __call__(self, images): images = np.array(images) print(images.shape, images.dtype) augmented = self.seq.augment_image(images) return Image.fromarray(augmented) transform = transforms.Compose([ IaaTransforms() ]) 컷아웃 및 무작위 지우기 컷아웃은 임의의 ROI의 픽셀값을 0으로 채우는 것이고, 무작위 지우기는 랜덤 픽셀값으로 채우는 것이다.\n1 2 3 4 5 6 transform = transforms.Compose([ transforms.ToTensor(), transforms.RandomErasing(p=1.0, value=0), transforms.RandomErasing(p=1.0, value=\u0026#39;random\u0026#39;), transforms.ToPILImage() ]) 일부 영역이 누락되거나, 폐색 영역에 대해 모델을 더욱 견고하게 만들어준다.\n컷믹스 컷믹스(CutMix)는 이미지 패치 영역에 다른 이미지를 덮어씌우는 방법이다. 패치 위에 새로운 패치를 덮어씌워 자연스러운 이미지를 구성한다. 패치 영역의 크기와 비율을 고려해 덮어쓴다.\nLabel($y$)은 이미지가 얼마나 기여하였는지($\\lambda$)를 이용하여 아래 공식과 같이 계산된다.\n$$ \\tilde{y}=\\lambda y_a + (1-\\lambda)y_b $$\n","date":"2024-03-02T00:00:00Z","image":"https://gyeongmin.kr/images/pytorch-transformer-nlp-computer-vision.png","permalink":"https://gyeongmin.kr/p/data-augmentation/","title":"데이터 증강 및 변환"},{"content":"정칙화(Regularization) 머신러닝과 딥러닝 모델을 학습시킬 때, 오버피팅은 모델이 훈련 데이터에 지나치게 적합되어 새로운 데이터에 대한 예측 성능이 저하되는 오버피팅(overfitting) 문제는 흔히 접해보았을 것이다. 이러한 문제를 해결하기 위해 사용되는 기술이 바로 정칙화(Regularization)이다.\n정칙화는 모델이 암기(Memorization)가 아니라 일반화(Generalization)할 수 있도록 손실 함수에 규제(Penalty)를 가하는 방식이다.\n정칙화를 적용하면 학습 데이터들이 갖고 있는 작은 차이점에 대해 덜 민감해져 모델의 분산 값이 낮아진다. 그러므로 정칙화는 모델이 데이터를 학습할 때 의존하는 특징의 수를 줄임으로써 모델의 추론 능 력을 개선한다.\n오버피팅과 일반화 오버피팅은 모델이 훈련 데이터의 노이즈나 특정 패턴을 학습하여, 실제 데이터의 일반적인 패턴을 파악하지 못할 때 발생한다. 이로 인해 모델은 훈련 데이터에서는 높은 성능을 보이지만, 새로운 데이터에서는 성능이 급격히 떨어진다. 반면, 일반화는 모델이 새로운 데이터에서도 정확한 예측을 수행할 수 있는 능력을 의미한다.\n즉, 모델이 데이터의 일반적인 패턴을 학습하여 노이즈에 의존하지 않고, 다양한 데이터에 대해 일관된 성능을 유지하는 것이다.\n정칙화의 종류 L1 정칙화 L1 정칙화는 라쏘 정칙화(Lasso Regularization)라고도 불리며, 가중치의 절댓값 합을 손실 함수에 추가하여 오버피팅을 방지한다.\n이 방식은 모델이 불필요한 피처의 가중치를 0으로 수렴시키는 특징이 있어, 자동으로 특징 선택(feature selection)의 효과를 제공한다. 그러나 L1 정칙화는 하이퍼파라미터인 규제 강도(lambda)를 적절히 조절해야 하며, 과도한 규제는 정보의 손실을 초래할 수 있다. 주로 선형 회귀 모델에서 활용되며, 계산 복잡도가 다소 높을 수 있다는 단점이 있다.\n$$ \\text{Loss}\\text{L1} = \\text{Loss}{\\text{original}} + \\lambda \\sum_{i=1}^{n} |w_i| $$\n$ \\text{Loss}_{L1} $: L1 정칙화가 적용된 전체 손실 함수 $ \\text{Loss}_{\\text{original}} $: 원래의 손실 함수 (ex. 평균 제곱 오차) $ \\lambda $: 규제 강도 하이퍼파라미터 $ w_i $: 각 가중치 파라미터 $ n $: 가중치의 총 개수 L2 정칙화 L2 정칙화는 릿지 정칙화(Ridge Regularization)라고도 하며, 가중치 제곱의 합을 손실 함수에 추가하여 오버피팅을 방지한다.\nL2 정칙화는 가중치를 0에 가깝게 유지하므로, 모델의 가중치가 균일하게 분포되도록 도와준다. 이는 모델의 복잡도를 조정하여 일반화 성능을 향상시키는 데 기여한다. L1 정칙화와 달리, L2 정칙화는 모든 가중치를 조금씩 줄이는 경향이 있어, 희소성을 제공하지 않는다. 주로 심층 신경망 모델에서 많이 사용되며, 하이퍼파라미터 조정이 필요하다.\n$$ \\text{Loss}\\text{L2} = \\text{Loss}{\\text{original}} + \\lambda \\sum_{i=1}^{n} w_i^2 $$\n$ \\text{Loss}_{L2} $: L2 정칙화가 적용된 전체 손실 함수 가중치 감쇠 가중치 감쇠는 L2 정칙화와 유사하게, 손실 함수에 규제 항을 추가하여 모델의 가중치를 작게 유지하는 기법이다.\n딥러닝 라이브러리에서는 종종 최적화 함수에 weight_decay 파라미터로 구현되며, L2 정칙화와 동일한 효과를 가진다. 가중치 감쇠는 모델의 일반화 성능을 향상시키기 위해 사용되며, 다른 정칙화 기법과 함께 적용할 때 더욱 효과적일 수 있다.\n1 optimizer = torch.optim.SGD(model.parametersO, lr=0.01, weight_decay=0.01) 엘라스틱 넷 엘라스틱 넷(Elastic-Net)은 L1 정칙화와 L2 정칙화를 결합한 방식으로, 두 정칙화의 장점을 동시에 활용한다.\n이는 모델이 희소성과 작은 가중치의 균형을 맞추도록 도와주며, 특히 피처의 수가 샘플의 수보다 많을 때 유의미한 성능 향상을 제공한다. 혼합 비율을 조절하여 두 정칙화의 영향을 조절할 수 있으나, 새로운 하이퍼파라미터가 추가되므로 튜닝이 필요하다.\n$$ \\text{Loss}\\text{ElasticNet} = \\text{Loss}{\\text{original}} + \\lambda_1 \\sum_{i=1}^{n} |w_i| + \\lambda_2 \\sum_{i=1}^{n} w_i^2 $$\n$ \\lambda_1 $: L1 정칙화의 규제 강도 $ \\lambda_2 $: L2 정칙화의 규제 강도 드롭아웃 드롭아웃(Dropout)은 신경망의 훈련 과정에서 일부 노드를 임의로 제거하거나 0으로 설정하여 오버피팅을 방지하는 기법이다.\n이는 노드 간의 동조화(co-adaptation)를 억제하여 모델이 특정 노드에 지나치게 의존하지 않도록 한다. 드롭아웃은 모델의 일반화 성능을 향상시키는 동시에 모델 평균화 효과를 제공하지만, 충분한 데이터와 깊은 모델에 적용할 때 더욱 효과적이다. 배치 정규화와 함께 사용할 때는 신중하게 조합해야 한다.\n$$ y = \\begin{cases} 0 \u0026amp; \\text{with probability } p \\ \\frac{y}{1-p} \u0026amp; \\text{with probability } 1-p \\end{cases} $$\n$ y $: 뉴런의 출력값 $ p $: 뉴런을 제거할 확률 (드롭아웃 비율) $ 1-p $: 뉴런을 유지할 확률 출력값을 $ \\frac{1}{1-p} $로 스케일링하여 훈련 시와 추론 시의 활성화 분포를 일치 그레이디언트 클리핑 그레이디언트 클리핑(Gradient Clipping)은 모델 학습 시 기울기가 너무 커지는 현상을 방지하기 위해 사용되는 기법이다.\n이는 기울기의 크기를 특정 임곗값으로 제한하여, 학습 과정에서 발생할 수 있는 기울기 폭주 문제를 해결한다. 주로 순환 신경망(RNN)이나 LSTM 모델에서 활용되며, 학습률을 조절하는 효과와 유사한 역할을 한다. 그레이디언트 클리핑은 하이퍼파라미터인 최대 임곗값을 신중하게 설정해야 하며, 이를 통해 모델의 안정적인 학습을 도모할 수 있다.\n$$ \\text{if } ||g||_2 \u0026gt; r, \\quad g \\leftarrow \\frac{g}{||g||_2} \\times r $$\n$ g $: 기울기 벡터 $ ||g||_2 $: 기울기 벡터의 L2 노름 $ r $: 설정한 임계값 (threshold) 기울기의 방향은 유지하면서 크기를 $ r $로 제한 ","date":"2024-02-26T00:00:00Z","image":"https://gyeongmin.kr/images/pytorch-transformer-nlp-computer-vision.png","permalink":"https://gyeongmin.kr/p/regularization/","title":"정칙화"},{"content":"활성화 함수 활성화 함수는 인공 신경망에서 뉴런의 출력을 비선형으로 변환하여 은닉층을 활성화하는 역할을 한다. 이를 통해 네트워크는 데이터의 복잡한 패턴을 학습하고 비선형 문제를 해결할 수 있다.\n각 노드의 전달 보강이 다르므로 입력값에 따라 일부 노드는 활성화(Activate)되고 다른 노드는 비활성화(Deactivate)된다.\n활성화 함수는 비선형 구조를 가지며 미분 가능해야 학습이 가능하고, 입력값을 정규화(Normalization)하는 효과도 수행한다.\n계단 함수 (Step Function) 계단 함수는 입력값이 특정 임곗값(보통 0)을 넘으면 1을 출력하고, 그렇지 않으면 0을 출력하는 함수이다.\n이 함수는 출력이 이산적이므로 단순한 분류 작업에 사용될 수 있지만, 비연속적인 특성으로 인해 기울기(Gradient)가 존재하지 않아 역전파(Backpropagation)를 사용할 수 없다.\n$$ f(x) = \\begin{cases} 1 \u0026amp; \\text{if } x \\geq 0 \\ 0 \u0026amp; \\text{if } x \u0026lt; 0 \\end{cases} $$\nxychart-beta title \"Step Function\" y-axis 0 --\u003e 1 x-axis [-4, -3, -2, -1, 0, 0, 1, 2, 3, 4] line [ 0, 0, 0, 0, 0, 1, 1, 1, 1, 1] 임곗값 함수 (Threshold Function) 임곗값 함수는 계단 함수의 변형으로 특정 임계값을 기준으로 값을 출력한다. 입력값이 임계값 이상이면 1을 출력하고, 그 미만이면 특정 값을 출력한다.\n이 함수는 이진 분류에 사용될 수 있으나, 계단 함수와 마찬가지로 기울기가 없어서 신경망 학습에는 적합하지 않다.\n$$ f(x) = \\begin{cases} x \u0026amp; \\text{if } x \u0026gt; threshold \\ value \u0026amp; \\text{else } \\ \\ \\ otherwise \\end{cases} $$\nxychart-beta title \"Threshold Function (value = -5)\" y-axis -5 --\u003e 4 x-axis [-4, -3, -2, -1, 0, 0, 1, 2, 3, 4] line [-5, -5, -5, -5, -5, 0, 1, 2, 3, 4] 시그모이드 함수 (Sigmoid Function) 시그모이드 함수는 입력값을 0과 1 사이의 연속적인 값으로 변환하는 함수로, 출력값을 확률로 해석할 수 있어 이진 분류 문제에서 사용된다.\n함수의 출력이 부드럽게 변하므로 미분이 가능하지만, 큰 입력값에서는 기울기가 0에 가까워지는 Vanishing Gradient 문제가 발생할 수 있어 깊은 신경망에서는 성능이 저하될 수 있다.\n$$ f(x) = \\frac{1}{1 + e^{-x}} $$\nxychart-beta title \"Sigmoid Function\" y-axis 0 --\u003e 1 x-axis [-5, -4.5, -4, -3.5, -3, -2.5, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5] line [0.0067, 0.011, 0.018, 0.029, 0.047, 0.076, 0.119, 0.182, 0.269, 0.378, 0.5, 0.622, 0.731, 0.818, 0.881, 0.924, 0.953, 0.971, 0.982, 0.989, 0.993] 하이퍼볼릭 탄젠트 함수 (Tanh Function) 하이퍼볼릭탄젠트 함수는 시그모이드 함수의 확장판으로, 출력 범위가 -1에서 1 사이로 설정되어 있다.\n이 함수는 평균이 0에 가까워져 학습이 비교적 안정적이며, 시그모이드 함수보다 빠르게 수렴할 수 있다. 그러나 여전히 큰 입력값에서는 기울기가 0에 가까워지는 문제를 완전히 해결하지는 못한다.\n$$ f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $$\nxychart-beta title \"Tanh Function\" y-axis -1 --\u003e 1 x-axis [-5, -4.5, -4, -3.5, -3, -2.5, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5] line [-1, -0.999, -0.999, -0.998, -0.995, -0.986, -0.964, -0.905, -0.761, -0.462, 0, 0.462, 0.761, 0.905, 0.964, 0.986, 0.995, 0.998, 0.999, 0.999, 1] ReLU 함수 (Rectified Linear Unit Function) ReLU(Rectified Linear Unit) 함수는 입력값이 0 이하이면 0을 출력하고, 그 이상이면 입력값을 그대로 출력하는 함수이다.\n간단한 수식과 계산량 덕분에 학습 속도가 빠르고 효율적이지만, 0 이하의 값에서는 기울기가 0이 되어 뉴런이 더 이상 업데이트되지 않는 죽은 뉴런 문제가 발생할 수 있다.\n$$ f(x) = \\max(0, x) $$\nxychart-beta title \"ReLU Function\" y-axis -0.5 --\u003e 3 x-axis [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5] line [0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5] Leaky ReLU 함수 Leaky ReLU 함수는 ReLU 함수의 단점을 개선한 형태로, 입력값이 0 이하일 때도 작은 기울기 α를 곱한 값을 출력한다. 이로 인해 죽은 뉴런 문제를 완화하며 학습이 계속 이루어질 수 있도록 한다.\n기울기 α는 보통 0.01과 같은 작은 값으로 설정되며, ReLU의 비선형성과 계산 효율성을 유지한다.\n$$ f(x) = \\begin{cases} x \u0026amp; \\text{if } x \u0026gt; 0 \\ \\alpha x \u0026amp; \\text{if } x \\leq 0 \\end{cases} $$\nxychart-beta title \"Leaky ReLU Function (α = 0.1)\" y-axis -1 --\u003e 5 x-axis [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5] line [-0.5, -0.4, -0.3, -0.2, -0.1, 0, 1, 2, 3, 4, 5] PReLU 함수 (Parametric ReLU Function) PReLU(Parametric ReLU) 함수는 Leaky ReLU의 확장판으로, 0 이하의 기울기 α를 고정된 값이 아닌 학습 가능한 파라미터로 설정한다.\n이로 인해 데이터에 따라 기울기를 최적화할 수 있으므로 네트워크의 성능을 더욱 개선할 수 있다. 다만 학습할 파라미터가 늘어나기 때문에 계산 비용이 조금 증가할 수 있다.\nELU 함수 (Exponential Linear Unit Function) ELU(Exponential Linear Unit) 함수는 ReLU와 Leaky ReLU의 단점을 보완한 함수로, 입력값이 0 이하일 때 $ e^x - 1 $ 형태의 부드러운 곡선을 갖는다.\nELU는 0 이하의 출력이 음수 값을 가지기 때문에 평균 출력이 0에 가깝게 유지되어 학습을 더 안정적으로 만들며, 죽은 뉴런 문제도 해결할 수 있다.\n$$ ELU(x) = \\begin{cases} x \u0026amp; \\text{if } x \u0026gt; 0 \\ \\alpha (e^x - 1) \u0026amp; \\text{else } \\text{otherwise} \\end{cases} $$\nxychart-beta title \"ELU Function (α = 1)\" y-axis -3 --\u003e 5 x-axis [-4, -3.6, -3.2, -2.8, -2.4, -2, -1.6, -1.2, -0.8, -0.4, 0, 0.4, 0.8, 1.2, 1.6, 2, 2.4, 2.8, 3.2, 3.6, 4] line [-0.9817, -0.9736, -0.9502, -0.9131, -0.8647, -0.7869, -0.6988, -0.6065, -0.4866, -0.3297, 0, 0.4, 0.8, 1.2, 1.6, 2, 2.4, 2.8, 3.2, 3.6, 4] 소프트맥스 함수 (Softmax Function) 소프트맥스 함수는 다중 클래스 분류에서 사용되는 함수로, 입력값을 확률 분포로 변환한다. 각 클래스의 출력값에 대해 지수 함수로 변환한 후, 전체 클래스의 지수값 합으로 나눈다.\n이로 인해 출력값의 합이 항상 1이 되며, 이를 통해 각 클래스에 대한 확률로 해석할 수 있다. 소프트맥스 함수는 주로 신경망의 출력층에서 사용된다.\n$$ p_k = \\frac{e^{z_k}}{\\sum_{i=1}^n e^{z_i}} $$\n","date":"2024-02-24T00:00:00Z","image":"https://gyeongmin.kr/images/pytorch-transformer-nlp-computer-vision.png","permalink":"https://gyeongmin.kr/p/activation-functions/","title":"활성화 함수"},{"content":"최적화 함수란? 신경망(neural network)의 학습 목적은 손실 함수(loss function)의 값을 최대한 낮추는 매개변수(parameter)를 찾는 것이다. 이는 곧 매개변수의 최적값을 찾는 문제이며, 이를 최적화 문제(optimization)라고 한다. 최적화 문제를 해결하기 위해 사용하는 도구가 바로 최적화 함수(optimizer)이다.\n최적화 함수는 손실 함수의 값을 최소화하는 방향으로 모델의 매개변수를 조정한다. 손실 함수는 예측값(predicted value)과 실제값(true value) 간의 차이를 나타내며, 최적화 함수는 손실을 점진적으로 줄여 모델의 예측 성능을 향상시킨다.\n최적화 함수는 기울기(gradient)를 활용하여 손실 함수의 값을 낮추는 방향으로 매개변수를 조정한다. 기울기는 각 지점에서 함수의 값을 줄이는 방향을 나타내는 지표로, 손실 값을 줄이기 위한 업데이트의 핵심 정보를 제공한다. 그러나 딥러닝에서 사용하는 손실 함수는 고차원적이고 복잡한 지형을 가지기 때문에, 기울기가 항상 최적의 방향을 가리킨다고 보장할 수는 없다. 이는 다음과 같은 문제로 이어질 수 있다.\n안장점(Saddle Point)\n안장점은 특정 방향에서는 극대값처럼 보이고, 다른 방향에서는 극솟값처럼 보이는 지점이다. 이 지점에서는 기울기가 0에 가까워져 학습이 정체될 수 있다.\n지역 최적해(Local Minimum)\n지역 최적해는 특정 구역에서 손실 값이 가장 작은 지점이다. 그러나 이는 전역 최솟값(global minimum)과는 거리가 있을 수 있으며, 최적화 과정이 이 지점에 갇히면 더 나은 해를 찾지 못할 위험이 있다.\n평탄한 구간(Flat Regions)\n손실 함수가 일정하거나 기울기가 매우 작은 평탄한 구간에서는 학습이 느려지거나 멈출 수 있다.\n이러한 문제를 극복하고 손실 값을 줄이기 위해, 최적화 함수는 기울기의 정보를 활용해 적절한 방향으로 이동한다. 이러한 접근 방식의 시초가 되는 알고리즘이 바로 경사하강법(Gradient Descent)이다.\nGradient Descent (GD) 경사하강법은 손실 함수의 기울기를 계산하여 손실 값을 줄이는 방향으로 매개변수를 업데이트하는 알고리즘이다.\n경사하강법의 기본 수식은 다음과 같다.\n$$ \\theta_{t+1} = \\theta_t - \\eta \\nabla_{\\theta} L(\\theta_t) $$\n$ \\theta_t $ : 현재 파라미터, $ \\eta $ : 학습률(learning rate), $ \\nabla_{\\theta} L(\\theta_t) $ : 손실 함수의 기울기. 이 수식에서 기울기는 손실 값을 줄이는 방향을 가리킨다. 이를 반복적으로 수행하면 모델의 매개변수가 점진적으로 최적값에 근접한다.\n1950s, Stochastic Gradient Descent (SGD) SGD는 초기의 Batch Gradient Descent의 비효율성을 해결하기 위해 개발되었다. Batch Gradient Descent는 전체 데이터셋을 사용하여 손실 함수의 기울기를 계산하고 파라미터를 업데이트하기 때문에 데이터셋이 클수록 계산 비용이 급격히 증가한다. 이러한 문제를 해결하기 위해 SGD는 데이터의 일부인 샘플이나 미니배치를 사용하여 손실 함수의 기울기를 계산하고 파라미터를 업데이트한다.\nSGD의 업데이트 방식은 GD와 거의 같으나, 전체 데이터가 아닌 샘플들을 가지고 gradient 를 구하며 parameter 를 업데이트 해가는 방식이다.\nSGD는 전체 데이터셋을 처리하지 않고도 학습을 진행할 수 있어 대규모 데이터 학습에서 계산 비용을 대폭 줄일 수 있다. 또한, 실시간 학습이나 스트리밍 데이터 처리와 같은 환경에서도 효과적으로 사용할 수 있다. 그러나 기울기의 노이즈로 인해 최적점 근처에서 진동(oscillation) 현상이 발생하며, 손실 함수의 평탄한 구간(flat regions)에서는 느리게 수렴한다는 한계가 있다.\nPyTorch에서 SGD는 아래와 같이 구현할 수 있다.\n1 2 import torch.optim as optim optimizer = optim.SGD(model.parameters(), lr=0.01) 1964, Momentum Momentum은 SGD의 진동 문제를 해결하기 위해 제안되었다. SGD는 현재 기울기만을 기반으로 업데이트를 수행하기 때문에 손실 함수의 지형이 비대칭인 경우 진동이 발생하여 최적점에 도달하는 데 오랜 시간이 걸릴 수 있다. Momentum은 과거 기울기의 이동 평균을 반영하여 더욱 안정적이고 효율적인 학습 경로를 제공한다.\nMomentum의 업데이트 방식은 아래와 같다. $$ v_{t+1} = \\beta v_t + (1-\\beta) \\nabla_{\\theta_t} L(\\theta_t) $$\n$$ \\theta_{t+1} = \\theta_t - \\eta_t v_{t+1} $$\n$ v_t $ : 이동 평균(모멘텀 변수), $ \\beta $ : 모멘텀 계수로, 일반적으로 0.9로 설정된다. Momentum은 기울기의 이동 평균을 계산하여 진동을 줄이고, 손실 함수가 좁고 깊은 구간에서도 빠르게 수렴할 수 있도록 한다. 그러나 추가적인 하이퍼파라미터 $ \\beta $를 적절히 설정해야 한다는 점에서 복잡성이 증가한다.\nPyTorch에서 Momentum은 아래와 같이 구현할 수 있다.\n1 optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) 1983, Nesterov Accelerated Gradient (NAG) NAG는 Momentum의 단점을 보완하기 위해 개발되었다. Momentum 방식은 현재 위치에서 기울기를 계산하므로, 최적점 근처에서 오버슈팅(overshooting)이 발생할 가능성이 있다. NAG는 모멘텀에 의해 예측된 미래의 위치에서 기울기를 계산함으로써 이러한 문제를 해결한다.\nNAG의 업데이트 방식은 아래와 같다. $$ v_{t+1} = \\beta v_t + \\nabla_{\\theta} L(\\theta_t - \\eta \\beta v_t) $$\n$$ \\theta_{t+1} = \\theta_t - \\eta v_{t+1} $$\nNAG는 Momentum이 제공하는 이동 방향에 대해 한 단계 더 정교하게 접근하여, 최적점 근처에서의 오버슈팅 문제를 완화한다. 또한, 수렴 속도를 개선하며 최적점 근처에서도 안정적으로 작동한다. 그러나 기울기 계산이 더 복잡해지고 계산 비용이 증가한다는 한계가 있다.\nNAG는 PyTorch에서 아래와 같이 구현할 수 있다.\n1 optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True) 2011, Adagrad Adagrad는 SGD와 Momentum이 모든 파라미터에 동일한 학습률을 적용한다는 문제를 해결하기 위해 도입되었다. 데이터셋에 드문 특징(sparse feature)이 포함되어 있는 경우, 이러한 방식은 드문 특징을 학습하는 데 비효율적이다. Adagrad는 각 파라미터의 과거 기울기의 제곱합을 기반으로 학습률을 조정하여 이를 해결한다.\nAdagrad의 업데이트 방식은 아래와 같다. $$ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\nabla_{\\theta} L(\\theta_t) $$\n$ G_t $ : 과거 기울기의 제곱합, $ \\epsilon $ : 0으로 나누는 것을 방지하기 위한 작은 값. Adagrad는 희소 데이터에 대한 학습에서 특히 효과적이며, 각 파라미터에 적응적인 학습률을 적용한다는 점에서 큰 장점이 있다. 그러나 기울기의 제곱합이 점진적으로 증가하면서 학습률이 감소하여 장기 학습에는 적합하지 않다는 한계가 있다.\nAdagrad는 PyTorch에서 아래와 같이 구현할 수 있다.\n1 optimizer = optim.Adagrad(model.parameters(), lr=0.01) 2012, RMSprop RMSprop은 Adagrad의 단점을 해결하기 위해 제안되었다. Adagrad는 기울기의 제곱합이 점진적으로 증가하여 학습률이 감소하고, 장기 학습에서는 효과가 떨어진다. RMSprop은 기울기의 제곱 이동 평균(Exponentially Weighted Moving Average)을 사용하여 학습률을 조정함으로써 이러한 문제를 해결한다.\nRMSprop의 업데이트 방식은 아래와 같다.\n$$ E[g^2]_t = \\rho E[g^2] _{t-1} + (1-\\rho)g_t^2 $$\n$$ \\theta_{t+1} = \\theta _t - \\frac{\\eta}{\\sqrt{E[g^2] _t + \\epsilon}} \\nabla _{\\theta} L(\\theta _t) $$\n$ E[g^2]_t $ : 기울기의 제곱 이동 평균, $ \\rho $ : 지수 이동 평균의 가중치 계수로, 일반적으로 0.9로 설정된다, $ \\epsilon $ : 0으로 나누는 것을 방지하기 위한 작은 값. RMSprop은 Adagrad와 달리 이동 평균을 사용하여 학습률 감소 문제를 해결하고, 일정한 학습률을 유지하며 안정적으로 작동한다. 특히 RNN(Recurrent Neural Network)과 같은 모델에서 효과적으로 사용된다.\nRMSprop은 PyTorch에서 아래와 같이 구현할 수 있다.\n1 optimizer = optim.RMSprop(model.parameters(), lr=0.01) 2014, Adam Adam은 Momentum과 RMSprop의 장점을 결합하여 더욱 효율적이고 안정적인 학습을 제공하기 위해 제안되었다. Adam은 1차 모멘텀(기울기의 이동 평균)과 2차 모멘텀(기울기의 제곱 이동 평균)을 모두 사용하여 학습률을 조정한다.\nAdam의 업데이트 방식은 아래와 같다.\n$$ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t $$\n$$ v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)g_t^2 $$\n$$ \\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t} $$\n$$ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t $$\n$ m_t $ : 기울기의 1차 모멘텀(평균), $ v_t $ : 기울기의 2차 모멘텀(분산), $ \\hat{m}_t $, $ \\hat{v}_t $ : 편향 보정된 모멘텀. Adam은 빠르고 안정적인 수렴을 제공하며, 대부분의 딥러닝 모델에서 기본 최적화 알고리즘으로 사용된다. 학습률의 조정이 자동으로 이루어져 하이퍼파라미터 설정의 복잡성이 감소하는 장점이 있다. 그러나 과적합 가능성이 높아질 수 있으며, 일반화 성능이 저하될 위험이 있다.\nAdam은 PyTorch에서 아래와 같이 구현할 수 있다.\n1 optimizer = optim.Adam(model.parameters(), lr=0.001) 2017, AdamW AdamW는 Adam의 일반화 성능 문제를 해결하기 위해 제안되었다. Adam은 L2 정규화를 사용하는 방식이 가중치 감쇠(weight decay)와 동일하지 않다는 문제가 있었고, 이는 과적합 위험을 증가시켰다. AdamW는 가중치 감쇠를 명시적으로 적용하여 이러한 문제를 해결하였다.\nAdamW의 업데이트 방식은 아래와 같다. $$ \\theta_{t+1} = \\theta_t - \\eta (\\hat{m}_t / \\sqrt{\\hat{v}_t} + \\epsilon + \\lambda \\theta_t) $$\n$ \\lambda $ : 가중치 감쇠(weight decay) 계수. AdamW는 과적합을 줄이고 일반화 성능을 향상시키는 데 효과적이다. 특히 딥러닝 연구와 최신 모델 개발에서 기본 최적화 알고리즘으로 널리 사용되고 있다.\nAdamW는 PyTorch에서 아래와 같이 구현할 수 있다.\n1 optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01) ","date":"2024-02-09T00:00:00Z","image":"https://gyeongmin.kr/images/pytorch-transformer-nlp-computer-vision.png","permalink":"https://gyeongmin.kr/p/optimization-functions/","title":"최적화 함수의 종류와 발전 과정"},{"content":"Word2Vec의 최적화 Word2Vec은 자연어 처리 분야에서 매우 중요한 도구로 자리 잡고 있다. 그러나 Word2Vec은 어휘의 양이 방대해질수록 계산량과 메모리 사용량이 커지는 문제를 가지고 있다. 이러한 문제를 해결하기 위하여 \u0026lsquo;임베딩(Embedding)\u0026rsquo; 계층을 도입하고 \u0026lsquo;네거티브 샘플링(Negative Sampling)\u0026rsquo; 기법을 적용하는 두 가지 주요 개선 방법을 살펴보자.\nEmbedding 계층 배경 전통적으로 단어를 표현하는 방법 중 하나는 원-핫 인코딩이다. 이 방법은 각 단어를 하나의 긴 벡터로 표현하며, 벡터의 크기는 어휘의 크기와 같다. 벡터에서 단어에 해당하는 위치는 1이고 나머지는 모두 0이다. 이 방식은 직관적이지만, 벡터가 대부분 0으로 채워지는 희소성 문제와 차원이 커질수록 계산 비효율성이 증가하는 문제를 가지고 있다.\n예를 들어, 어휘 사전에 10,000개의 단어가 있다면, 각 단어는 10,000차원의 벡터로 표현된다. 이렇게 고차원 벡터와 가중치 행렬의 곱셈 계산은 많은 계산 자원을 소모한다.\n임베딩 레이어는 각 단어를 고정된 크기의 밀집 벡터로 변환한다. 이 밀집 벡터는 단어의 의미를 수치적으로 포착할 수 있으며, 벡터의 각 요소는 연속된 값으로 이루어져 있다. 이로 인해 희소성 문제를 해결하고, 효율적인 계산이 가능해진다.\n작동 원리 임베딩 레이어는 각 단어를 고유한 인덱스에 매핑하고, 이 인덱스를 사용하여 단어의 밀집 벡터를 찾는다. 이 밀집 벡터는 학습 가능한 파라미터로, 모델 학습 과정에서 최적화된다.\n전통적인 원-핫 인코딩 방식은 단어의 인덱스에 해당하는 위치에만 1을 두고 나머지는 0으로 채워 계산을 수행한다. 이에 반해, 임베딩 레이어는 각 단어에 대한 밀집 벡터를 직접 참조하여 계산을 수행하기 때문에, 불필요한 계산을 크게 줄여준다.\n구현 예시 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import numpy as np class Embedding: def __init__(self, W): self.params = [W] self.grads = [np.zeros_like(W)] self.idx = None def forward(self, idx): W, = self.params self.idx = idx out = W[idx] return out def backward(self, dout): dW, = self.grads dW[...] = 0 for i, word_id in enumerate(self.idx): dW[word_id] += dout[i] return None 네거티브 샘플링 기법 네거티브 샘플링은 다중 분류 문제를 이진 분류로 근사하여 계산량을 대폭 줄이는 기법이다. 이 기법은 특히 대규모 어휘를 다루는 자연어 처리에서 중요한 역할을 한다.\n배경 Word2Vec과 같은 언어 모델은 단어의 의미를 벡터로 변환하여 수치화하는 과정을 수행한다. 이 과정에서 전체 어휘에 대한 예측을 수행하게 되면, 어휘의 크기가 커질수록 계산량이 매우 늘어나는 문제가 있다. 특히, Softmax 계층에서의 계산은 모든 어휘에 대해 수행되어야 하므로, 어휘 수에 비례하여 계산량이 급격히 증가한다. 네거티브 샘플링은 이러한 문제를 효과적으로 해결하는 방법으로 제안되었다.\n다중 분류 문제의 이진 분류 근사 네거티브 샘플링의 핵심 아이디어는 다중 분류 문제를 이진 분류 문제로 근사하는 것이다. 전체 어휘에 대한 예측 대신, 모델이 특정 단어를 정답으로 예측하는지 여부만을 판단하게 한다. 즉, \u0026lsquo;이 단어가 맞는가? 아닌가?\u0026lsquo;라는 간단한 질문에 답하는 형식으로 문제를 단순화한다.\n긍정적 예와 부정적 예의 선택 긍정적 예(Positive Samples): 모델이 맞추어야 하는 실제 단어. 예를 들어 문맥이 \u0026ldquo;The cat sits on the\u0026rdquo; 일 때, 실제 다음 단어인 \u0026ldquo;mat\u0026rdquo; 이 긍정적 예가 된다.\n부정적 예(Negative Samples): 무작위로 선택된 단어들로, 모델이 이 단어들을 정답으로 선택하지 않도록 학습한다. 이들은 긍정적 예와 구분되어야 할 대상들이다.\n계산 효율성의 증가 네거티브 샘플링을 사용하면 모델이 전체 어휘에 대한 Softmax 계산을 수행할 필요가 없어진다. 대신, 긍정적 예에 대해서는 확률을 높이고, 선택된 부정적 예에 대해서는 확률을 낮추는 방식으로 학습이 이루어진다. 이 과정은 계산량을 현저히 줄여주며, 특히 어휘의 크기가 큰 경우에 매우 효과적이다.\n구현 예시 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class NegativeSamplingLoss: def __init__(self, W, corpus, power=0.75, sample_size=5): self.sample_size = sample_size self.sampler = UnigramSampler(corpus, power, sample_size) self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)] self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)] self.params, self.grads =[], [] for layer in self.embed_dot_layers: self.params += layer.params self.grads += layer.grads def forward(self, h, target): batch_size = target.shape[0] negative_sample = self.sampler.get _negative_sample(target) # 긍정적 예 순전파 score = self.embed_dot_layers[0].forward(h, target) correct_label = np.ones(batch_size, dtype=np.int32) loss = self.loss_layers[0].forward(score, correct_label) # 부정적 예 순전파 negative_label = np.zeros(batch_size, dtype=np.int32) for i in range(self.sample.size): negative_target = negative_sample[:, i] score = self.embed_dot_layers[1 + i].forward(h, negative_target) loss += self.loss_layers[1 + i].forward(score, negative_label) return loss def backward(self, dout=1): dh = 0 for l0, l1 in zip(self.loss_layers, self.embed_dot_layers): dscore = 10.backward(dout) dh += l1.backward(dscore) return dh CBOW 모델 구현 이전 포스팅에서 구현했던 CBOW 모델에 Embedding과 Negative Sampling Loss 계층을 적용해 개선해 보자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class CBOW: def __init__(self, vocab_size, hidden_size, window_size, corpus): V, H = vocab_size, hidden_size W_in = 0.01 * np.random.randn(V, H).astype(\u0026#39;f\u0026#39;) W_out = 0.01 * np.random.randn(V, H).astype(\u0026#39;f\u0026#39;) self.in_layers =[] for i in range(2 * window_size): layer = Embedding(W_in) self.in_layers.append(layer) self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5) layers = self.in_layers + [self.ns_loss] self.params, self.grads = [], [] for layer in layers: self.params += layer.params self.grads += layer.grads self.word_vecs = W_in def forward(self, contexts, target): h = 0 for i, layer in enumerate(self.in_layers): h += layer.forward(contexts[:, i]) h *= 1 / len(self.in_layers) loss = self.ns_loss.forward(h, target) return loss def backward(self, dout=1)： dout = self.ns_loss. backward(dout) dout *= 1 / len(self.in_layers) for layer in self.in_layers: layer.backward(dout) return None 단어 ID의 배열이 contexts와 target의 예이다. 맥락은 2차원 배열이고 타겟은 1차원 배열이고, 이러한 데이터가 순전파에에 입력되는 것이다.\n결론 임베딩 계층과 네거티브 샘플링 기법은 Word2Vec 모델의 계산량과 메모리 사용량을 현저하게 줄여주는 효과적인 방법이다. 특히, 대규모 어휘를 가진 말뭉치를 다루는 데 있어서 이러한 개선 방법은 필수적이다. 이를 통해 보다 빠르고 효율적으로 자연어 처리 모델을 학습이 가능하다.\nWord2Vec은 자연어 처리에 중요한 역할을 하고 있다. 이것으로 단어들이 고정 길이의 벡터로 변환되며, 이렇게 변환된 단어 벡터는 비슷한 의미를 가진 단어들을 찾는 데 유용하게 사용된다. 또한, 단어의 분산 표현은 전이 학습에도 적용할 수 있는데, 이는 한 분야에서 획득한 지식을 다른 분야에 적용하는 것을 의미한다. 이를 통해 다양한 자연어 처리 문제에 효과적으로 접근할 수 있다.\n자연어 처리 작업에서는 Word2Vec 모델이 주로 큰 말뭉치로 사전 학습된 상태에서 사용된다. 예를 들어, 텍스트 분류, 문서 클러스터링, 감정 분석 등의 작업에서 사전에 학습된 단어 벡터를 활용함으로써 작업의 성능을 향상시킬 수 있다. 이러한 방식은 자연어를 벡터로 변환함으로써 일반적인 머신러닝 기법(신경망, SVM 등등)을 자연어 처리 문제에 적용할 수 있게 해준다. 따라서 Word2Vec의 단어 분산 표현은 자연어 처리 분야에서 높은 정확도와 효율성을 제공하는 핵심적인 요소가 되고 있다.\n","date":"2024-01-03T00:00:00Z","image":"https://gyeongmin.kr/images/deep-learning-from-scratch.jpeg","permalink":"https://gyeongmin.kr/p/word2vec-2/","title":"Word2Vec의 최적화"},{"content":"PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation 논문 PDF\nIntroduction Anomaly Detection Anomaly는 정상의 범주에서 벗어나 있는 모든 것들을 의미한다.\n주어진 데이터셋에서 Anomaly들을 탐지하는 것을 Anomaly Detection (이상치 탐지)라 한다. 주로 정상 데이터셋만으로 학습을 진행하며, 주어진 이미지를 Normal / Anomaly로 구분해야 하기에 One-Class Classification (OCC) 라고도 부른다.\n본 논문에서는 Anomaly를 검출하는 세 분야를 다음과 같이 정의한다.\nAnomaly Detection: 훈련 과정에서 오직 정상 이미지만 사용하는 경우\nAnomaly Segmentation: 정상 이미지가 주어진 상태에서 Anomaly가 포함된 모든 픽셀을 탐지하는 경우\nOutlier Exposure: 이상 현상을 시뮬레이션하는 데에 외부 데이터셋 사용이 가능한 경우\n최근 대부분의 이상 탐지 방법들은 제한된 정상 훈련 데이터셋을 사용하여 데이터의 feature를 학습하며, 이것에 의존하고 있다.\nCatastrophic Collapse 특히 Multi-Class Calssifiaction보다 One-Class Classification 분야에서 전이 학습에 대한 연구가 부족한 상황이다. 이미지 전이 학습 분야에서는 Catastrophic Collapse가 잘 발생하기 때문이다. Catastrophic Collapse는 Normal과 Anomaly 데이터가 잘 구분되지 않고, feature 공간에서 동일한 지점에 매핑되는 현상을 말한다. 이 현상은 데이터의 패턴을 학습하지 못하거나 학습 과정에서 Overfitting에 빠지는 것에 기인하기 때문에, Early Stopping을 위해 Epoch를 잘 조절한다면 이를 방지할 수 있다.\n본 논문에서는 Catastrophic Collapse를 방지하기 위해 Epoch 수에 의존적이지 않은 early stopping variant 방법과 continual learning에서 영감을 받은 Elastic Regularization 기법을 제안하는데, 이는 뒤에서 자세히 설명하겠다.\nBackground 3-Stage Framework 일반적인 프레임워크는 세 단계로 구성된다. 정상 훈련 데이터셋 $D_{train} = \\{ x_1, x_2, \\dots, x_N \\} $이 주어졌다고 가정하자.\nInitial feature extractor\nInitial feature extractor인 $\\psi_0$는 pre-train을 통해 얻을 수 있으며, 손실 함수는 $L_{pretrain}$이다. auxiliary task (보조 작업)은 외부 데이터셋을 통한 pre-train이나 self-supervised learning일 수 있다.\nFeature Adaptation\nauxiliary task나 데이터셋을 통해 학습된 feature는 anomaly score를 매기기 전에 적응이 필요할 수 있다. 이는 훈련 데이터셋을 통한 fine-tuning으로, 적응된 feature extractor는 $\\psi$로 표기한다.\nAnomaly Scoring\nFeature를 적응시킨 후, 훈련 데이터셋 샘플들의 특성 $\\psi(x_1), \\psi(x_2), \\dots, \\psi(x_N)$ 을 추출한다. 그 다음, Anomaly score를 구하는 함수를 학습하는 과정을 진행한다. 일반적으로 scoring function은 테스트 샘플 $\\psi(x)$ 주변의 정상 데이터 밀도를 측정하며 밀도가 낮은 지역에 높은 anomaly score를 할당한다.\nDeep Nearest Neighbours (DN2) \u0026ldquo;Deep Nearest Neighbor Anomaly Detection\u0026rdquo;에서 제안된 방법으로, DN2에서는 ImageNet 데이터셋에서 pre-trained된 ResNet을 사용해 feature를 추출하고, kNN을 적용해 Normal과 Anomaly를 구분한다. (Normal과 Anomaly 이미지 간의 평균 거리를 Anomaly Score로 사용한다)\nSemantic Pyramid Anomaly Detection (SPADE) \u0026ldquo;Sub-Image Anomaly Detection with Deep Pyramid Correspondences\u0026rdquo;에서 제안된 방법으로, ImageNet으로 사전 훈련된 ResNet을 사용하여 모든 이미지에 대한 픽셀별 특성을 추출한다. 또한 Feature Pyramid를 이용하여 다양한 수준의 feature를 동시에 추출하고 concat해 사용한다. DN2와는 달리 SPADE에서는 Anomaly Segmentation도 가능하다.\nDeep Support Vector Data Description (DeepSVDD) \u0026ldquo;Deep One-Class Classification\u0026rdquo;에서 제안된 방법으로, 데이터의 정상적인 패턴을 학습하여 정상 범주에서 크게 벗어난 데이터 포인트를 Anomaly로 식별하는 것이다. CNN을 사용해 이미지 데이터를 저차원의 feature 공간으로 매핑하고, 데이터 포인트가 중심에서 얼마나 떨어져 있는지를 측정하여 Anomaly 여부를 판단한다.\nJoint Optimization (JO) \u0026ldquo;Learning Deep Features for One-Class Classification\u0026rdquo;에서 제안된 방법으로, ImageNet 데이터셋에서 객체 분류를 위해 Pretrained된 Feature Extractor를 사용한다. 오분류에 대한 패널티가 없는 경우 trivial한 솔루션을 학습할 수 있기에, compactness loss와 classification loss를 동시에 최적화한다. 이 방법은 메모리를 많이 요구하며, 두 작업을 함께 훈련하는 경우 Anomaly Detection 작업의 정확도가 떨어질 수 있다는 한계점을 가진다.\nMethod SVDD와 JO와 유사하게 compactness loss를 사용하여 pre-trained된 feature의 분포를 anomaly detection 작업에 적응시킨다. 하지만 구조를 제한하거나 외부 데이터를 사용하지 않고, 직접적으로 Catastrophic Collapse를 다룬다.\ncompactness loss의 최적 솔루션이 Collapse로 이어질 수 있다. Collapse가 일어나면 모든 입력을 같은 지점으로 매핑하게 되어, 더이상 구분이 불가능해진다.\n본 논문에서는 Catastrophic Collapse를 방지하기 위해 아래 3가지 방법을 제시하였다.\nSimple Early Stopping (PANDA-Early) collapse가 일어나기 전에, 특정 Epoch마다 Early Stopping을 진행하는 방법이다. 가장 단순하며 강력한 방법이지만, Hyperparameter의 설정이 필요하다는 단점이 있다. 예를 들어, 15 Epoch마다 early stopping을 진행하는 것이다.\nSample-Wise Early Stopping (PANDA-SES) sample 단위로 early stopping여부를 결정하는 방법이다. Anomaly 샘플과 중심 간의 거리가 멀고, 정상 샘플과 중심 간의 거리가 짧을 때 anomaly detection의 정확도가 상관관계가 있다. 이를 위해 훈련 과정 중 특정 Epoch마다 (예를 들어 5 Epoch 마다) 평균 거리를 저장하고, 정규화를 거치고 평균 거리의 최대 비율을 anomaly detection score로 사용한다.\nElastic Regularization (PANDA-EWC) Continual Learning은 adaptive regularization을 이용하는 방법이다.\ncontinual learning에서 영감을 받아 Elastic Weight Consolidation(EWC)을 사용한 방법이다. Continual Learning이란 이전에 학습한 것을 까먹지 않고 새로운 것을 학습하는 것이다. 여기서 보조 작업에 대해 사전 훈련을 진행하기 위해 100개의 미니배치를 사용한다. 이 과정에서 신경망의 모든 가중치 파라미터에 대한 Fisher 정보 행렬 $F$의 대각선을 계산한다. 이는 사전 훈련 단계가 끝난 후 한 번만 수행한다. 각 가중치 파라미터의 Fisher 행렬 값은 사전 훈련 데이터셋을 통해 주어진 식으로 계산된다.\n연구팀은 Fisher 정보 행렬의 대각선 요소를 사용하여 네트워크의 각 가중치가 사전 훈련된 상태($\\psi_0$)에서 미세 조정된 상태($\\psi^*$)로 변화하는 거리의 제곱을 가중치로 삼는다. 이는 가중치 함수의 손실 풍경 곡률을 측정하는 방법으로 볼 수 있으며, 값이 크면 곡률이 높고 가중치가 비탄력적임을 의미한다.\n이러한 정규화 방법은 $\\lambda$라는 하이퍼파라미터로 가중치가 부여된 compactness loss와 함께 사용된다. 본 연구에서는 $\\lambda = 10^4$를 사용했다. 이를 통해 최종적인 손실 함수를 정의하고, 이는 사전 훈련된 가중치와 미세 조정된 가중치 간의 차이를 기반으로 한다. 이 접근법은 네트워크가 새로운 작업을 학습하면서도 이전에 학습한 작업에 대한 정보를 유지할 수 있도록 돕는다.\nAnomaly Scoring 전통적인 Anomaly Detection와 같이, Anomaly Score는 밀도 추정을 통해 구할 수 있다. 따라서 본 연구에서도 kNN을 사용해 구현한다.\nOutlier Exposure Outlier Exposure는 이미지 이상 탐지 작업을 확장한 것으로, Normal 데이터보다 Anomaly에 더 유사한 Auxiliary 데이터셋 $D_{OE}$가 있다고 가정하고, 데이터셋을 Normal과 Abnormal로 분류한다. 이는 Linear Classification Layer $w$와 Feature $\\psi$를 이용해 Logistic Regression Loss를 계산하는 방식으로 이루어진다.\nExpreiments Dataset Dataset $N_{classes}$ $N_{train}$ $N_{test}$ CIFAR10 10 5,000 10,000 Fashion MNIST 10 6,000 10,000 CIFAR100 20 2,500 10,000 Flowers 102 10 7,169 Birds 200 30 5,794 CatsVsDogs 2 10,000 5,000 MVTec 15 242 1,725 WBC 4 59 62 DIOR 19 649 9,243 여기서 MVTec을 제외한 데이터셋은 정상 데이터를 한 개의 클래스로 구성하고, 나머지 클래스는 비정상으로 구성한다. 예를 들어, 고양이 이미지는 정상이고 강아지나 토끼 등 다른 이미지는 전부 비정상으로 구성된다.\nMVTec 데이터셋은 Anomaly Detection을 위해 제작된 데이터셋으로, 클래스마다 정상과 비정상 이미지가 라벨링되어 있다.\nResults 위 두 결과를 보았을 때, Self-Supervised 방법보다 DN2와 PANDA 방법이 훨씬 높은 성능을 보이는 것을 확인할 수 있다.\n기존의 Anomaly Segmentation 방법들보다 SPADE 방법이 매우 높은 성능을 보이는 것을 확인할 수 있다.\n위 표를 보면, 모든 데이터셋에서 PANDA 방법들이 JO 방법보다 높은 성능을 보이는 것을 확인할 수 있다.\nConclusion 본 논문에서는 Anomaly Detection과 Anomaly Aegmentation을 위한 간단한 baseline을 제안하였으며, 이 방법은 현재 SOTA 방법들을 능가하며, 한계점을 해결했다. 또한 pre-trained된 Feature을 적응시키고 Catastrophic Collapse를 완화하는 방법을 제안하였다.\n하지만 이 연구의 주요 한계점은 pre-trained된 강력한 Feature Extractor가 필요하다는 것이다.\n","date":"2023-12-27T00:00:00Z","image":"https://gyeongmin.kr/images/paper-review.png","permalink":"https://gyeongmin.kr/p/paper-review-panda/","title":"[논문리뷰] PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation"},{"content":" 본 포스팅은 \u0026lsquo;밑바닥부터 시작하는 딥러닝 2\u0026rsquo; 교재를 참고했습니다.\n추론 기반 기법과 신경망 단어를 벡터로 표현하는 방법은 통계 기반 기법과 추론 기반 기법이 있다. 둘 모두 분포 가설을 기반으로 한다.\n통계 기반 기법의 문제점 통계 기반 기법은 주변 단어의 빈도를 기초로 단어를 표현한다. 단어 수가 $N$개일 때, $N \\times N$이라는 거대한 행렬을 만들게 된다. 영어 어휘만 해도 100만 개에 가까운데, 그렇다면 1조개의 원소를 가진 행렬이 필요하다는 것이다.\n이렇게 통계 기반 기법처럼 학습 데이터를 한번에 처리하지 말고, 데이터를 작게 나눠 순차적으로 학습시키는 (미니배치 학습) 방법이 필요하다.\n추론 기반 기법의 개요 추론이란 주변 단어 (맥락)가 주어졌을 때, 무슨 단어가 들어갈 지 단어를 유추하는 것이다.\n맥락 정보를 입력받아 출현할 수 있는 단어들의 확률분포를 나타내는 모델을 만들고, 학습의 결과로 분산 표현을 얻는 것이 추론 기반 기법이다.\n신경망에서의 단어 처리 신경망은 단어를 그대로 처리할 수 없기 때문에, 단어를 고정된 길이의 벡터로 변환해야 한다. 이를 위해 가장 대표적으로 사용되는 방법이 원핫 인코딩(one-hot encoding)이다.\n단어(텍스트) 단어 ID 원핫 표현 you 0 (1, 0, 0, 0, 0, 0, 0, 0) goodbye 2 (0, 0, 1, 0, 0, 0, 0, 0) 위와 같이 단어는 텍스트, 단어 ID, 원핫 표현으로 나타낼 수 있다. 단어를 고정 크기의 원핫 표현으로 나타내게 되면 뉴런의 수를 고정할 수 있다.\n신경망을 구성하는 계층들이 벡터를 처리할 수 있으므로, 이제 단어를 신경망으로 처리할 수 있을 것이다.\n완전연결계층의 계산은 행렬 곱으로 수행할 수 있고. 행렬 곱은 넘파이의 np.matmul()로 할 수 있다.\nCBOW CBOW 모델의 추론 처리 CBOW 모델은 맥락으로부터 타깃을 추측하는 용도의 신경망이다. (타깃은 중앙 단어, 맥락은 주변 단어를 의미한다)\n입력층이 2개 있고, 은닉층을 거쳐 출력층에 도달한다.\n두 입력층에서 은닉층으로의 변환은 완전연결계층이 수행한다. 그리고 은닉층에서 출력층 뉴런으로의 변환은 다른 완전연결계층이 처리한다. 입력층이 여러 개이면 전체를 평균하면 된다.\n출력층의 뉴런은 총 7개인데, 이 뉴런 하나하나가 각각의 단어에 대응한다. 출력층 뉴런은 각 단어의 점수를 뜻하며, 값이 높을수록 대응 단어의 출현 확률도 높아진다. 이 점수에 소프트맥스 함수를 적용해서, 확률을 얻을 수 있다.\n학습을 진행할수록 맥락에서 출현하는 단어를 잘 추측하는 방향으로 이 분산 표현들이 갱신된다. 이렇게 얻은 벡터에는 단어의 의미도 포함되어 있다.\n이제 CBOW 모델의 추론 처리를 파이썬으로 구현해 보자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class MatMul: def __init__(self, W): self.params = [W] self.grads = [np.zeros_like(W)] self.x = None def forward(self, x): W, = self.params out = np.dot(x, W) self.x = x return out def backward(self, dout): W, = self.params dx = np.dot(dout, W.T) dW = np.dot(self.x.T, dout) self.grads[0][...] = dW return dx MatMul 계층은 내부에서 행렬 곱을 계산한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 샘플 맥락 데이터 c0 = np.array([[1, 0, 0, 0, 0, 0, 0]]) c1 = np.array([[0, 0, 1, 0, 0, 0, 0]]) # 가중치 초기화 W_in = np.random.randn(7, 3) W_out = np.random.randn(3, 7) # 계층 생성 in_layer0 = MatMul(W_in) in_layer1 = MatMul(W_in) out_layer = MatMul(W_out) # 순전파 h0 = in_layer0.forward(c0) h1 = in_layer1.forward(c1) h = 0.5 * (h0 + h1) s = out_layer.forward(h) print(s) CBOW 모델은 활성화 함수를 사용하지 않는 간단한 구성의 신경망이다.\nCBOW 모델의 학습 모델이 올바른 예측을 할 수 있도록 가중치를 조정해야 한다.\n소프트맥스 함수를 이용 해 점수를 확률로 변환하고, 그 확률과 정답 레이블로부터 교차 엔트로피 오차를 구한 후, 그 값을 손실로 사용해 학습을 진행한다.\n앞서 구현한 추론 처리를 수행하는 CBOW 모델에 Softmax 계층과 Cross Entropy Error 계층을 추가하기만 하면 된다.\nword2vec의 가중치와 분산 표현 word2vec에서 사용되는 신경망에는 두 가지 가중치가 존재한다.\n입력 측 가중치 $ W_\\text{in} $: 입력 측 완전연결계층의 가중치로, 각 행은 해당 단어의 분산 표현을 나타낸다.\n출력 측 가중치 $ W_\\text{out} $: 출력 측 완전연결계층의 가중치로, 단어의 의미가 인코딩된 벡터가 각 열에 저장된다.\n일반적으로 word2vec에서는 입력 측 가중치 $ W_\\text{in} $만을 최종 단어의 분산 표현으로 사용한다. 출력 측 가중치는 대부분의 연구에서 버려진다.\n두 가중치는 하나만 이용할 수도 있고, 둘을 합쳐서 사용할 수도 있다.\nword2vec의 skip-gram 등 많은 연구에서는 입력 측의 가중치만 사용하고, GloVe에서는 두 가중치를 더하여 사용한다.\n학습 데이터 준비 \u0026ldquo;You say goodbye and I say hello\u0026rdquo; 문장을 이용해 학습을 진행해 보자.\n문장을 전처리하는 preprocess 함수는 여기를 참고하자.\n맥락과 타깃 word2vec에서 이용하는 신경망의 입력은 맥락이다. 그 정답 레이블은 중앙 단어인 타깃 이다. 우리는 맥락을 입력했을 때, 타깃을 출력할 확률이 높아지도록 학습시키면 된다.\n맥락과 타깃을 만드는 함수를 구현해 보자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 def create_contexts_target(corpus, window_size=1): target = corpus[window_size:-window_size] contexts = [] for idx in range(window_size, len(corpus)-window_size): cs = [] for t in range(-window_size, window_size + 1): if t == 0: continue cs.append(corpus[idx + t]) contexts.append(cs) return np.array(contexts), np.array(target) 원핫 벡터로 변환 맥락과 타깃을 단어 ID에서 원핫 표현으로 변환하기 위해, 원핫 벡터로 변환하는 함수를 구현해 보자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def convert_one_hot(corpus, vocab_size): N = corpus.shape[0] if corpus.ndim == 1: one_hot = np.zeros((N, vocab_size), dtype=np.int32) for idx, word_id in enumerate(corpus): one_hot[idx, word_id] = 1 elif corpus.ndim == 2: C = corpus.shape[1] one_hot = np.zeros((N, C, vocab_size), dtype=np.int32) for idx_0, word_ids in enumerate(corpus): for idx_1, word_id in enumerate(word_ids): one_hot[idx_0, idx_1, word_id] = 1 return one_hot CBOW 모델 구현 그럼 이제 모델을 구현해 보자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class SimpleCBOW: def __init__(self, vocab_size, hidden_size): V, H = vocab_size, hidden_size # 가중치 초기화 W_in = 0.01 * np.random.randn(V, H).astype(\u0026#39;f\u0026#39;) W_out = 0.01 * np.random.randn(H, V).astype(\u0026#39;f\u0026#39;) # 계층 생성 self.in_layer0 = MatMul(W_in) self.in_layer1 = MatMul(W_in) self.out_layer = MatMul(W_out) self.loss_layer = SoftmaxWithLoss() # 모든 가중치와 기울기를 리스트에 모은다. layers = [self.in_layer0, self.in_layer1, self.out_layer] self.params, self.grads = [], [] for layer in layers: self.params += layer.params self.grads += layer.grads # 인스턴스 변수에 단어의 분산 표현을 저장한다. self.word_vecs = W_in def forward(self, contexts, target): h0 = self.in_layer0.forward(contexts[:, 0]) h1 = self.in_layer1.forward(contexts[:, 1]) h = (h0 + h1) * 0.5 score = self.out_layer.forward(h) loss = self.loss_layer.forward(score, target) return loss def backward(self, dout=1): ds = self.loss_layer.backward(dout) da = self.out_layer.backward(ds) da *= 0.5 self.in_layer1.backward(da) self.in_layer0.backward(da) return None 학습코드구현 학습 데이터를 준비해 신경망에 입력한 다음, 기울기를 구하고 가중치 매개변수를 순서대로 갱신해보자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 window_size = 1 hidden_size = 5 batch_size = 3 max_epoch = 1000 text = \u0026#39;You say goodbye and I say hello.\u0026#39; corpus, word_to_id, id_to_word = preprocess(text) vocab_size = len(word_to_id) contexts, target = create_contexts_target(corpus, window_size) target = convert_one_hot(target, vocab_size) contexts = convert_one_hot(contexts, vocab_size) model = SimpleCBOW(vocab_size, hidden_size) optimizer = Adam() trainer = Trainer(model, optimizer) trainer.fit(contexts, target, max_epoch, batch_size) trainer.plot() word_vecs = model.word_vecs for word_id, word in id_to_word.items(): print(word, word_vecs[word_id]) Optimizer는 Adam을 사용해서 학습시켰다.\n학습을 거듭할수록 손실이 줄어들고 있다. 그럼 이제 학습이 끝난 후의 가중치 매개변수를 확인해 보자.\n1 2 3 4 5 6 7 you [ 1.001226 1.0100921 -1.0480953 -1.1371888 1.4559563] say [-1.1566567 -1.176362 1.1436371 1.10196 0.29608265] goodbye [ 0.93248147 0.8733082 -0.8807387 -0.79100204 0.47581592] and [-0.77025014 -0.7765116 0.7260802 0.86010003 1.9209603 ] i [ 0.9177614 0.86838746 -0.8880995 -0.7853987 0.48234403] hello [ 0.97636664 1.0043367 -1.0315654 -1.1388433 1.4567573 ] . [-1.2583737 -1.2487313 1.2197953 1.20546 -1.6672388] 이제 드디어 단어를 밀집벡터로 나타낼 수 있게 되었다. 이 밀집 벡터가 바로 단어의 분산 표현이다.\n학습이 잘 이루어졌으니, 이 분산 표현은 단어의 실제 의미를 담고 있을 것이다.\n여태 구현한 CBOW 모델은 처리 효율 면에서 문제가 있다. 이제 그걸 개선해 보자.\nword2vec 보충 CBOW 모델과 확률 사건 A가 일어날 확률은 $P(A)$와 같이 표기하고, A와 B가 동시에 일어날 확률은 $P(A, B)$와 같이 표기한다. 사건 B가 일어났을 때 사건 A가 일어날 확률은 $P(A|B)$와 같이 표기한다.\n맥락으로 $w_{t-1}$과 $w_{t+1}$ 이 주어졌을 때 $w_t$가 일어날 확률은\n$$ P(w_t | w_{t-1}, w_{t+1}) \\tag 1 $$\n식 1과 같이 쓸 수 있다. 이는 CBOW를 모델링 하는 식이다.\n교차 엔트로피 오차 식은 $L = - \\sum_k t_k \\log y_k$이다. $y_k$는 $k$번째에 해당하는 사건이 일어날 확률을 의미하고, $t_k$는 정답 레이블로 원핫 벡터로 표현된다. 여기서 문제의 정답은 $w_i$가 발생하는 것이므로 $w_i$에 해당하는 원소만 1이고 나머지는 0이 된다. 이 점을 활용하여, 다음 식을 유도할 수 있다.\n$$ L = - \\log P(w_t \\ | \\ w_{t-1}, \\ w_{t+1}) \\tag 2 $$\n식 2는 음의 로그 가능도라고 부른다. 이처럼 CBOW 모델의 손실 함수는 식 1의 확률에 $\\log$를 취한 다음 마이너스를 붙인 것이다. 이는 샘플 데이터 하나에 대한 손실 함수이고, 이를 corpus 전체로 확장시키면 아래 식 3과 같다.\n$$ L = - \\frac{1}{T} \\sum_{t=1}^T \\log P(w_t \\ | \\ w_{t-1}, \\ w_{t+1}) \\tag 3 $$\nCBOW 모델의 학습이 수행하는 일은 이 손실 함수의 값을 가능한 한 작게 만드는 것이다.\nskip-gram 모델 skip-gram은 CBOW에서 다루는 맥락과 타깃을 역전시킨 모델이다.\nCBOW 모델은 맥락이 여러 개 있고, 그 여러 맥락으로부터 타깃을 추측한다. 반면에 skip-gmm 모델은 중앙의 타깃으로부터 주변의 맥락을 추측한다.\nskip-gram 모델을 확률 표기로 나타내면 아래 식 4와 같다.\n$$ P(w_{t-1}, \\ w_{t+1} \\ | \\ w_t) \\tag 4 $$\n조건부 독립이라고 가정하고 (맥락의 단어 사이에 관련성이 없다고 가정하고) 식 4를 아래 식 5와 같이 분해한다.\n$$ P(w_{t-1}, \\ w_{t+1} \\ | \\ w_t) = P(w_{t-1} \\ | \\ w_{t}) \\ P(w_{t+1} \\ | \\ w_{t}) \\tag 5 $$\n위 식을 교차 엔트로피 오차에 적용하고, corpus 전체로 확장시키면 아래 식 6이 된다.\n$$ L = - \\frac{1}{T} \\sum_{t=1}^T ( \\log P(w_{t-1} \\ | \\ w_{t})+ P(w_{t+1} \\ | \\ w_{t})) \\tag 6 $$\nskip-gram 모델은 맥락의 수만큼 추측하기 때문에 그 손실 함수는 각 맥락에서 구한 손실의 총합이어야 하는 반면, CBOW 모델은 타깃 하나의 손실을 구한다.\n단어 분산 표현의 정밀도 면에서 skip-gram 모델의 결과가 더 좋은 경우가 많고, corpus가 클 수록 성능 면에서 skip-gram이 뛰어난 경향이 있다.\n통계 기반 vs 추론 기반 통계 기반 기법과 추론 기반 기법인 word2vec은 학습과 갱신 방식에서 차이를 보인다.\n통계 기반은 새 단어 추가 시 처음부터 다시 계산해야 하지만, word2vec은 기존 가중치를 활용해 효율적으로 갱신할 수 있다. 단어의 유사성과 복잡한 패턴 인코딩에서도 word2vec이 더 복잡한 관계를 파악할 수 있으며, \u0026lsquo;king - man + woman = queen\u0026rsquo; 같은 유추 문제를 풀 수 있다.\n그러나 실제 유사성 평가에서는 두 기법 간 우열을 가리기 어렵다. 추론 기반과 통계 기반은 서로 관련되어 있으며, 이를 바탕으로 추론 기반과 통계 기반을 융합한 GloVe 기법이 등장하여 두 방법의 장점을 결합했다.\n","date":"2023-12-19T00:00:00Z","image":"https://gyeongmin.kr/images/deep-learning-from-scratch.jpeg","permalink":"https://gyeongmin.kr/p/word2vec/","title":"word2vec을 이용한 단어 임베딩"},{"content":"소수와 약수, 배수의 정의 소수는 영어로 prime 이라고 한다. 또한, $p$가 소수일 때 $p^k$를 prime power 라고 한다.\n정수 $a, b$가 있을 때 $a$가 $b$의 약수라는 것은 정수 $n$이 있어 $b=an$이라는 것이다. 이때 $b$를 $a$의 배수라고 한다.\n자연수 $p\\geq2$의 약수가 $1$과 $p$ 뿐이면, $p$를 소수라고 부른다.\n소수 판별 알고리즘 소수의 정의만 가지고 단순히 생각해 보자. 주어진 수 $N$을 $1$부터 $N$까지 나누어 보고, 나누어 떨어지는 수가 2개라면 소수이다.\n다르게 말한다면, $N$을 $2$부터 $N-1$까지의 수로 나누어 봤을 때 나누어 떨어지는 경우가 있으면 소수가 아니고, 나누어 떨어지는 경우가 없으면 소수라는 것이다.\n1 2 3 4 5 6 7 8 9 10 11 int isPrime(int n) { if (n \u0026lt; 2) { return false; } for (int i = 2; i \u0026lt; n; i++) { if (n % i == 0) { return false; } } return true; } 위 코드는 주어진 수가 소수인지 판별하는 c++ 코드이다. 2부터 검사하기에, 1이 들어오는 경우를 예외처리 해 주어야 한다.\n위 코드는 $2$부터 $N-1$ 까지의 모든 수를 계산해 본다. 따라서 시간복잡도는 $O(N)$이다.\n소수 판별 알고리즘 최적화 합성수 $N$에서 $1$을 제외한 가장 작은 약수를 $d$라고 하자. $\\frac{N}{d}$도 $1$이 아닌 $N$의 약수이므로, $d \\leq\\frac{N}{d}$ 이다. 우변의 $d$를 이항시키면 $d^2 \\leq N$이고, 둘 다 자연수이므로 $d \\leq \\sqrt{N}$이다.\n즉, 소수를 판별할 때 2부터 $\\sqrt{N}$ 까지만 검사 해 주어도 충분하다.\n반복문의 범위를 (int i = 2; i \u0026lt;= sqrt(n); i++)로 해도 괜찮지만, sqrt 함수는 실수연산이기에 오차가 발생할 수 있다. 따라서 for (int i = 2; i*i \u0026lt;= n; i++)으로 작성하는 것을 권장한다.\n1 2 3 4 5 6 7 8 9 10 11 int isPrime(int n) { if (n \u0026lt; 2) { return false; } for (int i = 2; i*i \u0026lt;= n; i++) { if (n % i == 0) { return false; } } return true; } 위 코드의 시간복잡도는 $O(\\sqrt{N})$이다.\n소수 판별 예제 문제 https://www.acmicpc.net/problem/1978\nhttps://www.acmicpc.net/problem/2581\n약수 구하기 알고리즘 소수 판별 알고리즘을 조금만 응용하면 약수를 구할 수 있다.\n$1$부터 $\\sqrt{n}$까지 나누어 떨어질 때마다 벡터에 약수를 넣어 주면 된다.\n자연수 $d$가 $n$의 약수라면, $n/d$ 역시 $n$의 약수라는 것을 잊지 말자. 이 때, $n=m^2$이 제곱수인 경우 $\\sqrt{n}=m$이 약수로 두 번 세지는 경우를 조심하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #include \u0026lt;iostream\u0026gt; #include \u0026lt;algorithm\u0026gt; #include \u0026lt;vector\u0026gt; using namespace std; int main() { int n; cin \u0026gt;\u0026gt; n; vector\u0026lt;int\u0026gt; v; for (int i = 1; i * i \u0026lt;= n; i++) { if (n % i == 0) { v.push_back(i); int d = n / i; if (i != d) v.push_back(d); } } sort(v.begin(), v.end()); cout \u0026lt;\u0026lt; n \u0026lt;\u0026lt; \u0026#34;의 약수: \u0026#34;; for (int i : v) { cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } } $O(\\sqrt{N})$의 시간복잡도 내에 약수를 모두 구할 수 있다.\n약수를 구한 이후 정렬을 해 주고 있다. STL 정렬의 시간복잡도가 $O(NlogN)$이라는 것은 모두 알고 있을 것이다.\nlong long 범위 내 약수의 개수의 상한선은 $\\sqrt[3]{N}$개라고 어림짐작해볼 수 있다. 따라서 약수의 개수가 아무리 많아도 정렬에 쓰이는 시간복잡도는 $O(\\sqrt[3]{N}log({\\sqrt[3]{N}}))$이하이다.\n양수 범위에서 $\\sqrt[3]{N}log({\\sqrt[3]{N}}) \u0026lt;\\sqrt{N}$임이 자명하므로, 위 코드의 시간복잡도는 $O(\\sqrt{N})$이다.\n약수 구하기 알고리즘 최적화 위 코드를 조금만 개선하면 정렬을 할 필요가 없다.\nv.push_back(d); 대신 다른 벡터에 담아둔 뒤, 역순으로 넣어주면 정렬된 약수를 구할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #include \u0026lt;algorithm\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; using namespace std; int main() { int n; cin \u0026gt;\u0026gt; n; vector\u0026lt;int\u0026gt; divisor; vector\u0026lt;int\u0026gt; tmp; for (int i = 1; i * i \u0026lt;= n; i++) { if (n % i == 0) { divisor.push_back(i); int d = n / i; if (i != d) tmp.push_back(d); } } divisor.insert(divisor.end(), tmp.rbegin(), tmp.rend()); cout \u0026lt;\u0026lt; n \u0026lt;\u0026lt; \u0026#34;의 약수: \u0026#34;; for (int i : divisor) { cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } } 약수 구하기 예제 문제 https://www.acmicpc.net/problem/2501\nhttps://www.acmicpc.net/problem/9506\n범위 내 소수를 모두 구하는 알고리즘 기존 방법에서 반복문을 돌려 소수를 모두 찾으려 한다면, 범위가 조금만 커져도 시간초과가 뜰 것이다. 범위 내 소수를 모두 구해야 한다면, 에라토스테네스의 체를 활용하면 된다.\n에라토스테네스의 체 알고리즘과 최적화에 대해서는 여기를 참고하고, 이번에는 간단하게 코드만 보고 넘어가자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 vector\u0026lt;int\u0026gt; findPrime(int max) { vector\u0026lt;int\u0026gt; prime; vector\u0026lt;bool\u0026gt; check(max + 1, false); prime.push_back(2); for (int t = 3; t \u0026lt;= max; t += 2) { if (!check[t]) { prime.push_back(t); for (int i = t * t; i \u0026lt;= max; i += t) { check[i] = true; } } } return prime; } 소인수 분해 알고리즘 여태까지 잘 따라왔다면, 소인수분해도 간단하게 할 수 있을 것이다. factorize 함수는 주어진 $N$을 효율적으로 소인수분해 하는 함수이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #define ull unsigned long long using namespace std; vector\u0026lt;ull\u0026gt; factorize(ull n) { vector\u0026lt;ull\u0026gt; factors; for (int i = 2; i * i \u0026lt;= n; i++) { while (n % i == 0) { factors.push_back(i); n /= i; } } if (n \u0026gt; 1) { factors.push_back(n); } return factors; } int main() { ull n; cin \u0026gt;\u0026gt; n; vector\u0026lt;ull\u0026gt; factors = factorize(n); cout \u0026lt;\u0026lt; n \u0026lt;\u0026lt; \u0026#34;의 소인수분해 : \u0026#34;; for (int i = 0; i \u0026lt; factors.size(); i++) { cout \u0026lt;\u0026lt; factors[i] \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } if (factors.size() == 1) { cout \u0026lt;\u0026lt; \u0026#34;(소수)\u0026#34;; } } 번외 알고리즘 정수론 파트에는 어렵지만 재미있는 알고리즘들이 많다. 굳이 알 필요는 없지만, 번외로 준비해 봤다.\n폴라드 로 소인수분해 int 범위 까지는 괜찮은데, long long 범위의 수를 위 소인수분해 알고리즘으로 계산하면 TLE가 난다.\n이런 문제들은 범위가 $1 \\leq N \\leq 10^{18}$인데, 밀러-라빈 소수 판정법을 이용하여 폴라드 로 소인수분해 알고리즘을 구현하여 소인수분해한 후, DFS 등으로 약수를 구해주어야 한다.\n나중에 시간이 되면 위 알고리즘도 다룰 예정이다.\n소수 계량 함수 소수 계량 함수 $\\pi(n)$은 $n$ 이하의 소수 개수를 나타내는 함수이다. 이 함수에 대해 다음 식이 성립한다.\n$$\\pi(n) \\approx \\frac{n}{\\ln(n)}$$\n예를 들어 $\\pi(10^6)$의 근삿값은 $72382$고, 정확한 값은 $78498$이다.\n이 함수를 이용하면 소수의 개수와 관련된 문제에서 시간복잡도를 간접적으로 구할 수 있다.\n","date":"2023-12-03T00:00:00Z","image":"https://gyeongmin.kr/images/algorithm.png","permalink":"https://gyeongmin.kr/p/prime-number-and-divisors/","title":"소수 판별과 약수 구하기"},{"content":" 문제 링크 : https://www.acmicpc.net/problem/1331\n문제 나이트 투어는 체스판에서 나이트가 모든 칸을 정확히 한 번씩 방문하며, 마지막으로 방문하는 칸에서 시작점으로 돌아올 수 있는 경로이다. 다음 그림은 나이트 투어의 한 예이다.\n영식이는 6×6 체스판 위에서 또 다른 나이트 투어의 경로를 찾으려고 한다. 체스판의 한 칸은 A, B, C, D, E, F 중에서 하나와 1, 2, 3, 4, 5, 6 중에서 하나를 이어 붙인 것으로 나타낼 수 있다. 영식이의 나이트 투어 경로가 주어질 때, 이것이 올바른 것이면 Valid, 올바르지 않으면 Invalid를 출력하는 프로그램을 작성하시오.\n입력 36개의 줄에 나이트가 방문한 순서대로 입력이 주어진다. 체스판에 존재하는 칸만 입력으로 주어진다.\n출력 첫째 줄에 문제의 정답을 출력한다.\n풀이 두 지점에 대한 x와 y의 좌표 차이가 각각 2,1 또는 1,2면 나이트의 움직임과 같다. 마지막으로 방문하는 칸에서 시작점으로 돌아올 수 있어야 하기 때문에, 시작점과 끝점이 나이트의 움직임인지 검사한다. 모든 지점 $p$에 대해 반복하며, $p$와 그 다음 지점 $p+1$ 간의 움직임을 검사하고, 이미 방문한 지점을 또 방문하는지 검사한다. 모든 검사 조건을 통과하면 Valid이다. 소스 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 #include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; typedef struct pos { int r, c; } pos; bool check[6][6]; vector\u0026lt;pos\u0026gt; v(36); bool is_knight_move(pos a, pos b) { bool ret = false; int dy = abs(a.r - b.r); int dx = abs(a.c - b.c); if (dy == 2 \u0026amp;\u0026amp; dx == 1) ret = true; if (dy == 1 \u0026amp;\u0026amp; dx == 2) ret = true; return ret; } string solve() { if (!is_knight_move(v.front(), v.back())) return \u0026#34;Invalid\u0026#34;; pos post = {-1, -1}; for (pos \u0026amp;now : v) { bool \u0026amp;visit = check[now.r][now.c]; if (visit) return \u0026#34;Invalid\u0026#34;; else visit = true; if (post.r != -1 \u0026amp;\u0026amp; !is_knight_move(now, post)) return \u0026#34;Invalid\u0026#34;; post = now; } return \u0026#34;Valid\u0026#34;; } int main() { ios::sync_with_stdio(false), cin.tie(nullptr); for (pos \u0026amp;p : v) { string s; cin \u0026gt;\u0026gt; s; p = {s[0] - \u0026#39;A\u0026#39;, s[1] - \u0026#39;1\u0026#39;}; } cout \u0026lt;\u0026lt; solve(); return 0; } ","date":"2023-11-30T00:00:00Z","image":"https://gyeongmin.kr/images/boj.png","permalink":"https://gyeongmin.kr/p/boj-1331/","title":"BOJ 1331: 나이트 투어 (C++)"},{"content":" 본 포스팅은 \u0026lsquo;밑바닥부터 시작하는 딥러닝 2\u0026rsquo; 교재를 참고했습니다.\n자연어 처리와 단어의 의미 자연어(Natural Language)란 우리가 평소에 사용하는 언어, 예를 들어 한국어나 영어를 말한다. 자연어 처리(NLP, Natural Language Processing)는 이러한 자연어를 컴퓨터가 이해하도록 만드는 기술 분야이다.\n우리의 말은 문자로 이루어져 있고, 말의 의미는 단어로 구성된다. 따라서 컴퓨터가 자연어를 이해하도록 하려면 우선 단어의 의미부터 이해시켜야 한다.\n시소러스 단어의 의미를 나타내는 가장 Naive한 방법\n사람이 직접 단어의 의미를 정의하는 방식으로, 쉽게 말해 \u0026lsquo;유의어 사전\u0026rsquo;이다.\ncar, auto, automobile은 모두 자동차를 나타낸다. 시소러스에서는 이러한 유의어/동의어를 한 그룹으로 분류한다.\ngraph LR car~~~auto~~~automobile 또한 단어 간의 상위/하위, 전체/부분 등 세세한 관계까지 정의하기도 한다.\nflowchart TD a[object] --\u003e b[mortor vehicle] b --\u003e d[go-cart] b --\u003e c[car] b --\u003e e[truck] c --\u003e f[suv] c --\u003e g[compact] c --\u003e h[hatch-back] WordNet 1985년 구축된 WordNet은 자연어 처리 분야에서 가장 유명한 시소러스이다.\nWordNet을 사용하면 유의어를 얻거나, 단어 네트워크를 사용해 단어 간의 유사도를 구할 수 있다.\n문제점 사람이 수작업으로 라벨링 해야하기에 여러 단점이 존재한다.\n시대 변화에 대응하기 어렵다.\n단어의 의미는 시간이 지남에 따라 변하기도 하고, 새로운 단어가 생기기도 한다. 비용이 많이 든다.\n영어 단어만 해도 1000만개가 넘으며, 이는 높은 인적 비용을 요구한다. 단어 간의 미묘한 차이를 표현할 수 없다.\n예를 들어 빈티지와 레트로의 경우 의미는 같지만, 용법은 다르다. 시소러스는 이러한 차이를 표현할 수 없다. 통계 기반 기법 통계 기반 기법을 사용하기 위해 우리는 말뭉치(corpus)를 이용할 것이다.\n말뭉치란 자연어처리 연구나 어플리케이션을 위해 수집된 대량의 텍스트 데이터로, 대표적인 말뭉치는 위키백과, 구글뉴스, 셰익스피어의 소설 등이 있다.\n말뭉치 전처리 작은 말뭉치를 전처리하는 과정을 살펴보자.\n1 2 3 4 5 \u0026gt;\u0026gt;\u0026gt; text = \u0026#39;You say goodbye and I say hello.\u0026#39; \u0026gt;\u0026gt;\u0026gt; text = text.lower() # 모두 소문자로 변환 \u0026gt;\u0026gt;\u0026gt; text = text.replace(\u0026#39;.\u0026#39;, \u0026#39; .\u0026#39;) # \u0026#39;.\u0026#39;을 \u0026#39; .\u0026#39;으로 변환 \u0026gt;\u0026gt;\u0026gt; text \u0026#39;you say goodbye and i say hello .\u0026#39; 모든 단어를 소문자로 변환하고, 단어의 마지막 점을 띄워줬다.\n1 2 3 \u0026gt;\u0026gt;\u0026gt; words = text.split() # 공백을 기준으로 나눔 \u0026gt;\u0026gt;\u0026gt; words [\u0026#39;you\u0026#39;, \u0026#39;say\u0026#39;, \u0026#39;goodbye\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;i\u0026#39;, \u0026#39;say\u0026#39;, \u0026#39;hello\u0026#39;, \u0026#39;.\u0026#39;] 공백을 기준으로 나눠, 리스트에 담았다.\n1 2 3 4 5 6 7 \u0026gt;\u0026gt;\u0026gt; word_to_id = {} \u0026gt;\u0026gt;\u0026gt; id_to_word = {} \u0026gt;\u0026gt;\u0026gt; for word in words: ... if word not in word_to_id: ... new_id = len(word_to_id) ... word_to_id[word] = new_id ... id_to_word[new_id] = word word_to_id 의 경우 key가 단어, value는 id이다. id_to_word는 그 반대이다.\n1 2 3 4 \u0026gt;\u0026gt;\u0026gt; id_to_word {0: \u0026#39;you\u0026#39;, 1: \u0026#39;say\u0026#39;, 2: \u0026#39;goodbye\u0026#39;, 3: \u0026#39;and\u0026#39;, 4: \u0026#39;i\u0026#39;, 5: \u0026#39;hello\u0026#39;, 6: \u0026#39;.\u0026#39;} \u0026gt;\u0026gt;\u0026gt; word_to_id {\u0026#39;you\u0026#39;: 0, \u0026#39;say\u0026#39;: 1, \u0026#39;goodbye\u0026#39;: 2, \u0026#39;and\u0026#39;: 3, \u0026#39;i\u0026#39;: 4, \u0026#39;hello\u0026#39;: 5, \u0026#39;.\u0026#39;: 6} 마지막으로 단어 목록을 단어 ID 목록으로 변환하면 된다.\n1 2 3 4 \u0026gt;\u0026gt;\u0026gt; import numpy as np \u0026gt;\u0026gt;\u0026gt; corpus = np.array([word_to_id[w] for w in words]) \u0026gt;\u0026gt;\u0026gt; corpus array([0, 1, 2, 3, 4, 1, 5, 6]) 이렇게 범주형 변수를 숫자로 바꾸는 것을 원 핫 인코딩(one-hot encodeing) 이라고 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def preprocess(text): text = text.lower() text = text.replace(\u0026#39;.\u0026#39;, \u0026#39; .\u0026#39;) words = text.split(\u0026#39; \u0026#39;) word_to_id = {} id_to_word = {} for word in words: if word not in word_to_id: new_id = len(word_to_id) word_to_id[word] = new_id id_to_word[new_id] = word corpus = np.array([word_to_id[w] for w in words]) return corpus, word_to_id, id_to_word 위 과정을 합쳐 단어를 전처리하는 preprocess 함수를 구현했다.\n분포 가설과 분산 표현 비슷한 위치에서 등장한 단어는 비슷한 의미를 가지지 않을까?\n\u0026ldquo;단어의 의미는 주변 단어에 의해 형성된다.\u0026rdquo; 라는 것을 분포 가설이라고 한다.\n단어 자체에는 의미가 없고, 그 단어가 사용 된 맥락이 의미를 형성한다는 것이다. 여기서 맥락이란 특정 단어를 중심에 둔 그 주변 단어를 말한다.\n좌우 모든 단어를 고려하며 계산하면 컴퓨팅 비용이 너무 많이 들기에, 우리는 특정 크기만큼만 고려할 것이다. 즉, 슬라이딩 윈도우를 적용할 것이다. \u0026lsquo;맥락의 크기\u0026rsquo;는 슬라이딩 윈도우의 사이즈와 같다.\n분산 표현 이란 분포 가설에 기반해 주변 단어의 분포를 기준으로 단어의 벡터 표현을 결정하는 것 이다.\n동시 행렬 발생 분포 가설에 기초해 단어를 벡터로 나타내 보자.\n가장 간단한 방법은 한 단어에 주목하여, 주변에 어떤 단어가 몇 번 등장했는지 계산하는 것이다. 이는 통계 기반 기법(statistical based)이라고 한다.\n{\u0026lsquo;you\u0026rsquo;: 0, \u0026lsquo;say\u0026rsquo;: 1, \u0026lsquo;goodbye\u0026rsquo;: 2, \u0026lsquo;and\u0026rsquo;: 3, \u0026lsquo;i\u0026rsquo;: 4, \u0026lsquo;hello\u0026rsquo;: 5, \u0026lsquo;.\u0026rsquo;: 6}\n예를 들어, \u0026lsquo;you say goodbye and i say hello .\u0026rsquo; 에서 \u0026lsquo;say\u0026rsquo;를 기준으로 살펴보자.\n\u0026lsquo;say\u0026rsquo; 좌우로 \u0026lsquo;you\u0026rsquo;, \u0026lsquo;goodbye\u0026rsquo;, \u0026lsquo;i\u0026rsquo;, \u0026lsquo;hello\u0026rsquo; 가 있다.\n이는 벡터 \u0026lsquo;[1, 0, 1, 0, 1, 1, 0]\u0026rsquo; 으로 표현 할 수 있을 것이다.\n이것을 모든 단어에 대해 적용시킨다면 아래와 같은 테이블을 얻을 수 있을 것이다.\nyou say goodbye and i hello . you 0 1 0 0 0 0 0 say 1 0 1 0 1 1 0 goodbye 0 1 0 1 0 0 0 and 0 0 1 0 1 0 0 i 0 1 0 1 0 0 0 hello 0 1 0 0 0 0 1 . 0 0 0 0 0 1 0 이것을 동시 발생 행렬 이라고 한다.\n동시 발생 행렬을 만드는 코드는 아래와 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def create_co_matrix(corpus, vocab_size, window_size=1): corpus_size = len(corpus) co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32) for idx, word_id in enumerate(corpus): for i in range(1, window_size + 1): left_idx = idx - i right_idx = idx + i if left_idx \u0026gt;= 0: left_word_id = corpus[left_idx] co_matrix[word_id, left_word_id] += 1 if right_idx \u0026lt; corpus_size: right_word_id = corpus[right_idx] co_matrix[word_id, right_word_id] += 1 return co_matrix 벡터간 유사도 앞서 구한 행렬을 통해 벡터 간의 유사도를 구한다면 단어 간의 유사도를 구할 수 있을 것이다.\n벡터의 유사도를 측정하는 대표적인 방법으로는 벡터의 내적이나 유클리드 거리, 코사인 유사도가 있다. 이 중, 우리는 코사인 유사도를 사용할 것이다.\n$$ \\tag{1} \\text{similarity}(A, B)=\\frac{A⋅B}{||A||\\ ||B||}=\\frac{\\sum_{i=1}^{n}{A_{i}B_{i}}}{\\sqrt{\\sum_{i=1}^{n}(A_{i})^2}\\sqrt{\\sum_{i=1}^{n}(B_{i})^2}} $$\n[식 1]의 분자에는 벡터의 내적이, 분모에는 각 벡터의 노름(norm)이 등장한다. 노름은 벡터의 크기를 나타낸 것으로, 여기선 L2 노름을 계산한다.\n코사인 유사도는 두 벡터가 가르키는 방향이 얼마나 유사한지를 나타낸다. 방향이 같으면 1, 반대면 -1이다.\n파이썬 코드로는 아래와 같이 나타낼 수 있다.\n1 2 3 4 def cos_similarity(x, y, eps=1e-8): nx = x / (np.sqrt(np.sum(x ** 2)) + eps) # x의 정규화 ny = y / (np.sqrt(np.sum(y ** 2)) + eps) # y의 정규화 return np.dot(nx, ny) 0으로 나누어 오류가 나는 일이 없도록 $10^{-8}$ 이라는 작은 값을 더해주는 것을 볼 수 있다.\n유사 단어의 랭킹 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def most_similar(query, word_to_id, id_to_word, word_matrix, top=5): if query not in word_to_id: print(\u0026#39;%s is not found\u0026#39; % query) return print(\u0026#39;\\n[query] \u0026#39; + query) query_id = word_to_id[query] query_vec = word_matrix[query_id] vocab_size = len(id_to_word) similarity = np.zeros(vocab_size) for i in range(vocab_size): similarity[i] = cos_similarity(word_matrix[i], query_vec) count = 0 for i in (-1 * similarity).argsort(): if id_to_word[i] == query: continue print(\u0026#39; %s: %s\u0026#39; % (id_to_word[i], similarity[i])) count += 1 if count \u0026gt;= top: return 위 코드로 \u0026lsquo;you\u0026rsquo; 와 유사한 단어를 찾아보자.\nValue goodbye 0.7071067691154799 i 0.7071067691154799 hello 0.7071067691154799 say 0.0 and 0.0 \u0026lsquo;goodbye\u0026rsquo;, \u0026lsquo;i\u0026rsquo;, \u0026lsquo;hello\u0026rsquo;의 경우 \u0026lsquo;say\u0026rsquo;나 \u0026lsquo;and\u0026rsquo;에 비해 유사하다고 볼 수 있다.\n통계 기반 기법의 개선 상호정보량 발생 횟수는 좋은 특징이 아니다\n동시 발생 행렬은 두 단어가 동시에 발생한 빈도를 측정한다. 하지만 이것만으로는 부족하다. \u0026rsquo;the\u0026rsquo;, \u0026rsquo;this\u0026rsquo;처럼 고빈도 단어 의 경우를 생각해 보자.\n\u0026lsquo;drive\u0026rsquo;, \u0026rsquo;the\u0026rsquo; 중에 \u0026lsquo;car\u0026rsquo;와 더 유사한 단어는 무엇인가? 모두 \u0026lsquo;drive\u0026rsquo;와 유사한 단어로 \u0026lsquo;car\u0026rsquo;를 고를 것이다.\n하지만 동시 발생 빈도는 \u0026rsquo;the\u0026rsquo;가 압도적으로 높을 것이다. 동시 발생 행렬에서는 \u0026rsquo;the\u0026rsquo; 자체가 문서에서 더 많이 등장하기에, 더 높은 유사성을 갖는다고 잘못 평가할 수 있다.\n이 문제를 해결하기 위해 점별 상호정보량(PMI, Pointwise Mutual Information) 이라는 척도를 사용할 것이다.\nPMI는 확률 변수 $x$와 $y$에 대해 다음과 같은 식으로 정의된다.\n$$ \\tag{2} \\text{PMI}(x,y)=\\log_2\\frac{P(x,y)}{P(x)P(y)} $$\n[식 2]에서 $P(x)$는 $x$가 일어날 확률, $P(y)$는 $y$가 일어날 확률, $P(x,y)$는 $x, y$가 동시에 일어날 확률이다. PMI가 높을 수록 관련성이 높다는 의미이다.\n자연어 처리에서 $P(x)$는 말뭉치에서 $x$라는 단어가 등장할 확률이다. 예를 들어, 단어 100,000개의 말뭉치에서 \u0026rsquo;the\u0026rsquo;라는 단어가 100번 등장했다면, $P(`\\text{the}\u0026rsquo;) = 0.0001$이다.\n하지만 PMI도 문제가 있다. 동시 발생 횟수가 0이라면 PMI 값은 $-\\infty$가 된다.\n따라서 PPMI(Positive PMI) 라는 척도를 쓴다. 이는 다음과 같다.\n$$ \\tag{3} \\text{PPMI}(x, y) = \\max(0, \\text{PMI}(x,y)) $$\n[식 3]을 보면, PPMI는 PMI값이 음수면 0으로 취급한다는 것을 확인할 수 있다.\n이제 PPMI를 파이썬으로 구현해 보자.\n1 2 3 4 5 6 7 8 9 10 11 def ppmi(C, eps = 1e-8): M = np.zeros_like(C, dtype=np.float32) N = np.sum(C) S = np.sum(C, axis=0) for i in range(C.shape[0]): for j in range(C.shape[1]): pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps) M[i, j] = max(0, pmi) return M 이제 동시 발생 행렬을 PPMI로 변환해 보자.\n1 2 3 4 5 6 7 8 9 10 11 12 text = \u0026#39;You say goodbye and I say hello.\u0026#39; corpus, word_to_id, id_to_word = preprocess(text) vocab_size = len(word_to_id) C = create_co_matrix(corpus, vocab_size) W = ppmi(C) np.set_printoptions(precision=3) # 유효 자릿수를 세 자리로 표시 print(\u0026#39;동시발생 행렬\u0026#39;) print(C) print(\u0026#39;-\u0026#39;*50) print(\u0026#39;PPMI\u0026#39;) print(W) 위 코드를 실행시킨 결과는 아래와 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 동시발생 행렬 [[0 1 0 0 0 0 0] [1 0 1 0 1 1 0] [0 1 0 1 0 0 0] [0 0 1 0 1 0 0] [0 1 0 1 0 0 0] [0 1 0 0 0 0 1] [0 0 0 0 0 1 0]] -------------------------------------------------- PPMI [[0. 1.807 0. 0. 0. 0. 0. ] [1.807 0. 0.807 0. 0.807 0.807 0. ] [0. 0.807 0. 1.807 0. 0. 0. ] [0. 0. 1.807 0. 1.807 0. 0. ] [0. 0.807 0. 1.807 0. 0. 0. ] [0. 0.807 0. 0. 0. 0. 2.807] [0. 0. 0. 0. 0. 2.807 0. ]] 이제 더 좋은 단어 벡터를 얻었다.\n하지만 아직 문제점이 있다. 벡터의 크기가 너무 크다는 것이다. 단어의 개수가 10만개라면, 벡터의 차운 수도 10만이 된다.\n또한, 대부분 0으로 구성된 희소행렬(Sparse Matrix)이다.\n이는 매우 비효율적이고, 노이즈에 취약하다.\n차원 축소 차원 축소는 중요한 정보는 최대한 유지하되, 벡터의 차원을 줄이는 것이다. 그 중 특잇값 분해를 적용해보자.\n특잇값 분해에 대한 자세한 설명은 여기 블로그를 참고하자.\n특잇값 분해를 사용한 파이썬 코드는 아래와 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 text = \u0026#39;You say goodbye and I say hello.\u0026#39; corpus, word_to_id, id_to_word = preprocess(text) vocab_size = len(id_to_word) C = create_co_matrix(corpus, vocab_size, window_size=1) W = ppmi(C) U, S, V = np.linalg.svd(W) np.set_printoptions(precision=3) print(C[0]) print(W[0]) print(U[0]) for word, word_id in word_to_id.items(): plt.annotate(word, (U[word_id, 0], U[word_id, 1])) plt.scatter(U[:,0], U[:,1], alpha=0.5) plt.show() 위 코드는 동시발생 행렬에 SVD를 적용한 후 각 단어를 2차원 벡터로 변환한 것을 시각화 한 것이다.\nPTB 데이터셋 평가 이번에는 많은 양의 데이터를 처리해야 하므로, sklearn의 고속 SVD를 사용하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from dataset import ptb from sklearn.utils.extmath import randomized_svd window_size = 2 wordvec_size = 100 corpus, word_to_id, id_to_word = ptb.load_data(\u0026#39;train\u0026#39;) vocab_size = len(word_to_id) C = create_co_matrix(corpus, vocab_size, window_size) W = ppmi(C, verbose=True) U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5) word_vecs = U[:, :wordvec_size] querys = [\u0026#39;you\u0026#39;, \u0026#39;year\u0026#39;, \u0026#39;car\u0026#39;, \u0026#39;toyota\u0026#39;] for query in querys: most_similar(query, word_to_id, id_to_word, word_vecs, top=5) 이제 드디어 단어의 의미를 벡터로 잘 인코딩했다.\n말뭉치를 사용해 맥락에 속한 단어의 등장 횟수를 센 후 PPMI 행렬로 변환하고, 다시 SVD를 이용해 차원을 감소시킴으로써 더 좋은 단어 벡터를 얻어냈다.\n이것이 단어의 분산 표현이고, 각 단어는 고정 길이의 밀집벡터로 표현되었다.\n단어의 벡터 공간에서는 의미가 가까운 단어는 그 거리도 가깝다.\n","date":"2023-11-27T00:00:00Z","image":"https://gyeongmin.kr/images/deep-learning-from-scratch.jpeg","permalink":"https://gyeongmin.kr/p/word-distributed-representation/","title":"단어의 분산 표현"},{"content":" 본 포스팅은 \u0026lsquo;밑바닥부터 시작하는 딥러닝 2\u0026rsquo; 교재를 참고했습니다.\n신경망의 학습 학습되지 않은 신경망은 좋은 추론을 할 수 없다. 따라서 학습을 먼저 수행하고, 학습된 매개변수를 이용해 추론을 수행해야 한다.\n손실 함수 신경망 학습에는 학습이 얼마나 잘 되고 있는지를 알기 위한 척도가 필요하다.\n손실이란 신경망이 예측한 결과를 비교하여 예측이 얼마나 나쁜가를 산출한 스칼라 값으로, 성능을 나타내는 척도이다.\n이것을 구하는 것이 바로 손실 함수이다.\n우리는 소프트맥스와 교차 엔트로피 오차를 통해 손실 함수를 구현할 것이다.\n소프트맥스 $$ p_k = \\frac{\\exp{(s_k)}}{\\sum_{k=1}^{n}{\\exp{(s_k)}}} \\quad for \\ k=1,2,\\dots,k \\tag{1} $$\n[식 1]에서 소프트맥스 함수의 출력의 각 원소 $p_k$는 $0 \\leq p_k \\leq 1,\\space p_k \\in \\mathbb{R}$ 이다.\n따라서 소프트맥스의 출력은 확률로 해석할 수 있다. 우리는 이것을 교차 엔트로피 오차에 입력할 것이다.\n교차 엔트로피 오차 $$ Loss = - \\sum_{k}t_k \\log{p_k} \\tag{2} $$\n[식 2]에서 $t_k$는 $k$번째 클래스의 정답 레이블이다. $t = \\begin{bmatrix} 0, 1, 1 \\end{bmatrix}$ 과 같이 one-hot vector로 표기한다.\none-hot vector는 단 하나의 원소만 1 이고 그 외에는 0인 벡터이다.\n미니 배치를 고려하면 교차 엔트로피 오차의 식은 아래와 같이 바뀌게 된다.\n$$ Loss = - \\frac{1}{N} \\sum_{n} \\sum_{k}t_{nk} \\log{p_{nk}} \\tag{3} $$\n[식 3]에서 $N$은 미니 배치의 개수, $t_{nk}$는 $n$번째 데이터의 $k$차원째의 값, $p_{nk}$는 신경망의 출력, $t_{nk}$는 정답 레이블이다.\n이는 N으로 나눠서 1 개당의 평균 손실 함수를 구하는 것이다. 미니배치의 크기에 관계없이 항상 일관된 척도를 얻을 수 있다.\n행렬의 미분 신경망 학습의 목표는 손실을 최소화하는 매개변수를 찾는 것이다. 이때 중요한 것이 바로 미분과 기울기이다.\n행렬을 입력이나 출력으로 가지는 함수를 미분하는 것을 행렬 미분이라고 한다. (정확하게는 편미분이다.)\n또한 행렬미분에는 분자중심 표현법과 분모중심 표현법 두 가지가 있는데, 본 포스팅에서는 분모중심 표현법으로 서술하겠다.\n행렬 미분에 대한 상세한 정의는 여기를 참고하기 바란다.\n$$ \\frac{\\partial L}{\\partial \\mathbf{x}}= \\begin{pmatrix} \\frac{\\partial L}{\\partial x_1} \u0026amp; \\frac{\\partial L}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial L}{\\partial x_n} \\end{pmatrix} \\tag{4} $$\n$L$은 스칼라, $x$는 벡터인 함수 $L=f(x)$가 있을 때, $x_i$에 대한 $L$의 미분은 $\\frac{\\partial y}{\\partial x_i}$로 쓸 수 있으며, 이를 정리하면 [식 4]와 같다.\n$$ \\frac{\\partial L}{\\partial \\mathbf{W}}= \\begin{pmatrix} \\frac{\\partial L}{\\partial W_{11}} \u0026amp; \\cdots \u0026amp; \\frac{\\partial L}{\\partial W_{1n}} \\\\ \\vdots \u0026amp; \\ddots \\\\ \\frac{\\partial L}{\\partial W_{m1}} \u0026amp; \u0026amp; \\frac{\\partial L}{\\partial W_{mn}} \\end{pmatrix} \\tag{5} $$\n$\\mathbf{W}$가 $m \\times n$ 행렬이라면, $L = g(\\mathbf{W})$ 함수의 기울기는 [식 5] 같이 쓸 수 있다.\n여기서 중요한 점은 $\\mathbf{W}$와 $\\frac{\\partial L}{\\partial \\mathbf{x}}$의 형상이 같다는 것이다. 이 성질을 이용하면 매개변수 갱신과 연쇄 법칙을 쉽게 구현할 수 있다.\n연쇄 법칙 우리는 신경망의 학습을 위해 각 매개변수에 대한 손실의 기울기를 구해 매개변수를 갱신할 것이다. 신경망의 기울기는 오차역전파법 (back-propagation)을 통해 구할 수 있으며, 이 때 필요한 것이 연쇄 법칙이다.\n$y=f(x)$와 $z=g(y)$라는 두 함수가 있을 때, $z=g(f(x))$이다.\n$$ \\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x} \\tag{6} $$\n$x$에 대한 $z$의 미분은 [식 6]과 같이 $y=f(x)$의 미분과 $z=g(y)$의 미분을 곱해 구할 수 있다. 이것이 바로 연쇄 법칙이다.\n즉, 함수가 아무리 복잡하더라도 개별 함수들의 미분을 통해 효율적인 계산을 할 수 있다는 것이다.\n가중치 갱신 신경망의 학습은 다음 순서로 수행된다.\ngraph LR a((미니배치))--\u003eb((기울기 계산))--\u003ec((매개변수 갱신))--\u003ea 우선 미니배치에서 데이터를 선택하고, 이어서 오차역전파법으로 가중치의 기울기를 얻는다. 이 기울기는 현재의 가중치 매개변수에서 손실을 가장 크게 하는 방향을 가리킨다. 따라서 매개변수를 그 기울기와 반대 방향으로 갱신하면 손실을 줄일 수 있다. 이것이 바로 경사하강법이다.\n확률적경사하강법 확률적 경사하강법 (Stochastic Gradient Descent)은 무작위로 선택된 데이터(미니배치)에 대한 기울기를 이용하여, 현재의 가중치를 기울기 방향으로 일정한 거리만큼 갱신한다.\n$$ W \\gets {W} - \\eta {\\frac{\\partial {L}}{\\partial {W}}} \\tag{7} $$\n[식 7]에서 갱신하는 가중치 매개변수는 $\\mathbf{W}$ 이고, $\\mathbf{W}$에 대한 손실 함수의 기울기는 $\\frac{\\partial {L}}{\\partial {W}}$이다. $\\eta$는 학습률 (learning rate)을 나타내고, 0.01이나 0.001 같은 값을 미리 정해서 사용한다.\n이것을 파이썬으로 구현하면 아래와 같다.\n1 2 3 4 5 6 7 class SGD: def __init__(self, lr=0.01): self.lr = lr # 학습률 def update(self, params, grads): for i in range(len(params)): params[i] -= self.lr * grads[i] 신경망 예제 이전 포스팅에서 설계한 신경망에 Softmax 레이어와 Cross Entropy Error 레이어를 새로 추가해 보자.\ngraph LR x(x)==\u003e A(Affine) A==\u003e B(Sigmoid) B==\u003e C(Affine) C==\u003eD(Softmax) t(t)==\u003eE D==\u003eE(Cross Entropy Error) E==\u003eF(L) $\\textbf{x}$는 입력 데이터, $\\textbf{t}$는 정답 레이블. $L$은 손실을 나타낸다.\nSigmoid의 역전파 구현 Sigmoid의 수식은 $y=\\frac{1}{1+\\exp{(-x)}}$이다. 그 미분은 아래 [식 7]과 같다.\n$$ \\frac{\\partial y}{\\partial x} = y(1-y) \\tag{8} $$\n이를 파이썬으로 구현하면 아래와 같다.\n1 2 3 4 5 6 7 8 9 class Sigmoid: def __init__(self): self.params = [] def forward(self, x): return 1 / (1 + np.exp(-x)) def backward(self, dout): return dout * (1.0 - self.out) * self.out Affine의 역전파 구현 Affine의 역전파는 MatMul 노드와 Repeat 노드의 역전파를 수행하면 구할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Affine: def __init__(self, W, b): self.params = [W, b] self.grads = [np.zeros_like(W), np.zeros_like(b)] self.x = None def forward(self, x): W, b = self.params out = np.matmul(xz W) + b self.x = x return out def backward(self, dout): W, b = self.params dx = np.matmul(doutz W.T) dW = np.matmul(self.x.T, dout) db = np.sum(dout, axis=0) self.grads[0][...] = dW self.grads[1][...] = db return dx TwoLayerNet 구현 Sigmoid와 Affine 레이어의 back-propagation을 구현했으니, 이제 TwoLayerNet 클래스를 완성해 보자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class SoftmaxWithLoss: def __init__(self): self.params, self.grads = [], [] self.y = None # softmax의 출력 self.t = None # 정답 레이블 def forward(self, x, t): self.t = t self.y = softmax(x) # 정답 레이블이 원핫 벡터일 경우 정답의 인덱스로 변환 if self.t.size == self.y.size: self.t = self.t.argmax(axis=1) loss = cross_entropy_error(self.y, self.t) return loss class TwoLayerNet: def __init__(self, input_size, hidden_size, output_size): I, H, O = input_size, hidden_size, output_size W1 = 0.01 * np.random.randn(I, H) b1 = np.zeros(H) W2 = 0.01 * np.random.randn(H, O) b2 = np.zeros(O) self.layers = [ Affine(W1, b1) Sigmoid() Affine(W2, b2) ] self.loss_layer = SoftmaxWithLoss() self.params, self.grads = [], [] for layer in self.layers: self.params += layer.params self.grads += layer.grads def predict(self, x): for layer in self.layers: x = layer.forward(x) return x def forward(self, x, t): score = self.predict(x) loss = self.loss_layer.forward(score, t) return loss def backward(self, dout=1): dout = self.loss_layer.backward(dout) for layer in reversed(self.layers): dout = layer.backward(dout) return dout ","date":"2023-11-21T00:00:00Z","image":"https://gyeongmin.kr/images/deep-learning-from-scratch.jpeg","permalink":"https://gyeongmin.kr/p/neural-network-trainning/","title":"신경망의 학습"},{"content":" 본 포스팅은 \u0026lsquo;밑바닥부터 시작하는 딥러닝 2\u0026rsquo; 교재를 참고했습니다.\n신경망의 추론 추론이란 다중 클래스 분류 등의 문제에 답을 구하는 작업이다.\n신경망 예시 신경망은 두뇌의 신경세포, 즉 뉴런이 연결된 형태를 모방한 모델이다.\n위 신경망의 경우 입력층 4개, 중간층(은닉층) 2개, 출력층 3개로 구성되어 있다.\n입력층과 중간층 사이를 보면 인접한 층의 모든 뉴런들이 서로 연결되어 있는데, 이것을 fully connected layer (완전연결계층) 이라고 한다.\n가중치와 편향 각 노드 사이에는 가중치가 존재한다. 가중치 값과 뉴런의 값을 곱해 그 합이 다음 뉴런의 입력으로 쓰인다.\n또한, 이 때 이전 뉴런의 값에 영향을 받지 않는 정수도 더해지는데, 이 정수를 bias (편향) 이라고 한다.\n입력층의 데이터를 $\\textbf{x}$, 가중치는 $\\textbf{W}$, 편향은 $\\textbf{b}$로 나타내면 은닉층의 뉴런 $\\textbf{h}$는 다음과 같이 나타낼 수 있다.\n$$ \\textbf{h} = \\textbf{x} \\textbf{W} + \\textbf{b} \\tag{1} $$\nSigmoid 활성화 함수 완전연결계층에 의한 변환은 선형 변환이다. 여기에 비선형 효과를 부여하는 것이 바로 활성화 함수이다. 이를 통해 신경망의 표현력을 높일 수 있다.\n가장 대표적인 활성화 함수인 Sigmoid를 알아보자.\nxychart-beta title \"Sigmoid\" y-axis 0 --\u003e 1 x-axis [-5, \"-4.5\", -4, \"-3.5\", -3, \"-2.5\", -2, \"-1.5\", -1, \"-0.5\", 0, \"0.5\", 1, \"1.5\", 2, \"2.5\", 3, \"3.5\", 4, \"4.5\", 5] line [0.0066928509242848554, 0.01098694263059318, 0.01798620996209156, 0.02931223075135632, 0.04742587317756678, 0.07585818002124355, 0.11920292202211755, 0.18242552380635635, 0.2689414213699951, 0.3775406687981454, 0.5, 0.6224593312018546, 0.7310585786300049, 0.8175744761936437, 0.8807970779778823, 0.9241418199787566, 0.9525741268224334, 0.9706877692486436, 0.9820137900379085, 0.9890130573694068, 0.9933071490757153] 시그모이드 함수는 S자와 유사한 완만한 곡선을 가진다. 식은 아래와 같다.\n$$ \\sigma(x)=\\frac{1}{1+\\exp(-x)} \\tag{2} $$\n이를 파이썬으로 구현하면 다음과 같다.\n1 2 def sigmoid(x)： return 1 / (1 + np.exp(-x)) 신경망과 순전파의 구현 신경망 추론 과정에서 하는 처리는 순전파(forward propagation)에 해당한다. 말 그대로 입력층에서 출력층으로 향하는 전파이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import numpy as np # 시그모이드 함수에 의한 변환 class Sigmoid: def __init__(self)： self.params = [] def forward(self, x)： return 1 / (1 + np.exp(-x)) # 완전연결계층에 의한 변환 class Affine: def __init__（self, W, b）： self.params = [W, b] def forward(self, x)： W, b = self.params return np.matmul(x, W) + b 완전연결계층에 의한 변환은 기하학에서의 Affine 변환에 해당한다.\n입력 $\\textbf{x}$가 Affine 계층 Sigmoid 계층 Affine 계층을 차례로 거쳐 점수인 $\\textbf{s}$를 출력하는 신경망을 만들어 보자.\ngraph LR X==\u003e A[Affine] A==\u003e B[Sigmoid] B==\u003e C[Affine] C==\u003e S[S] 이 신경망을 파이썬으로 구현하면 아래와 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class TwoLayerNet: def __init__(self, input_size, hidden_size, output_size)： I, H, O = input_size, hidden_size, output_size # 가중치와 편향 초기화 W1 = np.random.randn(I, H) b1 = np.random.randn(H) W2 = np.random.randn(H, O) b2 = np.random.randn(O) # 계층 생성 self.layers = [ Affine(W1, b1), Sigmoid(), Affine(W2, b2) ] # 모든 가중치를 리스트에 모은다. self.params = [] for layer in self.layers: self.params += layer.params def predict(self, x)： for layer in self.layers: x = layer.forward(x) return x 위에서 구현한 TwoLayerNet 클래스를 이용해 신경망의 추론을 수행해 보자.\n1 2 3 x = np.random.randn(10, 2) model = TwoLayerNet(2, 4, 3) s = model.predict(x) 이처럼 계층을 클래스로 만들어두면 신경망을 쉽게 사용할 수 있다.\n","date":"2023-11-20T00:00:00Z","image":"https://gyeongmin.kr/images/deep-learning-from-scratch.jpeg","permalink":"https://gyeongmin.kr/p/neural-network-inference/","title":"신경망의 추론"},{"content":" 본 포스팅은 \u0026lsquo;밑바닥부터 시작하는 딥러닝 2\u0026rsquo; 교재를 참고했습니다.\n기초 선형대수 스칼라와 벡터 스칼라와 벡터는 선형 대수에서 가장 기본적인 개념이다. 스칼라는 크기, 벡터는 크기와 방향을 가지고 있다.\n스칼라 크기만으로 나타낼 수 있는 물리량이다.\n길이, 부피, 거리 등과 같이 숫자 하나로 표현되는 값이다.\n변수에 저장 할때는 일반적으로 소문자를 이용하여 표기한다.\n벡터 벡터는 스칼라의 집합이며, 행렬을 구성하는 기본 단위이다.\n크기와 방향을 모두 나타내는 개념이다.\n일반적으로 영어 볼드체로 표기하고, 파이썬에선 1차원 리스트로 취급할 수 있다.\n행벡터와 열벡터 열벡터(열 행렬) $m × 1$ 행렬은 $m$ 원소들의 단일 열벡터이다.\n$$ \\mathbf{x}=\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{bmatrix} \\tag{1} $$\n행벡터(행 행렬) 1 × m 행렬은 그 원소들 m의 단일 행벡터이다.\n행렬 행렬은 숫자가 2차원 형태로 숫자를 나열하는 것이다.\n행렬은 행과 열로 구성되어 있다. 행은 가로 방향을 나타내고, 열을 세로 방향을 나타낸다.\n아래와 같이 소괄호를 사용하기도 하고, 대괄호를 사용하기도 한다.\n$$ \\mathbf{A} = \\begin{pmatrix} 2 \u0026amp; 4 \\\\ 7 \u0026amp; 3 \\end{pmatrix} = \\begin{bmatrix} 2 \u0026amp; 4 \\\\ 7 \u0026amp; 3 \\end{bmatrix} \\tag{2} $$\n전치 행렬 $$ \\mathbf{x} = \\begin{bmatrix} x_1 \u0026amp; x_2 \u0026amp; \\dots \u0026amp; x_m \\end{bmatrix} \\tag{3} $$\n행벡터의 전치행렬(윗첨자 T로 표기)은 열벡터이고, 마찬가지로 열벡터의 전치 행렬은 행 벡터이다.\n$$ \\begin{bmatrix} x_1 \u0026amp; x_2 \u0026amp; \\dots \u0026amp; x_m \\end{bmatrix}^\\intercal = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{bmatrix} \\tag{4} $$\n행렬의 덧셈과 뺄셈 각 위치에 대응하는 원소끼리 더하거나 빼는 것이다.\n$$ \\mathbf{A} = \\begin{pmatrix} 1 \u0026amp; 2 \\\\ 3 \u0026amp; 4 \\end{pmatrix} , \\space \\mathbf{B} = \\begin{pmatrix} 5 \u0026amp; 6 \\\\ 7 \u0026amp; 8 \\end{pmatrix} , \\space \\mathbf{A + B} = \\begin{pmatrix} 6 \u0026amp; 8 \\\\ 10 \u0026amp; 12 \\end{pmatrix} \\tag{5} $$\n행렬의 내적 벡터의 내적은 두 벡터에서 대응하는 원소들의 곱을 모두 더한 것이다.\n$$ \\mathbf{x} \\cdot \\mathbf{y} = x_1y_1 + x_2y_2 + \\dots + x_ny_n \\tag{6} $$\n행렬의 곱셈 행렬의 곱셈은 일반적인 곱셈과 다르다. 일종의 함수로 이해하는 것이 좋다.\n행렬곱은 앞 행렬의 열의 수와 뒷 행렬의 행의 수가 같을 때만 정의된다.\n두 행렬 $A, B$가 각각 $m\\times n, n\\times r$ 행렬일 때,\n$$ A=\\begin{pmatrix}a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\ {\\color{blue}a_{21}} \u0026amp; {\\color{blue}a_{22}} \u0026amp; {\\color{blue}\\cdots} \u0026amp; {\\color{blue}a_{2n}} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn}\\end{pmatrix}, B=\\begin{pmatrix}{\\color{red}b_{11}} \u0026amp; b_{12} \u0026amp; \\cdots \u0026amp; b_{1r} \\\\ {\\color{red}b_{21}} \u0026amp; b_{22} \u0026amp; \\cdots \u0026amp; b_{2r} \\\\ {\\color{red}\\vdots} \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ {\\color{red}b_{n1}} \u0026amp; b_{n2} \u0026amp; \\cdots \u0026amp; b_{nr}\\end{pmatrix} \\tag{7} $$\n이라고 하면 행렬의 곱 $AB$는 $m\\times r$ 행렬이며,\n$$ AB=\\begin{pmatrix}\\sum_k a_{1k}b_{k1} \u0026amp; \\sum_k a_{1k}b_{k2} \u0026amp; \\cdots \u0026amp; \\sum_k a_{1k}b_{kr} \\\\ {\\color{#C0C}\\sum_k a_{2k}b_{k1}} \u0026amp; \\sum_k a_{2k}b_{k2} \u0026amp; \\cdots \u0026amp; \\sum_k a_{2k}b_{kr} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\sum_k a_{mk}b_{k1} \u0026amp; \\sum_k a_{mk}b_{k2} \u0026amp; \\cdots \u0026amp; \\sum_k a_{mk}b_{kr}\\end{pmatrix} \\tag{8} $$\n이다. (단, $k=1,2,\u0026hellip;,n$)\n항상 행렬을 다룰땐 형상에 주의해야 한다.\n파이썬에서의 벡터와 행렬 파이썬에서는 numpy 라이브러리를 통해 쉽게 벡터와 행렬을 표현할 수 있다.\n1 2 3 4 5 \u0026gt;\u0026gt;\u0026gt; x = np.array([1, 2, 3]) \u0026gt;\u0026gt;\u0026gt; x.shape (3,) \u0026gt;\u0026gt;\u0026gt; x.ndim 1 1 2 3 4 5 \u0026gt;\u0026gt;\u0026gt; W = np.array([1, 2, 3], [4, 5, 6]) \u0026gt;\u0026gt;\u0026gt; W.shape (2, 3) \u0026gt;\u0026gt;\u0026gt; W.ndim 2 행렬의 원소별 연산 서로 대응하는 원소들끼리 독립적인 연산이 이루어진다.\n1 2 3 4 5 6 \u0026gt;\u0026gt;\u0026gt; x + W array([1, 3, 5], [7, 9, 11]) \u0026gt;\u0026gt;\u0026gt; x * W array([0, 2, 6], [12, 20, 30]) 브로드캐스트 넘파이의 다차원 배열은 형상이 다른 배열끼리 연산을 하는 브로드캐스트가 가능하다.\n1 2 3 4 \u0026gt;\u0026gt;\u0026gt; A = np.array([[1, 2], [3, 4]]) \u0026gt;\u0026gt;\u0026gt; A * 10 array([[10, 20], [30, 40]]) 1 2 3 4 \u0026gt;\u0026gt;\u0026gt; A = np.array([[1, 2], [3, 4]]) \u0026gt;\u0026gt;\u0026gt; b = np.array([10, 20]) array([[10, 40], [30, 80]]) ","date":"2023-11-15T00:00:00Z","image":"https://gyeongmin.kr/images/deep-learning-from-scratch.jpeg","permalink":"https://gyeongmin.kr/p/basic-linear-algebra/","title":"기초 선형대수 - 스칼라, 벡터, 행렬"},{"content":" 문제 링크 : https://www.acmicpc.net/problem/20437\n문제 작년에 이어 새로운 문자열 게임이 있다. 게임의 진행 방식은 아래와 같다.\n알파벳 소문자로 이루어진 문자열 $W$가 주어진다.\n양의 정수 $K$가 주어진다.\n어떤 문자를 정확히 $K$개를 포함하는 가장 짧은 연속 문자열의 길이를 구한다.\n어떤 문자를 정확히 $K$개를 포함하고, 문자열의 첫 번째와 마지막 글자가 해당 문자로 같은 가장 긴 연속 문자열의 길이를 구한다.\n위와 같은 방식으로 게임을 $T$회 진행한다.\n입력 문자열 게임의 수 $T$가 주어진다. $(1 ≤ T ≤ 100)$\n다음 줄부터 2개의 줄 동안 문자열 $W$와 정수 $K$가 주어진다. $(1 ≤ K ≤ |W| ≤ 10,000) $\n출력 $T$개의 줄 동안 문자열 게임의 3번과 4번에서 구한 연속 문자열의 길이를 공백을 사이에 두고 출력한다.\n만약 만족하는 연속 문자열이 없을 시 -1을 출력한다.\n풀이 인덱스 전처리 문자별로 인덱스를 전처리한다. 예를 들어, 문자열이 apple 이라면 아래와 같이 저장하면 된다.\nchar index a 0 p 1, 2 l 3 e 4 1 2 3 4 vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; v((int) \u0026#39;z\u0026#39; + 1); for (int i = 0; i \u0026lt; s.length(); ++i) { v[s[i]].push_back(i); } 슬라이딩 윈도우 적용 인덱스들은 자연스럽게 정렬이 되어 있을 것이다.\n길이가 K인 슬라이딩 윈도우를 적용한다. a부터 z까지 인덱스가 저장된 벡터를 보면서, 문자가 K인 범위의 길이를 구해주자. start는 0부터 시작하고, end는 k-1 부터 시작하면 된다.\n문자가 K개 나오는 범위의 길이는 인덱스[end] - 인덱스[start] + 1 로 구할 수 있다.\n1 2 3 4 5 for (char c = \u0026#39;a\u0026#39;; c \u0026lt;= \u0026#39;z\u0026#39;; ++c) { for (int start = 0, end = k - 1; end \u0026lt; v[c].size(); ++end, ++start) { int length = v[c][end] - v[c][start] + 1; } } 아래와 같이 Range-based for loop을 사용해도 된다.\n1 2 3 4 5 for (auto indexes : v) { for (int start = 0, end = k - 1; end \u0026lt; indexes.size(); ++end, ++start) { int length = indexes[end] - indexes[start] + 1; } } 정답 출력 최대값과 최소값을 구한 후 출력해 주자. 이 때, min_len이 INT_MAX라는 것은 값을 찾지 못했다는 뜻이므로 -1을 출력한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 int min_len = INT_MAX; int max_len = INT_MIN; for (auto indexes : v) { for (int start = 0, end = k - 1; end \u0026lt; indexes.size(); ++end, ++start) { int length = indexes[end] - indexes[start] + 1; min_len = min(min_len, length); max_len = max(max_len, length); } } if (min_len == INT_MAX) { cout \u0026lt;\u0026lt; -1 \u0026lt;\u0026lt; endl; } else { cout \u0026lt;\u0026lt; min_len \u0026lt;\u0026lt; \u0026#39; \u0026#39; \u0026lt;\u0026lt; max_len \u0026lt;\u0026lt; endl; } 소스 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 #include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; void solve() { string s; cin \u0026gt;\u0026gt; s; int k; cin \u0026gt;\u0026gt; k; vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; v((int) \u0026#39;z\u0026#39; + 1); for (int i = 0; i \u0026lt; s.length(); ++i) { v[s[i]].push_back(i); } int min_len = INT_MAX; int max_len = INT_MIN; for (auto indexes : v) { for (int start = 0, end = k - 1; end \u0026lt; indexes.size(); ++end, ++start) { int length = indexes[end] - indexes[start] + 1; min_len = min(min_len, length); max_len = max(max_len, length); } } if (min_len == INT_MAX) { cout \u0026lt;\u0026lt; -1 \u0026lt;\u0026lt; endl; } else { cout \u0026lt;\u0026lt; min_len \u0026lt;\u0026lt; \u0026#39; \u0026#39; \u0026lt;\u0026lt; max_len \u0026lt;\u0026lt; endl; } } int main() { int t; cin \u0026gt;\u0026gt; t; while (t--) solve(); return 0; } ","date":"2023-07-07T00:00:00Z","image":"https://gyeongmin.kr/images/boj.png","permalink":"https://gyeongmin.kr/p/boj-20437/","title":"BOJ 20437: 문자열 게임 2 (C++)"},{"content":"A - N-choice question https://atcoder.jp/contests/abc300/tasks/abc300_a\n문제 첫째 줄에 배열의 길이 $N$과 두 수 $A$와 $B$가 주어진다. 둘째 줄에 배열이 주어진다.\n설명 배열 중에서 $A+B$와 일치하는 것의 인덱스를 출력한다.\n소스 코드 1 2 3 4 5 6 import sys input = sys.stdin.readline n, a, b = map(int, input().split()) arr = [int(x) for x in input().split()] print(arr.index(a+b)+1) C - Cross https://atcoder.jp/contests/abc300/tasks/abc300_c\n문제 첫째 줄에 배열의 높이 $H$와 너비 $W$가 주어진다. 둘째 줄부터 $1+H$ 번째 줄까지 배열이 주어진다.\n\u0026lsquo;#\u0026lsquo;이 크로스하는 경우를 세 주면 되는데, 사이즈별로 개수를 세 주어야 한다.\n1 2 3 4 5 #...# .#.#. ..#.. .#.#. #...# 위 경우는 사이즈가 2인 것이다.\n사이즈가 $1$인 개수부터 사이즈가 $N$인 개수까지 출력하면 된다. 여기서 $N$은 $min(H,W)$이다.\n해설 중심점을 먼저 찾으면 되는데, 모든 중심점은 두 가지 조건을 만족한다.\n모든 크로스의 중심점은 아래 에시의 형태이다. 1 2 3 #.# .#. #.# 모든 중심점의 좌표값은 다음 범위 내에 있다. $1 \\leq R \\leq H-2$ $1 \\leq C \\leq W-2$ 이걸로 중심점을 한 개씩 잡고, 1씩 크기를 늘려가면서 체크해주면 된다.\n소스 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import sys input = sys.stdin.readline def check(x, y): i = 1 while True: for dx, dy in zip([i, i, -i, -i], [i, -i, i, -i]): nx, ny = x + dx, y + dy if not nx in range(w) or not ny in range(h): return i-2 if arr[ny][nx] != \u0026#39;#\u0026#39;: return i-2 i += 1 h, w = map(int, input().split()) arr = [input().rstrip() for _ in range(h)] result = [0 for _ in range(min(h, w))] for y in range(1, h-1): for x in range(1, w-1): if arr[y][x] == arr[y-1][x-1] == arr[y-1][x+1] == arr[y+1][x-1] == arr[y+1][x+1] == \u0026#39;#\u0026#39;: result[check(x, y)] += 1 print(*result) D - AABCC https://atcoder.jp/contests/abc300/tasks/abc300_d\n문제 첫째 줄에 $N$이 주어진다. $(3 \\leq N \\leq 10^{12})$\n$N$보다 크지 않은 양의 정수 중에서, $a^2\\times b\\times c^2$ 로 표현될 수 있는 수의 개수를 출력한다. $(a\u0026lt;b\u0026lt;c)$ 이며, $a, b,c$는 소수이다.\n해설 재미있는 문제였다.\n$\\sqrt{N}$까지 소수를 구하고, $a\u0026lt;b\u0026lt;c$가 되도록 세 가지 소수를 선택해 조건을 만족하는지 개수를 세 주면 된다.\n커팅 없이 돌리면 시간 초과가 나고, 두 가지 커팅을 해 주어야 한다.\n$b \u0026lt; c$ 이므로 $a^2 \\times b^3$이 $N$ 보다 크다면 $a^2\\times b\\times c^2 \\leq N$을 만족할 수 없으므로, 그 뒤는 확인하지 않아도 된다.\n$a^2\\times b\\times c^2$ 가 $N$보다 크다면, 마찬가지로 그 뒤는 확인하지 않아도 된다.\n소스 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import sys from math import * input = sys.stdin.readline def eratosthenes(n): primes = [True] * (n + 1) p = 2 while p * p \u0026lt;= n: if primes[p]: for i in range(p * p, n + 1, p): primes[i] = False p += 1 primes_list = [] for p in range(2, n): if primes[p]: primes_list.append(p) return primes_list n = int(input()) primes = eratosthenes(floor(sqrt(n))) l = len(primes) count = 0 for i in range(l): for j in range(i+1, l): a, b = primes[i], primes[j] if a*a*b*b*b \u0026gt; n: break for k in range(j+1, l): a, b, c = primes[i], primes[j], primes[k] if a*a*b*c*c \u0026lt;= n: count += 1 else: break print(count) ","date":"2023-04-30T00:00:00Z","image":"https://gyeongmin.kr/images/atcoder.png","permalink":"https://gyeongmin.kr/p/abc-300/","title":"AtCoder ABC 300: A, C, D"},{"content":" 문제 링크 : https://www.acmicpc.net/problem/23807\n문제 서준이는 아빠로부터 생일선물로 세계 지도를 받아서 매우 기뻤다. 세계 지도에서 최단 경로를 찾는 프로그램을 개발해서 아빠께 감사의 마음을 전달하려고 한다. 세계 지도는 도시를 정점으로 갖고 도시 간의 도로를 간선으로 갖는 무방향성 그래프이며(undirected graph), 도로의 길이가 간선의 가중치이다. 출발 정점 $X$에서 출발해서 $P$개의 중간 정점 중 적어도 세 개의 정점을 반드시 거친 후 도착 정점 $Z$에 도달하는 최단 거리를 구해서 우리 서준이를 도와주자.\n입력 첫째 줄에 정점의 수 $N(10 \\leq N \\leq 100,000)$, 간선의 수 $M(10 \\leq M \\leq 300,000)$이 주어진다.\n다음 $M$개 줄에 간선 정보 $u, v, w$가 주어지며 도시 $u$와 도시 $v$ 사이의 가중치가 정수 $w$인 양방향 도로를 나타낸다. $(1 \\leq u, v \\leq N, u ≠ v, 1 \\leq w \\leq 1,000,000)$\n다음 줄에 $X Z$가 주어진다. $(1 \\leq X, Z \\leq N, X \\neq Z)$\n다음 줄에 $P$가 주어진다. $(3 \\leq P \\leq \\min(100, N - 3))$\n다음 줄에 $P$개의 서로 다른 중간 정점 $Y(1 \\leq Y \\leq N, X ≠ Y ≠ Z)$가 빈칸을 사이에 두고 주어진다.\n출력 출발 정점 $X$에서 출발해서 $P$개의 중간 정점 중 적어도 세 개의 정점을 반드시 거친 후 도착 정점 $Z$에 도달하는 최단 거리를 출력한다. 도착 정점 Z에 도착할 수 없는 경우 $-1$을 출력한다.\n풀이 브루트포스 + 다익스트라 문제이다.\n$X - A - B - C - Z$ 로 가는 최단 거리를 찾아야 한다.\n우선순위 큐를 활용한 다익스트라의 시간복잡도는 $O(E\\log{V})$이다. 중간 정점은 최대 100개가 주어지며, 이 중 3개를 선택해야 하므로 경우의 수가 최대 ${}_{100}{\\rm P}_3$ 이다. 모든 경우에서 다익스트라를 돌리면 당연히 시간초과가 난다.\n$X$와 중간 정점들을 기준으로 다익스트라를 먼저 돌리고, 그 결과들을 활용해 모든 경우 중 최소 거리를 구해 주면 된다.\n다익스트라를 최대 101번 돌리고, ${}_{100}{\\rm P}_3 = 970,200$이다. 종합하여 시간 복잡도를 계산해 보면 $300,000\\times\\log{100,000} \\times 101 + 970,200 = 152,470,200$ 가 나온다.\n이 문제의 시간 제한은 6초이다. 바로 통과할 줄 알았다. 하지만 우리의 파이썬은 생각보다 느리다.\npython3 말고 pypy3로 제출해야 한다. 다익스트라 결과를 저장할 dist를 리스트가 아닌 딕셔너리로 구현하고, min() 함수 대신 직접 if문으로 최소값을 찾는 등 최적화를 조금 진행해 주어야 겨우 풀린다.\n마지막에 결과값이 INF일 땐 -1로 출력하는 것을 잊지 말자.\n소스 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 from heapq import heappop, heappush from itertools import permutations import sys input = sys.stdin.readline INF = float(\u0026#39;inf\u0026#39;) def dijkstra(graph, start): n = len(graph) dist = [INF] * n dist[start] = 0 queue = [(0, start)] while queue: path_len, v = heappop(queue) if path_len == dist[v]: for w, edge_len in graph[v]: if edge_len + path_len \u0026lt; dist[w]: dist[w] = edge_len + path_len heappush(queue, (edge_len + path_len, w)) return dist n, m = map(int, input().split()) graph = [[] for _ in range(n+1)] for _ in range(m): u, v, w = map(int, input().split()) graph[u].append((v, w)) graph[v].append((u, w)) x, z = map(int, input().split()) p = int(input()) mid = [int(x) for x in input().split()] dist = {} for node in mid: dist[node] = dijkstra(graph, node) dist[x] = dijkstra(graph, x) result = INF for a, b, c in permutations(mid, 3): total = dist[x][a] + dist[a][b] + dist[b][c] + dist[c][z] if result \u0026gt; total: result = total if result == INF: result = -1 print(result) 후기 파이썬 기준 시간 제한이 정말 빡빡한 문제였다. 이 문제를 파이썬으로 풀어본 사람이 없길래 파이썬으로 풀어 봤다.\n","date":"2023-04-21T00:00:00Z","image":"https://gyeongmin.kr/images/boj.png","permalink":"https://gyeongmin.kr/p/boj-23807/","title":"BOJ 23807: 두 단계 최단 경로 3 (Python)"},{"content":" 문제 링크 : https://www.acmicpc.net/problem/16935\n문제 크기가 N×M인 배열이 있을 때, 배열에 연산을 R번 적용하려고 한다. 연산은 총 6가지가 있다.\n1번 연산은 배열을 상하 반전시키는 연산이다.\n1 2 3 4 5 6 7 1 6 2 9 8 4 → 4 2 9 3 1 8 7 2 6 9 8 2 → 9 2 3 6 1 5 1 8 3 4 2 9 → 7 4 6 2 3 1 7 4 6 2 3 1 → 1 8 3 4 2 9 9 2 3 6 1 5 → 7 2 6 9 8 2 4 2 9 3 1 8 → 1 6 2 9 8 4 \u0026lt;배열\u0026gt; \u0026lt;연산 결과\u0026gt; 2번 연산은 배열을 좌우 반전시키는 연산이다.\n1 2 3 4 5 6 7 1 6 2 9 8 4 → 4 8 9 2 6 1 7 2 6 9 8 2 → 2 8 9 6 2 7 1 8 3 4 2 9 → 9 2 4 3 8 1 7 4 6 2 3 1 → 1 3 2 6 4 7 9 2 3 6 1 5 → 5 1 6 3 2 9 4 2 9 3 1 8 → 8 1 3 9 2 4 \u0026lt;배열\u0026gt; \u0026lt;연산 결과\u0026gt; 3번 연산은 오른쪽으로 90도 회전시키는 연산이다.\n1 2 3 4 5 6 7 1 6 2 9 8 4 → 4 9 7 1 7 1 7 2 6 9 8 2 → 2 2 4 8 2 6 1 8 3 4 2 9 → 9 3 6 3 6 2 7 4 6 2 3 1 → 3 6 2 4 9 9 9 2 3 6 1 5 → 1 1 3 2 8 8 4 2 9 3 1 8 → 8 5 1 9 2 4 \u0026lt;배열\u0026gt; \u0026lt;연산 결과\u0026gt; 4번 연산은 왼쪽으로 90도 회전시키는 연산이다.\n1 2 3 4 5 6 7 1 6 2 9 8 4 → 4 2 9 1 5 8 7 2 6 9 8 2 → 8 8 2 3 1 1 1 8 3 4 2 9 → 9 9 4 2 6 3 7 4 6 2 3 1 → 2 6 3 6 3 9 9 2 3 6 1 5 → 6 2 8 4 2 2 4 2 9 3 1 8 → 1 7 1 7 9 4 \u0026lt;배열\u0026gt; \u0026lt;연산 결과\u0026gt; 5, 6번 연산을 수행하려면 배열을 크기가 N/2×M/2인 4개의 부분 배열로 나눠야 한다. 아래 그림은 크기가 6×8인 배열을 4개의 그룹으로 나눈 것이고, 1부터 4까지의 수로 나타냈다.\n1 2 3 4 5 6 1 1 1 1 2 2 2 2 1 1 1 1 2 2 2 2 1 1 1 1 2 2 2 2 4 4 4 4 3 3 3 3 4 4 4 4 3 3 3 3 4 4 4 4 3 3 3 3 5번 연산은 1번 그룹의 부분 배열을 2번 그룹 위치로, 2번을 3번으로, 3번을 4번으로, 4번을 1번으로 이동시키는 연산이다.\n1 2 3 4 5 6 7 3 2 6 3 1 2 9 7 → 2 1 3 8 3 2 6 3 9 7 8 2 1 4 5 3 → 1 3 2 8 9 7 8 2 5 9 2 1 9 6 1 8 → 4 5 1 9 5 9 2 1 2 1 3 8 6 3 9 2 → 6 3 9 2 1 2 9 7 1 3 2 8 7 9 2 1 → 7 9 2 1 1 4 5 3 4 5 1 9 8 2 1 3 → 8 2 1 3 9 6 1 8 \u0026lt;배열\u0026gt; \u0026lt;연산 결과\u0026gt; 6번 연산은 1번 그룹의 부분 배열을 4번 그룹 위치로, 4번을 3번으로, 3번을 2번으로, 2번을 1번으로 이동시키는 연산이다.\n1 2 3 4 5 6 7 3 2 6 3 1 2 9 7 → 1 2 9 7 6 3 9 2 9 7 8 2 1 4 5 3 → 1 4 5 3 7 9 2 1 5 9 2 1 9 6 1 8 → 9 6 1 8 8 2 1 3 2 1 3 8 6 3 9 2 → 3 2 6 3 2 1 3 8 1 3 2 8 7 9 2 1 → 9 7 8 2 1 3 2 8 4 5 1 9 8 2 1 3 → 5 9 2 1 4 5 1 9 \u0026lt;배열\u0026gt; \u0026lt;연산 결과\u0026gt; 입력 첫째 줄에 배열의 크기 $N$, $M$과 수행해야 하는 연산의 수 R이 주어진다.\n둘째 줄부터 $N$개의 줄에 배열 $A$의 원소 $A_{ij}$가 주어진다.\n마지막 줄에는 수행해야 하는 연산이 주어진다. 연산은 공백으로 구분되어져 있고, 문제에서 설명한 연산 번호이며, 순서대로 적용시켜야 한다.\n출력 입력으로 주어진 배열에 $R$개의 연산을 순서대로 수행한 결과를 출력한다.\n제한 $2 \\leq N, M \\leq 100$ $1 \\leq R \\leq 1,000$ $N$, $M$은 짝수 $1 \\leq A_{ij} \\leq 10^8$ 소스 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 import sys input = sys.stdin.readline n, m, r = map(int, input().split()) arr = [[int(x) for x in input().split()] for _ in range(n)] options = [int(x) for x in input().split()] for option in options: if option == 1: arr = arr[::-1] # 상하 반전 elif option == 2: arr = list(map(lambda x: x[::-1], arr)) # 좌우 반전 elif option == 3: arr = list(zip(*arr[::-1])) # 오른쪽으로 90도 회전 n, m = m, n # 행, 열 사이즈 스왑 elif option == 4: arr = list(map(list, zip(*arr)))[::-1] # 왼쪽으로 90도 회전 n, m = m, n else: arr1 = map(lambda x: x[:m//2], arr[:n//2]) # 1사분면 arr2 = map(lambda x: x[m//2:], arr[:n//2]) # 2사분면 arr3 = map(lambda x: x[:m//2], arr[n//2:]) # 3사분면 arr4 = map(lambda x: x[m//2:], arr[n//2:]) # 4사분면 if option == 5: # 시계 방향 사분면 회전 arr = list(zip(arr3, arr1)) + list(zip(arr4, arr2)) elif option == 6: # 반시계 방향 사분면 회전 arr = list(zip(arr2, arr4)) + list(zip(arr1, arr3)) arr = list(map(lambda x: x[0]+x[1], arr)) for row in arr: print(*row) 풀이 후기 좀 귀찮은 문제였는데, 깔끔하게 구현해 보려고 노력했다. 파이썬 리스트컴프리헨션을 연습하기 좋은 문제였다.\n","date":"2023-02-06T00:00:00Z","image":"https://gyeongmin.kr/images/boj.png","permalink":"https://gyeongmin.kr/p/boj-16935/","title":"16935번 : 배열 돌리기 3 (Python)"},{"content":" 문제 링크 : https://www.acmicpc.net/problem/14620\n문제 2017년 4월 5일 식목일을 맞이한 진아는 나무를 심는 대신 하이테크관 앞 화단에 꽃을 심어 등교할 때 마다 꽃길을 걷고 싶었다.\n진아가 가진 꽃의 씨앗은 꽃을 심고나면 정확히 1년후에 꽃이 피므로 진아는 다음해 식목일 부터 꽃길을 걸을 수 있다.\n하지만 진아에게는 꽃의 씨앗이 세개밖에 없었으므로 세 개의 꽃이 하나도 죽지 않고 1년후에 꽃잎이 만개하길 원한다.\n꽃밭은 N $\\times$ N의 격자 모양이고 진아는 씨앗을 (1,1)~(N,N)의 지점 중 한곳에 심을 수 있다. 꽃의 씨앗은 그림 (a)처럼 심어지며 1년 후 꽃이 피면 그림 (b)모양이 된다.\n꽃을 심을 때는 주의할 점이있다. 어떤 씨앗이 꽃이 핀 뒤 다른 꽃잎(혹은 꽃술)과 닿게 될 경우 두 꽃 모두 죽어버린다. 또 화단 밖으로 꽃잎이 나가게 된다면 그 꽃은 죽어버리고 만다.\n그림(c)는 세 꽃이 정상적으로 핀 모양이고 그림(d)는 두 꽃이 죽어버린 모양이다.\n하이테크 앞 화단의 대여 가격은 격자의 한 점마다 다르기 때문에 진아는 서로 다른 세 씨앗을 모두 꽃이 피게하면서 가장 싼 가격에 화단을 대여하고 싶다.\n단 화단을 대여할 때는 꽃잎이 핀 모양을 기준으로 대여를 해야하므로 꽃 하나당 5평의 땅을 대여해야만 한다.\n돈이 많지 않은 진아를 위하여 진아가 꽃을 심기 위해 필요한 최소비용을 구해주자!\n입력 입력의 첫째 줄에 화단의 한 변의 길이 $N (6 \\leq N \\leq 10)$이 들어온다.\n이후 N개의 줄에 N개씩 화단의 지점당 가격$(0 \\leq G \\leq 200)$이 주어진다.\n출력 꽃을 심기 위한 최소 비용을 출력한다.\n풀이 꽃이 + 모양으로 생겼기 때문에, 중심점을 잘 생각해야 한다.\n다음 그림을 보면서 꽃이 죽는 규칙을 살펴보자.\n중심점의 좌표 $(x,y)$는 $1 \\leq x, \\space y \\leq n-2$ 이다.\n두 꽃이 서로 겹쳐 죽는 경우는 다음 3가지다.\n$x_1, x_2$의 차가 $0$인 경우, $y_1, y_2$의 차이가 $3$미만일 때\n$x_1, x_2$의 차가 $1$인 경우, $y_1, y_2$의 차이가 $2$미만일 때\n$x_1, x_2$의 차가 $2$인 경우, $y_1, y_2$의 차이가 $1$미만일 때\n소스코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 import sys from itertools import combinations input = sys.stdin.readline inf = int(2e9) def is_dead(p1, p2): if abs(p1[0] - p2[0]) == 0: if abs(p1[1] - p2[1]) \u0026lt; 3: return True elif abs(p1[0] - p2[0]) == 1: if abs(p1[1] - p2[1]) \u0026lt; 2: return True elif abs(p1[0] - p2[0]) == 2: if abs(p1[1] - p2[1]) \u0026lt; 1: return True return False def get_cost(p): x, y = p return cost[y][x] + cost[y-1][x] + cost[y+1][x] + cost[y][x-1] + cost[y][x+1] def solution(p1, p2, p3): if is_dead(p1, p2) or is_dead(p2, p3) or is_dead(p3, p1): return inf return get_cost(p1) + get_cost(p2) + get_cost(p3) n = int(input()) cost = [[int(x) for x in input().split()] for _ in range(n)] pos = [(x, y) for x in range(1, n-1) for y in range(1, n-1)] result = inf for p1, p2, p3 in combinations(pos, 3): result = min(result, solution(p1, p2, p3)) print(result) ","date":"2023-02-01T00:00:00Z","image":"https://gyeongmin.kr/images/boj.png","permalink":"https://gyeongmin.kr/p/boj-14620/","title":"BOJ 14620: 꽃길 (Python)"},{"content":" 문제 링크 : https://www.acmicpc.net/problem/9461\n문제 오른쪽 그림과 같이 삼각형이 나선 모양으로 놓여져 있다. 첫 삼각형은 정삼각형으로 변의 길이는 1이다. 그 다음에는 다음과 같은 과정으로 정삼각형을 계속 추가한다. 나선에서 가장 긴 변의 길이를 k라 했을 때, 그 변에 길이가 k인 정삼각형을 추가한다.\n파도반 수열 P(N)은 나선에 있는 정삼각형의 변의 길이이다. P(1)부터 P(10)까지 첫 10개 숫자는 1, 1, 1, 2, 2, 3, 4, 5, 7, 9이다.\nN이 주어졌을 때, P(N)을 구하는 프로그램을 작성하시오.\n입력 첫째 줄에 테스트 케이스의 개수 T가 주어진다. 각 테스트 케이스는 한 줄로 이루어져 있고, N이 주어진다. $(1 \\leq N \\leq 100)$\n출력 각 테스트 케이스마다 P(N)을 출력한다.\n풀이 $N$의 범위는 $(1 \\leq N \\leq 100)$ 이므로, 100까지 미리 계산해 두고 꺼내 쓰면 된다.\n조금만 찾아보면 규칙이 보이는데, $a_i$는 $a_{i-5}$와 $a_{i-1}$를 더한 값이다.\n다음 코드와 같이, 파이썬을 사용하면 배열의 특정 부분에 값을 편하게 넣어줄 수 있다.\n1 dp[1:10] = 1, 1, 1, 2, 2, 3, 4, 5, 7, 9 소스코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import sys input = sys.stdin.readline dp = [0 for _ in range(100)] dp[1:10] = 1, 1, 1, 2, 2, 3, 4, 5, 7, 9 for i in range(11, 101): dp[i] = dp[i-5] + dp[i-1] t = int(input()) for _ in range(t): n = int(input()) print(dp[n]) ","date":"2023-01-17T00:00:00Z","image":"https://gyeongmin.kr/images/boj.png","permalink":"https://gyeongmin.kr/p/boj-9461/","title":"BOJ 9461: 파도반 수열 (Python)"},{"content":"백준 디스코드 봇 \u0026lsquo;백준봇\u0026rsquo; 백준 스터디를 진행할 때 문제를 조금 더 이쁘게 올려 보고 싶어 만들었습니다.\nsolved.ac 비공식 API 를 사용하여 만들었습니다.\n사용 방법 문제 올리기 /백준 문제번호 로 문제를 올릴 수 있습니다. 또는 /백준 문제링크 로 문제를 올릴 수 있습니다. 문제와 소스코드를 함께 올리기 /백준 문제번호 또는 /백준 문제링크 하단에 소스코드를 같이 입력하면, 문제와 소스코드를 함께 올릴 수 있습니다.\n줄바꿈은 Shift + Enter 키를 눌러주시면 됩니다.\n위 사진과 같이 디스코드의 코드 블럭을 이용하시면, 하이라이팅된 코드를 업로드 할 수 있습니다.\n소스 코드가 길 때 디스코드의 Embed의 내용은 4096자 제한이 있습니다. 따라서 Embed의 범위를 벗어나면 아래 사진과 같이 출력합니다.\n명령어 입력이 잘못된 경우, \u0026ldquo;잘못된 입력입니다.\u0026rdquo; 라는 메세지를 출력합니다.\nEmbed에 실패하여 메세지를 보내지 못한 경우, 또는 solved.ac 서버가 불안정하여 데이터를 가져오지 못한 경우 \u0026ldquo;메세지 전송이 실패했습니다.\u0026rdquo; 라는 메세지를 출력합니다.\n백준봇 초대하기 초대 링크\n현재 너무 바빠 업데이트를 못 하고 있는데, 추후 다양한 기능을 업데이트할 예정입니다.\n","date":"2023-01-03T00:00:00Z","image":"https://gyeongmin.kr/p/baekjoonbot/image-1_hu3ba0e11ccd78cd419cf9cf4dfafe4b05_113858_120x120_fill_box_smart1_3.png","permalink":"https://gyeongmin.kr/p/baekjoonbot/","title":"백준 디스코드 봇 '백준봇'"},{"content":" 문제 링크 : https://www.acmicpc.net/problem/1011\n문제 우현이는 어린 시절, 지구 외의 다른 행성에서도 인류들이 살아갈 수 있는 미래가 오리라 믿었다. 그리고 그가 지구라는 세상에 발을 내려 놓은 지 23년이 지난 지금, 세계 최연소 ASNA 우주 비행사가 되어 새로운 세계에 발을 내려 놓는 영광의 순간을 기다리고 있다.\n그가 탑승하게 될 우주선은 Alpha Centauri라는 새로운 인류의 보금자리를 개척하기 위한 대규모 생활 유지 시스템을 탑재하고 있기 때문에, 그 크기와 질량이 엄청난 이유로 최신기술력을 총 동원하여 개발한 공간이동 장치를 탑재하였다. 하지만 이 공간이동 장치는 이동 거리를 급격하게 늘릴 경우 기계에 심각한 결함이 발생하는 단점이 있어서, 이전 작동시기에 k광년을 이동하였을 때는 $k-1$ , $k$ 혹은 $k+1$ 광년만을 다시 이동할 수 있다. 예를 들어, 이 장치를 처음 작동시킬 경우 -1 , 0 , 1 광년을 이론상 이동할 수 있으나 사실상 음수 혹은 0 거리만큼의 이동은 의미가 없으므로 1 광년을 이동할 수 있으며, 그 다음에는 0 , 1 , 2 광년을 이동할 수 있는 것이다. ( 여기서 다시 2광년을 이동한다면 다음 시기엔 1, 2, 3 광년을 이동할 수 있다. )\n김우현은 공간이동 장치 작동시의 에너지 소모가 크다는 점을 잘 알고 있기 때문에 $x$지점에서 $y$지점을 향해 최소한의 작동 횟수로 이동하려 한다. 하지만 $y$지점에 도착해서도 공간 이동장치의 안전성을 위하여 $y$지점에 도착하기 바로 직전의 이동거리는 반드시 1광년으로 하려 한다.\n김우현을 위해 $x$지점부터 정확히 $y$지점으로 이동하는데 필요한 공간 이동 장치 작동 횟수의 최솟값을 구하는 프로그램을 작성하라.\n입력 입력의 첫 줄에는 테스트케이스의 개수 $T$가 주어진다. 각각의 테스트 케이스에 대해 현재 위치 $x$ 와 목표 위치 $y$ 가 정수로 주어지며, $x$는 항상 $y$보다 작은 값을 갖는다. $(0 ≤ x \u0026lt; y \u0026lt; 231)$\n출력 각 테스트 케이스에 대해 $x$지점으로부터 $y$지점까지 정확히 도달하는데 필요한 최소한의 공간이동 장치 작동 횟수를 출력한다.\n풀이 우선 거리 $d$를 구한다. $d = y - x$\n그리고 $d$보다 작은 최대의 제곱수 $k^2$를 구한다.\n거리가 제곱수일 때에 주목하여 규칙을 살펴보자.\nk 거리 이동 횟수 1 1 1 1 1 2 11 2 1 3 111 3 2 4 121 3 2 5 1211 4 2 6 1221 4 2 7 12211 5 2 8 12221 5 3 9 12321 5 3 10 123211 6 3 11 123221 6 3 12 123321 6 3 13 1233211 7 3 14 1233221 7 3 15 1233321 7 4 16 1234321 7 보다시피 제곱수일 때 이동을 살펴보면\n$$1,2,3,\u0026hellip;,k,\u0026hellip;,3,2,1$$\n1부터 k까지 1씩 증가하고, 다시 1까지 감소하는 것을 확인할 수 있다.\n이때, 이동 거리가 제곱수인 경우 이동 횟수의 일반항을 구할 수 있다.\n$$a_{n} = 2n-1$$\n거리가 제곱수인 경우를 이용해서 이외의 경우도 구할 수 있다.\n$d - k^2\u0026lt;k$ 인 경우, 이동 거리는 거리가 $k^2$일 때 횟수에 1을 더한 값이다.\n$d - k^2\u0026lt;2k$ 인 경우, 이동 거리는 거리가 $k^2$일 때 횟수에 2을 더한 값이다.\n이를 수식화하면 $2 * k - 1 + [(d - k^2 + k - 1) / k]$ 가 된다.\n파이썬 코드로는 2 * k - 1 + (d - k ** 2 + k - 1) // k 가 된다.\n소스 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 n = int(input()) for _ in range(n): x, y = map(int, input().split()) d = y - x # d보다 작은 최대의 제곱수 k^2 k = 0 while k ** 2 \u0026lt;= d: k += 1 k -= 1 result = 2 * k - 1 + (d - k ** 2 + k - 1) // k print(result) ","date":"2022-06-30T00:00:00Z","image":"https://gyeongmin.kr/images/boj.png","permalink":"https://gyeongmin.kr/p/boj-1011/","title":"BOJ 1011: Fly me to the Alpha Centauri (Python)"},{"content":"고정 소수점 표현 고정 소수점 방식에서는 정수 부분과 소수 부분을 이진수로 변환한 후 각각 고정된 위치에 표현한다. 아래 그림과 같이 첫번째 비트는 부호를, 그 다음 16비트는 정수를, 우측 15비트는 소수를 나타낸다.\n십진 실수를 고정 소수점으로 표현하는 과정은 다음과 같다.\n부호(sign)에 양수면 0, 음수면 1을 넣는다. 정수부를 이진화하여, 정수부에 넣는다. 남는 부분은 0으로 채운다. 소수부의 근사치를 이진화하여 소수부에 넣는다. 뒷 부분은 잘라내거나, 남는 부분은 0으로 채운다. 고정 소수점은 부동소수점에 비해 빠르고 간단하다는 장점이 있지만, 정수부로 사용 가능한 비트 수는 정해져 있기 때문에 큰 실수를 표현할 수 없다는 단점이 있다.\n부동 소수점 표현 부동소수점 수는 가수(mantissa)와 지수(exponent)로 나누어 표현한다. 아래 그림과 같이 첫번째 비트는 부호를, 그 다음 8비트는 지수를, 우측 23비트는 가수를 나타낸다.\n십진 실수를 부동 소수점으로 표현하는 과정은 다음과 같다.\n부호(sign)에 양수면 0, 음수면 1을 넣는다. $1.m \\times 2^{n}$ 형태로 수를 정규화한다. $m$은 가수, $n$은 지수이다. 정규화된 수의 소수부를 이진화하여 가수에 넣는다. 뒷 부분은 잘라내거나, 0으로 채운다. 지수에 편항(bias) 127을 더해 지수 부분에 담는다. 남는 부분은 0으로 채운다. 부동 소수점은 고정 소수점에 비해 더 큰 실수를 표현할 수 있기에 대부분 부동 소수점을 기본적으로 채택한다.\n정규화 부동소수점은 $1.m \\times 2^{e-\\text{bias}}$ 꼴로 표현되는데, 여기서 정규화란 가수를 $1.m$ 형태로 맞추는 과정이다. 정규화를 통해 동일한 크기의 비트로 더 넓은 범위의 실수를 다룰 수 있다.\n실수의 가장 왼쪽에 위치한 1이 가수의 첫 부분에 오도록 위치를 조정한다. 지수 $e$는 가수를 정규화한 결과에 맞게 증가하거나 감소한다. 예를 들어, 10진수 12.75를 부동 소수점으로 변환한다면, 아래와 같다.\n12.75를 이진수로 변환하면 $1100.11_2$이다. 정규화하면 $1.10011_2 \\times 2^3$이 된다. 여기서 $1.10011_2$는 가수, $3$은 지수가 된다. 정규화를 진행하면 항상 정수부가 1이기 때문에, 1은 굳이 저장하지 않고 소수부만 저장한다. 마찬가지로 $2^{e-\\text{bias}}$ 에서 $e$만 가수에 저장한다.\n편향 (Bias) 부호 없는 정수 형태로 지수를 표현하기 위해, 실제 지수 값에 특정 값(편향)을 더해 저장한다. 일반적으로 IEEE 754 표준에서는 편향 값으로 $2^{n-1} - 1$을 사용한다. 32비트 단정밀도는 지수 비트수가 8이므로 편향 값은 $127$이다. 계산할 땐 다시 편향 값을 더해 복구한다.\n양수와 음수 지수를 모두 다룰 수 있고, 하드웨어 구현이 더 간단해지기 때문에 편향을 이용하여 표현한다. 또한, 0을 정확히 표현할 수 있고, infinity와 NaN을 표현할 수 있다.\n부동소수점 오차 부동소수점의 가장 큰 한계 중 하나는 바로 오차이다.\n근사 오차 (Rounding Error)\n부동소수점은 유한한 비트로 실수를 표현하기 때문에, 일부 값은 근사치로 표현될 수밖에 없다. 예를 들어, 10진수 0.1은 2진수로 정확히 표현할 수 없기 때문에 근사치로 저장된다.\n연산 오차 (Arithmetic Error)\n두 개 이상의 부동소수점 수를 연산할 때, 소수점 아래 비트가 잘리거나 반올림되면서 오차가 발생한다. 특히, 값의 크기 차이가 클수록 오차가 커질 수 있다.\n특히, 부동소수점을 == 비교하는 것은 자제하는 것이 옳다.\n실수를 더 정확하게 연산하는 방법 정수 연산 치환 항상 소수부가 2자리라면, 100을 곱해 정수로 치환한 뒤, 계산한 후 다시 소수로 변환하는 방법이 있다.\n보다 큰 자료형 선택 더 많은 비트를 사용하여, 더욱 정밀도를 높일 수 있다. double 대신 long double, __int128을 사용하면 된다.\n분수 클래스 사용 파이썬의 Fraction과 같이, 모든 수를 분수 형태로 표현하여 연산한다면 정확한 실수 연산이 가능하다.\n고정 소수점 사용 부동소수점 대신 고정 소수점을 사용하면 일부 환경에서는 오차를 줄일 수 있다. 특히, 값의 범위가 작고 정밀도가 중요한 경우에 적합하다.\n","date":"2022-04-20T00:00:00Z","image":"https://gyeongmin.kr/images/computer-system-architecture.png","permalink":"https://gyeongmin.kr/p/fixed-point-and-floating-point/","title":"고정 소수점과 부동 소수점 표현"},{"content":" 본 포스팅은 \u0026lsquo;Mano의 컴퓨터시스템구조\u0026rsquo; 교재를 참고했습니다.\n진법 N진법은 수를 셀 때 자릿수가 올라가는 단위를 기준으로 하는 셈법으로, 위치적 기수법이라고도 한다.\n우리가 일반적으로 수를 셀 때는 10진법을 사용한다. 시계에서 시간은 12진법을, 분은 60진법을 사용한다. 진법은 분명히 표시하기 위해 다음과 같이 첨자를 붙이기도 한다.\n$$ (101101)2 = (45){10} $$\n2진법 (Binary) 2진법은 0과 1이라는 두개의 숫자만을 사용하여 수를 나타내는 것이다. 2가 되는 순간 자리올림이 발생한다.\n$$ (1011)_2=\\mathtt{0b1011} $$\n10진법 (Decimal) 우리가 가장 일반적으로 사용하고 있는 기수법으로, 한 자리에 0~9의 숫자로 나타낸다. 9를 넘어서면 자리올림이 발생한다.\n$$ 724.5 = 7 \\times 10^2 + 2 \\times 10^1 + 4 \\times 10^0 + 5 \\times 1010^{-1} $$\n16진법 (Hexadecimal) 0~F까지 사용한다. 컴퓨터 분야에서 1바이트의 크기를 쉽게 표현할 수 있어 많이 사용된다.\n$$ (5A)_{16}=\\mathtt{0x5A} $$\n컴퓨터는 십진수를 이진화 집진법(BCD)의 형태로 저장하고 표현한다.\n진법의 변환 10진수 -\u0026gt; N진수 10진수를 N으로 나누고, 나머지를 기록한다. 나머지를 기록한다. (뒤에서 앞으로) 나눈 몫이 N보다 작으면 멈춘다. 마지막 몫을 기록한다. N진수 -\u0026gt; 10진수 1의 자리수부터 N의 0승, N의 1승, \u0026hellip; 이렇게 차례대로 곱하여 더해주면 된다.\n$$ (1011)_2 = 1 \\times 2^3 + 0 \\times 2^2 + 1 \\times 2^1 + 1 \\times 2^0 = 11 $$\n이를 수식으로 일반화하면 아래와 같다.\n$$ (a_k a_{k-1} \\cdots a_1 a_0)N = \\sum{i=0}^{k} a_i \\times N^i = (x)_{10} $$\n진법 변환 C++ 구현 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 int charToInt(char c) { if (c \u0026gt;= \u0026#39;0\u0026#39; \u0026amp;\u0026amp; c \u0026lt;= \u0026#39;9\u0026#39;) { return c - \u0026#39;0\u0026#39;; } else { return c - \u0026#39;A\u0026#39; + 10; } } char intToChar(int num) { if (num \u0026gt;= 0 \u0026amp;\u0026amp; num \u0026lt;= 9) { return char(\u0026#39;0\u0026#39; + num); } else { return char(\u0026#39;A\u0026#39; + num - 10); } } int convertToDecimal(const string \u0026amp;num, int base) { int decimal = 0; for (int i = 0; i \u0026lt; num.length(); ++i) { decimal += charToInt(num[i]) * pow(base, num.length() - i - 1); } return decimal; } string convertFromDecimal(int num, int base) { string ret; while (num \u0026gt; 0) { ret += intToChar(num % base); num /= base; } reverse(ret.begin(), ret.end()); return ret; } 보수 보수(compleement)는 디지털 컴퓨터에서 뺄셈 연산과 논리 계산에 사용된다. $r$진법에는 $r$의 보수와 $(r-1)$의 보수가 있다.\n(r-1)의 보수 일반적으로 $r$진법의 $n$자리수의 수 $N$에 대하여, $(r-1)$의 보수는 $(r^n-1)-N$으로 정의된다. 예를 들어 10진수 $546700$에 대한 9의 보수는 $999999 - 546700 = 453299$이다. 이진수에서 1의 보수는 각 자리 수를 뒤집는 것이다.\nr의 보수 일반적으로 $r$진법의 $n$자리수의 수 $N$에 대하여, $r$의 보수는 $N \\neq 0$일 때 $r^n-N$이고, $N=0$일 때는 0으로 정의된다. $r$의 보수는 $(r-1)$의 보수에 1을 더한 것과 같다.\n보수의 대칭 어떤 수에 대한 보수를 다시 보수화하면 원래 수가 된다.\n$r^n-(r^n-N)=N$이고, $(r^n-1)-((r^n-1)-N)=N$ 이므로, 보수의 대칭성을 만족하는 것을 확인할 수 있다.\n부호 없는 숫자의 뺄셈 $r$진수 부호 없는 두 $n$자리수 사이의 뺼셈 $M-N(N \\neq 0)$은 다음과 같이 계산된다.\n피감수 $M$에 감수 $N$에 대한 $r$의 보수를 더한다. $M+(r^n-N)=M-N+r^n$ $M \\geq N$이라면, 위의 값은 end캐리 $r^n$을 만들어내고, 이를 무시하면 $M-N$을 얻을 수 있다. $M \u0026lt; N$이라면, 위의 값은 end캐리를 만들어내지 않고 그 값은 $r^n-(N-M)$이다. 이것은 $(N-M)$에 대한 $r$보수이므로, 이것에 대한 $r$의 보수를 취하고 앞에 뺄셈 기호를 붙여 뺼셈을 할 수 있다. ","date":"2022-04-15T00:00:00Z","image":"https://gyeongmin.kr/images/computer-system-architecture.png","permalink":"https://gyeongmin.kr/p/notation-and-complement/","title":"진법과 보수"},{"content":" 본 포스팅은 \u0026lsquo;Mano의 컴퓨터시스템구조\u0026rsquo; 교재를 참고했습니다.\n레지스터 $n$비트의 레지스터는 $n$비트의 이진 정보를 저장하기 위한 $n$개의 플립플롭과 데이터 처리를 위한 조합 회로로 구성되어 있다.\n위 레지스터는 아무런 외부 게이트를 가지지 않고 플립플롭으로만 구성된 가장 단순한 형태의 레지스터이다. 클리어 입력은 그 값이 0이 될 경우, 클럭 동작과 관계 없이 레지스터의 모든 플립플롭 출력을 0으로 만든다.\n대부분의 디지털 시스템은 지속적으로 클럭 펄스를 제공하는 주 클럭 발생기를 가지고 있다. 따라서 특정 레지스터에 지정된 클럭 펄스만이 영향을 줄 수 있도록 하는 제어 신호가 필요하다. 위 4비트 레지스터는 로드 제어 입력을 가지고 있어 클럭 펄스의 작용 여부를 결정할 수 있다.\n로드 입력이 0인 경우는 입력이 차단되고 플립플롭의 D 입력은 자신의 출력에 연결된다. 이와 같은 피드백 연결로 레지스터의 내용은 불변하게 된다.\n버퍼 게이트는 클럭 발생기로부터의 전력 소모를 줄이는 역할을 한다.\n시프트 레지스터 레지스터에 저장되어 있는 이진 정보를 단방향 혹은 양방향으로 이동시킬 수 있는 것이 시프트 레지스터이다. 시프트 레지스터의 각 플립플롭들은 각각의 입력과 출력이 연쇄적으로 연결되어 있고, 공통의 클럭 펄스가 다음 상태로의 이동을 제어한다.\n위 그림에서 serial input은 시프트 될 경우 가장 왼쪽 플립플롭에 들어갈 값을 결정하며, serial output은 가장 오른쪽 플립플롭의 출력이다.\n시프트 레지스터에서 원하지 않는 클럭을 제한함으로써 특정 클럭 펄스에만 시프트가 일어나도록 제어할 수 있다. 위 그림에서는 클럭을 AND게이트의 한 입력에 연결하고, 다른 입력에는 제어 신호를 연결함으로써 구현하였다. 또한 D 입력을 제어하는 추가 회로를 이용하여 구현할 수도 있다.\n시프트 레지스터는 주로 원격지 시스템 사이에 데이터를 전송하고자 할 때 사용한다. 즉 원거리의 두 지점 간에 $n$비트의 전송이 필요할 때 $n$개의 라인을 이용하여 병렬적으로 전송하는 것보다 하나의 라인을 통해 한 비트씩 보내는 것이 더 경제적이다. 송신 측에서는 병렬에서 직렬로 변환을 하여 전송하고, 수신 측에서는 받은 데이터를 다시 병렬로 변환하는 것이다.\n이진 카운터 입력 펄스에 따라 미리 정해진 순서대로 상태 변이가 진행되는 레지스터를 카운터라고 한다. 입력 펄스가 클럭 펄스를 사용하거나 외부로부터 얻을 수도 있다. 입력 펄스의 시간 간격은 일정할수도 있고 랜덤할수도 있다.\n카운터는 어떤 사건의 발생 횟수를 세거나, 동작 순서를 제어하는 타이밍 신호를 만드는 데 사용된다. 이진수의 순서를 따르는 카운터를 이진 카운터라고 한다. $n$개의 플립플롭을 가진 $n$비트 이진 카운터는 0에서 $(2^n-1)$까지 카운트한다.\n카운터 회로는 보통 보수화 기능을 가지고 있는 T 플립플롭이나 JK 플립플롭을 이용한다.\n카운터의 초기값 설정을 위해 병렬 로드 기능을 가진 카운터가 필요하다. 클리어 입력이 1일 대, 모든 플립플롭의 K 입력이 1로 설정되기 때문에 다음 클럭 변이에서 모든 플립플롭 출력이 0으로 클리어된다.\n클리어 입력과 로드 입력이 모두 0이고, increment 입력이 1일 때, 정상적인 동작을 수행한다.\n메모리 장치 메모리 장치는 정보의 입출력 기능을 가지고 있는 저장요소들의 집합으로서, 입출력에서 하나의 단위로 취급되는 비트의 그룹인 word로 정보를 저장한다. 즉 메모리 워드는 1과 0의 비트 그룹으로 숫자, 명령어, 문자 등의 이진화된 정보를 저장한다.\n특히 8비트로 이루어진 비트 그룹을 바이트(byte)라고 하며, 대부분의 컴퓨터 메모리는 8배수 크기의 워드를 채택한다.\n메모리 내부 구조는 한 워드를 구성하는 비트 수나 전체 워드 수에 의해 규정된다. 메모리의 각 워드는 0에서부터 $2^k-1$까지의 주소를 가지고 있어 주소 입력(k개의 입력 라인)의 값에 따라 특정 워드가 선택된다. 내부의 디코더가 이런 선택 동작을 수행한다. 따라서 컴퓨터의 메모리가 $2^n$개의 워드인 경우에는 $n$비트의 주소가 필요하다. 보통\nRAM RAM(Random Access Memory)에서는 워드의 물리적인 위치에 관계없이 접근 절차나 접근 시간이 동일하다. 메모리와 외부 세계와의 통신은 데이터 입출력 라인, 주소 라인, 제어 라인을 통해 이루어진다. RAM은 데이터 입력을 위한 쓰기 동작과 데이터 출력을 위한 읽기 동작을 제공하며, 제어 신호에 의해 선택된다.\n위 블럭도에 나타난 RAM은 $n$개의 입력과 출력을 가지고 있고, $k$개의 주소 라인으로 메모리 내의 $2^k$개의 워드 중에서 하나를 선택할 수 있다. 두 개의 제어 입력은 데이터의 전송 방향을 지정한다.\n하나의 새로운 워드가 전송되어 메모리에 저장될 때엔 다음과 같은 과정이 필요하다.\n원하는 워드의 이진 주소값을 주소 입력에 넣는다. 메모리에 저장될 데이터 비트들을 데이터 입력에 넣는다. 쓰기 입력을 활성화한다. 메모리에 저장된 한 워드를 꺼내오는 데엔 다음과 같은 과정이 필요하다.\n원하는 워드의 이진 주소값을 주소 입력에 넣는다. 읽기 입력을 활성화한다. ROM 읽기전용 메모리(Read-Only Memory, ROM)는 데이터가 한 번 저장되면 하드웨어의 수명이 다할 때까지 내용이 변경되지 않는다. ROM은 쓰기 동작이 허용되지 않으며, 한 번 저장된 데이터는 영구적으로 유지된다.\n반면, RAM과 같은 다른 메모리는 회로가 동작하는 동안 데이터를 자유롭게 읽고 쓰는 기능이 있지만, 전원이 차단되면 데이터가 사라진다.\nROM은 내부에 특정 전자적 구조를 통해 데이터를 저장한다. 데이터를 저장하기 위해 ROM 내부에는 프로그램 가능한 전자적 퓨즈(fuse)가 존재한다. 이러한 퓨즈는 특정 패턴을 형성하며, 이를 통해 전원이 끊어지더라도 데이터가 안정적으로 유지된다. ROM은 저장된 데이터를 직접 읽어내는 방식으로 작동하며, 데이터의 주소를 입력받아 고정된 출력값을 제공한다. 따라서 읽기 제어 입력이 필요하지 않아 구조적으로 간단하면서도 신뢰성이 높은 특징을 가진다.\nROM의 저장 방식은 매우 효율적이며, 제어 회로와 디코더의 결합을 통해 작동한다. ROM에 저장된 데이터는 주소 입력값에 따라 즉시 출력값이 결정되기 때문에 빠른 응답 속도를 제공한다. RAM과 달리, ROM은 데이터를 읽는 용도로만 사용되며 추가적인 저장 기능은 필요하지 않다. 이는 ROM이 데이터의 저장뿐 아니라 제어 회로의 설계에도 적합한 이유이다.\n컴퓨터 시스템에서 ROM은 주로 고정된 프로그램 저장 용도로 사용된다. 이는 운영체제의 초기 부팅 과정이나 하드웨어와의 상호작용을 위한 기초 제어 코드를 포함한다. 또한, ROM은 제어 회로에서 내부 변수의 상태 변화를 기록하거나 변경되지 않는 상수를 저장하는 데 활용된다. 예를 들어, 전자 기기의 기본 동작을 정의하는 펌웨어는 ROM에 저장되어 장치의 동작을 안정적으로 지원한다.\nROM의 종류 ROM은 정보 저장 방식에 따라 다양한 종류로 나뉜다. 가장 기본적인 형태는 제조 공정 단계에서 데이터를 미리 프로그래밍하는 고정된 프로그램 ROM(Mask ROM)이다. 이 방식은 고객이 요청한 데이터를 기반으로 제조 과정에서 ROM에 고정 데이터를 삽입한다. 대량 생산 시 경제적 효율성을 제공하지만, 동일한 ROM을 소량 주문할 경우 높은 비용이 발생하는 단점이 있다.\n프로그래밍 가능한 ROM(PROM)은 초기 상태에서 모든 출력이 1로 설정된 상태에서 시작한다. 사용자는 PROM 프로그래머라는 장비를 이용해 특정 비트를 끊어 데이터를 설정할 수 있다. PROM은 한 번 데이터가 기록되면 수정이 불가능하기 때문에 실험용 데이터 저장에 주로 사용된다.\n지울 수 있는 PROM(EPROM)은 한 단계 더 진화한 기술이다. EPROM은 자외선 노출을 통해 데이터를 초기 상태로 복구할 수 있으며, 이를 통해 데이터를 재프로그래밍하는 기능을 제공한다. 이 기술은 제한된 수정 가능성을 제공하지만, 여전히 물리적 절차를 요구한다.\n전기적 소거가 가능한 PROM(EEPROM)은 ROM 기술의 최종 진화 형태로, 전기적 신호를 통해 데이터를 수정하거나 삭제할 수 있다. EEPROM은 높은 유연성을 제공하며, 현대의 컴퓨터와 전자 장치에서 자주 사용된다. 이는 소규모 데이터를 반복적으로 갱신해야 하는 환경에서 매우 유용하다.\n","date":"2022-04-12T00:00:00Z","image":"https://gyeongmin.kr/images/computer-system-architecture.png","permalink":"https://gyeongmin.kr/p/register/","title":"레지스터"},{"content":" 본 포스팅은 \u0026lsquo;Mano의 컴퓨터시스템구조\u0026rsquo; 교재를 참고했습니다.\n집적 회로 집적 회로(IC)는 디지털 게이트를 구성하는 전자 부품들을 포함하는 실리콘 반도체(chip)이다. 기술이 발전함에 따라 칩 안에 담을 수 있는 게이트의 개수가 급격히 증가하였고, 집적된 정도에 따라 아래와 같이 소규모, 중규모, 대규모 집적 장치로 분류한다.\n소규모 집적 장치(SSI)는 10개 이하의 독립적인 게이트가 하나의 칩에 들어가 있고, 게이트의 입출력이 바로 외부 핀으로 연결된다. 중규모 집적 장치(MSI)는 10개에서 200개의 게이트가 들어가 있고, 디코더나 가산기, 레지스터와 같은 디지털 장치를 구현한다. 대규모 집적 장치(LSI)는 200에서 1000개의 게이트를 집적하고 프로세서나 메모리 칩과 같은 디지털 시스템을 형성한다. 초대규모 집적 장치(VLSI)는 수천 개의 게이트를 하나의 칩에 집적하여 대형 메모리나 복잡한 마이크로 컴퓨터 칩을 형성한다. 디지털 회로는 구현하는 데 적용된 기술에 따라 디지털 논리군으로 분류된다. 대표적으로 아래와 같은 것들이 있다.\nTTL: 트랜지스터-트랜지스터 논리 ECL: 에미터-결합 논리 MOS: 금속-산화물 반도체 CMOS: 상보 금속-산화물 반도체 TTL은 가장 많이 사용되고 있는 논리군이고, ECL은 고속도가 요구되는 시스템에 사용되며, MOS는 부품의 밀도가 높은 집적 회로에, CMOS는 적은 전력 소비가 요구되는 시스템에 많이 사용된다.\n디코더 $n$비트의 이진 코드는 서로 다른 $2^n$개의 원소 정보를 나타낼 수 있다. 디코더는 $n$비트로 코딩된 이진 정보를 최대 $2^n$개의 서로 다른 출력으로 바꾸어 주는 조합 회로이다. $n$개의 입력과 $m(m\u0026lt;2^n)$개의 출력을 가지는 디코더를 $n$대 $m$ 라인 디코더 혹은 $n \\times m$ 디코더라고 한다.\nE가 0일때 모든 출력은 항상 0이고, E가 1일때만 정상적으로 동작한다. 각 출력은 다른 일곱 개의 입력 조합에 대해서는 0이고, 오직 하나의 조합에 대해서만 1인 출력값을 가진다. 이것이 입력 이진수에 해당한는 8진수 값이라고 할 수 있다.\n보수화된 형태로 출력을 만드는 것이 더 경제적이기 때문에, NAND 게이트로 디코드를 형성하기도 한다.\n두 개 이상의 디코더를 동일한 인에이블 입력에 연결해 하나의 커다란 디코더를 구성할 수 있다. 즉 $4 \\times 16$ 디코더 네 개로 $16 \\times 64$ 디코더를 만들 수 있다.\n인코더 인코더는 디코더와 반대되는 동작을 수행하는 디지털 회로로서 $2^n$개 입력값에 대해 $n$개의 이진 코드를 출력한다.\n이 인코더는 진리표에 따라 세 개의 $\\text{OR}$ 게이트들로 구현할 수 있으며, 각 출력에 대한 부울식은 다음과 같다.\n$$ A_0 = D_1 + D_3 + D_5 + D_7 \\ A_1 = D_2 + D_3 + D_6 + D_7 \\ A_2 = D_4 + D_5 + D_6 + D_7 $$\n멀티플렉서 멀티플렉서는 $n$개의 선택 입력에 따라 $2^n$개의 출력을 하나의 출력에 선택적으로 연결시켜 주는 조합 회로이다. 멀티플렉서는 흔히 데이터 선택기(data selector)라고도 하며, 줄여서 MUX라고 쓴다.\n디코더에 $2^n$개의 입력 라인을 더하게 되면 $2^n$대 1 멀티플렉서를 구현할 수 있다. 디코더처럼 멀티플렉서도 동작을 제어하거나 확장을 위해 인에이블 입력을 가질 수 있다.\n위 멀티플렉서는 여섯 개의 입력을 가지기 때문에, $2^6=64$줄의 진리표가 필요하다. 하지만 위와 같이 함수표를 이용하면 간단하게 나타낼 수 있다.\n보통 하나의 IC칩에는 여러 개의 멀티플렉서가 포함된다. 이 회로는 함수표와 같이 두 개의 4비트 데이터를 선택적으로 출력해주는 멀티플렉서로 동작한다.\n","date":"2022-04-09T00:00:00Z","image":"https://gyeongmin.kr/images/computer-system-architecture.png","permalink":"https://gyeongmin.kr/p/ic-and-decoder-and-encoder-and-mux/","title":"집적 회로, 디코더, 인코더, 멀티플렉서"},{"content":" 본 포스팅은 \u0026lsquo;Mano의 컴퓨터시스템구조\u0026rsquo; 교재를 참고했습니다.\n순차 회로 순차 회로 (Sequential Circuit)란, 입력 및 현재 상태에 따라 출력 및 다음 상태가 결정되는 논리회로이다. 결국 현재의 입력과 과거의 출력 상태에 의해 출력값이 결정된다.\n게이트로만 이루어진 회로는 조합회로이고, 플립플롭과 게이트로 이루어진 회로는 순차 회로이다.\n동기 순차회로와 비동기 순차회로 동기 순차회로는 모두 같은 하나의 클럭을 서로 공유하고, 플립플롭들이 같은 시간에 동작한다.\n비동기 순차회로는 클럭을 서로 공유하지 않고, 플립플롭들이 각자 동작한다.\n비동기 순차회로는 각자 제어하거나 타이밍 신호를 해석하는 것이 복잡하므로, 컴퓨터 시스템과 같은 디지털 시스템은 대부분이 동기 순차회로이다.\n무어 머신과 밀리 머신 무어 머신 (Moore Machine)은 출력이 현재 상태에 의해서 만 결정된다. 즉, 플립플롭 출력들(현재 상태들)의 조합에 의해서 만 결정된다.\n$\\text{출력} = f(\\text{상태})$ $\\text{다음상태} = f(\\text{입력}, \\ \\text{현재상태})$ 밀리 머신 (Mealy Machine)은 출력이 현재 상태와 입력 모두에 의해서 결정된다. 즉, 같은 상태라도 입력에 따라서 달라질 수 있다.\n$\\text{출력} = f(\\text{입력}, \\ \\text{현재상태})$ $\\text{다음상태} = f(\\text{입력}, \\ \\text{현재상태})$ 밀리 머신과는 달리, 무어 머신은 상태가 변할 때만 출력이 변하여 더 간단한 구조이다. 무어 머신이 제어하기에 더 간단하며, 대부분의 디지털 시스템은 무어 머신이다.\n순차 회로의 설계 순차 회로를 설계할 때는 좌측에서 우측으로, 순차 회로를 분석할 때는 우측에서 좌측 단계로 진행된다.\nflowchart LR 기능정의[기능 정의\n기능 설명] \u003c--\u003e 상태도[상태도\nState Diagram] 상태도 \u003c--\u003e 상태표[상태표\nState Table] 상태표 \u003c--\u003e 입력논리식[입력 논리식\n출력 논리식] 입력논리식 \u003c--\u003e 논리회로도[논리회로도\nLogic Diagram] 순차 회로의 특성은 입력, 출력, 플립플롭의 상태로 만들어진다. 출력과 다음 상태는 입력과 현재 상태에 대한 함수인데, 이 사이의 관계를 상태표라고 한다.\n상태도 상태는 원으로 표시하고, 상태 사이의 전이는 원 사이를 연결하는 직선으로 표시한다. 일반적으로 원 안에는 플립플롭의 상태를 적고, 간선에는 입력/출력값을 적는다.\n상태표 $m$개의 플립플롭, $n$개의 입력 변수, $p$개의 출력 변수를 가지고 있는 순차 회로는 현상태에 $m$개의 열, 입력에 $n$개의 열, 출력에 $p$개의 행을 갖는 상태표가 된다. 또한 행에는 $2^{m+n}$개의 조합이 나오게 된다. 다음 상태와 출력열은 입력 변수의 함수이고 회로로부터 직접 구해진다.\n위 상태도 예시를 상태표로 바꾸면 다음과 같다.\n현재 상태 입력 다음 상태 출력 00 0 00 0 00 1 01 0 01 0 00 1 01 1 11 0 10 0 00 1 10 1 11 0 11 1 10 0 11 0 00 1 위와 같이 상태도를 가지고 상태표를 만들 수 있고, 상태표를 가지고 상태도를 만들 수 있다.\n","date":"2022-03-29T00:00:00Z","image":"https://gyeongmin.kr/images/computer-system-architecture.png","permalink":"https://gyeongmin.kr/p/boolean-algebrad/","title":"순차 회로"},{"content":" 본 포스팅은 \u0026lsquo;Mano의 컴퓨터시스템구조\u0026rsquo; 교재를 참고했습니다.\n플립플롭 플립플롭(flip-flop)은 한 비트의 정보를 저장하는 이진 셀(cell)로서, 정상 출력과 보수화된 출력을 가지고 있다. 플립플롭은 입력 펄스가 상태 변환을 일으키기 전까지 이진 상태를 계속 유지한다. 전기 신호가 지속적으로 공급 되어야만 정보를 유지할 수 있는 휘발성 메모리이다.\n입력의 수와 입력이 이진 상태에 영향을 미치는 방식에 따라 여러 종류로 분류할 수 있다.\nSR 플립플롭 $S$(set), $R$(reset), $C$(clock)로 이루어진 세 개의 입력과 하나의 출력 $Q$ 를 가지고 있으며, 경우에 따라 작은 원을 기호로 하는 보수화된 출력을 갖기도 한다. $C$에 있는 화살표는 동적 입력(dynamic input)을 나타내는 것으로, 플립플롭이 입력 클럭 신호의 상승 변이에서 동작함을 의미한다.\n$S=1, R=0$이면 $Q=1$이 되고, $S=0,R=1$이면 $Q=0$이 된다. $S=R=0$이면 $Q$는 이전 상태를 유지하고, $S=R=1$이면 $Q$값은 랜덤하다.\nD 플립플롭 D(data) 플립플롭은 SR 플립플롭의 S와 R 입력을 인버터로 연결하고 D라는 기호를 붙인 것이다. $D$ 값이 그대로 저장된다.\nD 플립플롭은 불변조건 $[Q(t+1)=Q(t)]$가 없기 때문에, 불변 조건을 만들기 위해서는 클럭을 끊거나 출력을 입력으로 되돌려 주어야 한다.\nJK 플립플롭 SR 플립플롭에서 $S=R=1$ 일 때 $Q$ 값이 랜덤하다는 단점을 보완한 것이 JK 플립플롭이다. $J=K=1$일 때 클럭 펄스는 플립플롭의 출력을 보수로 만든다. 이를 수식으로 표현하면 $Q(t+1)=Q^\\prime (t)$ 이다.\nT 플립플롭 T(toggle) 플립플롭은 JK 플립플롭에서 입력 J와 K를 T 하나로 묶은 것이다. $T=0$인 경우 상태의 변화가 없고, $T=0$인 경우 상태는 보수가 된다. 회로도를 보면서 이해하면 편하다.\nEdge-Triggered 플립플롭 상태 변경을 클럭 펄스의 변이 동안 동기화하는 것을 모서리-변이형 플립플롭이라고 한다. 입력 펄스가 일정한 임계값을 넘어설 때 입력값을 고정시켜 클럭 펄스가 0이 되거나 다음 펄스가 올 때까지 값을 유지한다. 클럭 펄스가 상승할 때 (Positive-edge-triggered) 반응하거나, 하강할 때 (Negative-edge-triggered) 변이한다. 입력 $D$의 변이가 효과를 미치기 위해서는 입력 $D$의 값이 일정하게 유지되어야 하는 최소 시간인 set up time과 클럭 변이 후 $D$의 값이 변화하지 않아야 하는 hold time을 필요로 한다. 전파 지연 시간보다 짧게 클럭 펄스가 High를 유지한다면, Race 현상을 방지할 수 있다.\n또 다른 형태로, master-slave 플립플롭이 있다. 이것은 클럭이 1일 때 반응하는 master 플립플롭과 클럭이 0일 때 반응하는 slave 플립플롭으로 구성된다. 클럭이 1에서 0으로 변할 때 입력이 출력으로 전달되는 효과를 가져 Race 현상을 방지할 수 있다.\n","date":"2022-03-10T00:00:00Z","image":"https://gyeongmin.kr/images/computer-system-architecture.png","permalink":"https://gyeongmin.kr/p/flip-flop/","title":"플립플롭"},{"content":" 본 포스팅은 \u0026lsquo;Mano의 컴퓨터시스템구조\u0026rsquo; 교재를 참고했습니다.\n조합 회로 조합 회로는 입력과 출력을 가진 논리 게이트의 집합으로 출력의 값은 0과 1들의 조합의 함수이다. 조합 회로와 상반되는 순차 회로는 게이트와 플립플롭 등이 있다.\n조합 회로를 설계 절차는 다음과 같다.\n문제가 제시된다. 입력과 출력 변수에 문자 기호를 붙인다. 입력과 출력 사이의 관계를 정의하는 진리표를 유도한다. 각 출력에 대한 간소화된 부울 함수를 얻는다. 논리도를 그린다. 반가산기 반가산기는 비트 두 개를 서로 산술적으로 가산하는 조합 회로이다.\n위 그림에서 C는 Carry(캐리, 자리올림)이고, S는 Sum이다.\n$A=0, B=1$ 이면 $0+1$ 이므로 $S=1, C=0$이다.\n$A=1, B=1$ 이면 $1+1$ 이므로 $S=1, C=1$이다.\nA, B 모두 1일 때만 C가 1이 된다. 따라서 C는 $\\text{AND}$ 게이트로 구현할 수 있다.\nA, B가 서로 다르면 S가 1이 된다. 따라서 S는 $\\text{XOR}$ 게이트로 구현할 수 있다.\n따라서 반가산기의 논리 표현식은 아래와 같다.\n$$ \\begin{aligned} S \u0026amp;= x{^\\prime}y+xy{^\\prime} = x⊕y \\ C \u0026amp;= xy \\end{aligned} $$\n전가산기 전가산기는 비트 두개와 밑의 자리에서 올라오는 캐리까지 고려하여 비트 세 개를 가산하는 조합 회로이다.\nA, B, X에서 1의 개수가 1개 혹은 3개이면 S가 1이 된다.\nA, B, X에서 1의 개수가 2개 혹은 3개이면 C가 1이 된다.\n전가산기의 논리 표현식은 아래와 같다. 전가산기는 반가산기 두 개와 $\\text{OR}$ 게이트로 구현할 수 있다.\n$$ \\begin{aligned} S \u0026amp;= x⊕y⊕z \\ C \u0026amp;= xy+(x⊕y)z \\end{aligned} $$\n전가산기를 4개 이어붙이면 4비트 덧셈기가 되고, 32개를 이어붙이면 32비트 덧셈기가 된다.\n","date":"2022-02-24T00:00:00Z","image":"https://gyeongmin.kr/images/computer-system-architecture.png","permalink":"https://gyeongmin.kr/p/half-adder-and-full-adder/","title":"반가산기와 전가산기"},{"content":" 본 포스팅은 \u0026lsquo;Mano의 컴퓨터시스템구조\u0026rsquo; 교재를 참고했습니다.\n맵과 민텀 논리표현식은 부울 대수를 사용해서 간단히 만들 수 있으나 여러 가지 규칙이 있다. 맵 방법은 karnaugh 맵과 Veitch 다이어그램이 있다.\n진리표에서 각 변수의 각 조합을 민텀(minterm)이라고 한다. n개의 변수가 있으면 $2^n$개의 민텀이 있다.\nA B C F 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 \u0026lt;Truth Table 1\u0026gt; 위 진리표를 민텀으로 표현하면 아래와 같다.\n$$ \\begin{aligned} F(x,y,z) \u0026amp;= \\sum (1,4,5,6,7) \\ \u0026amp;= x{^\\prime}y{^\\prime}z+xy{^\\prime}z{^\\prime}+xy{^\\prime}z+xyz{^\\prime}+xyz \\end{aligned} $$\n맵은 여러 개의 사각형으로 이루어지고, 각 사각형의 구역은 각각의 민텀을 표시한다. 함수가 1이 될 때 해당 구역에 1을 넣고, 0일 땐 0을 넣거나 빈 칸으로 둔다.\n맵은 두 변수일 땐 $2 \\times 2$, 세 변수일 땐 $2 \\times 4$, 네 변수일 땐 $4 \\times 4$ 크기로 그린다. 그 이상은 맵 방식으로 하면 복잡하여 다른 방법을 사용한다. 아래는 그 예시이다.\nB A A B 1 O X 2 O X \u0026lt;2 변수 맵 예제\u0026gt; BC A 00 01 11 10 0 0 1 1 0 1 1 1 0 1 \u0026lt;3 변수 맵 예제\u0026gt; CD AB 00 01 11 10 00 1 0 1 0 01 0 1 0 1 11 1 1 0 1 10 0 0 0 1 \u0026lt;4 변수 맵 예제\u0026gt; 맵을 이용한 간소화 카르노 맵을 이용한 간소화는 텍스트로 설명하면 이해하기 어렵다. 그 대신, 카르노 맵을 이용한 간소화를 매우 쉽게 설명한 \u0026lsquo;전기는빠지직(송건웅)\u0026lsquo;님의 유튜브 영상을 첨부한다.\nhttps://youtu.be/IsMRUf_3m6U?si=ocTJN__kqXcgR3ko\n","date":"2022-02-03T00:00:00Z","image":"https://gyeongmin.kr/images/computer-system-architecture.png","permalink":"https://gyeongmin.kr/p/karnaugh-map/","title":"카르노 맵과 부울 식의 간소화"},{"content":" 본 포스팅은 \u0026lsquo;Mano의 컴퓨터시스템구조\u0026rsquo; 교재를 참고했습니다.\n2진법과 논리 게이트 2진법은 0과 1이라는 두개의 숫자만을 사용하여 수를 나타내는 것이다.\n디지털 컴퓨터는 0과 1 두개의 숫자만을 사용하는데, 하나의 이진 숫자를 bit라고 부른다. 컴퓨터는 전압 신호를 이용하여 0과 1로 표현한다.\n이전 정보의 처리는 게이트라 불리는 논리 회로에서 이루어진다. 아래 표는 각 게이트의 이름, 대수 표현식, 진리표를 나타낸 것이다.\nName Algebraic function Truth table $\\text{AND}$ $x = A \\cdot B$ ABx 000 010 100 111 $\\text{OR}$ $x = A + B$ ABx 000 011 101 111 $\\text{Inverter}$ $x = A^\\prime$ Ax 01 10 $\\text{Buffer}$ $x = A$ Ax 00 11 $\\text{NAND}$ $x = (A · B)^\\prime$ ABx 001 011 101 110 $\\text{NOR}$ $x = (A + B)^\\prime$ ABx 001 010 100 110 $\\text{XOR}$ $x = A ⊕ B$ ABx 000 011 101 110 $\\text{XNOR}$ $x = (A ⊕ B)^\\prime$ ABx 001 010 100 111 부울 대수 부울 대수는 19세기 중반 조지 불이 만든 대수 체계로, 디지털 회로의 해석과 설계를 쉽게 하는 것이 목적이다.\n부울 대수의 기본 관계는 아래와 같다. 각각은 모두 진리표로 증명할 수 있다.\n부울 대수의 기본 관계 $x + 0 = x$ $x \\cdot 0 = 0$ $x + 1 = 1$ $x \\cdot 1 = x$ $x + x = x$ $x \\cdot x = x$ $x + x^\\prime= 1$ $x \\cdot x^\\prime 0$ $x + y = y + x$ $x \\cdot y = y \\cdot x$ $x + (y + z) = (x + y) + z$ $x \\cdot (y \\cdot z) = (x \\cdot y) \\cdot z$ $x (y + z) = xy + xz$ $x + yz = (x + y)(x + z)$ $(x + y)^\\prime= x^\\prime \\cdot y^\\prime $ $(x \\cdot y)^\\prime = x^\\prime + y^\\prime $ $(x^\\prime)^\\prime = x$ 위 식은 부울 대수의 기본적인 관계를 나타낸다. 부울대수는 교환 법칙과 결합 법칙이 성립한다. 또한 식 15번과 16번은 드 모르간의 정리이다.\n수식의 보수 드 모르간의 정리는 모든 $\\text{OR}$ 연산은 $\\text{AND}$ 로, 모든 $\\text{AND}$ 연산은 $\\text{OR}$ 로 바꾸어 주고, 각 변수를 보수화하면 간단히 적용할 수 있다.\n예들들어 다음과 같이 수식의 보수를 만들 수 있다.\n$$ F=AB+C^\\prime D^\\prime +B^\\prime D $$ $$ F^\\prime =(A^\\prime +B^\\prime )(C+D)(B+D^\\prime ) $$\n부울 대수의 활용 부울 대수를 통해 디지털 회로를 간단히 하는 데 사용할 수 있다. 아래와 같은 회로 $F$가 있다고 가정해 보자.\n$$ F=ABC+ABC^\\prime +A^\\prime C $$\n부울 대수를 적용하면, $(C+C)^\\prime =1$이고, $AB \\cdot 1=AB$ 이므로,\n$$ F=ABC+ABC^\\prime +A^\\prime C=AB(C+C^\\prime )+A^\\prime C=AB+A^\\prime C $$\n이다. 따라서 4개의 게이트만 사용하여 효율적인 회로를 설계할 수 있는 것이다.\n부울 대수 문제 $A \\cdot (A + B) = A$ 임을 보여라.\n풀이 $$ \\begin{aligned} AA + AB \u0026= A + AB \\\\ \u0026= A (1 + B) \\\\ \u0026= A \\cdot 1 \\\\ \u0026= A \\end{aligned} $$ $ (A + B) \\cdot (A + B^\\prime ) = A $ 임을 보여라.\n풀이 $$ \\begin{aligned} AA + AB{^\\prime} + AB + BB{^\\prime} \u0026= AA + AB{^\\prime} + AB + BB{^\\prime} \\\\ \u0026= A + A(B{^\\prime} + B) \\\\ \u0026= A + A \\cdot 1 \\\\ \u0026= A \\end{aligned} $$ ","date":"2022-01-26T00:00:00Z","image":"https://gyeongmin.kr/images/computer-system-architecture.png","permalink":"https://gyeongmin.kr/p/boolean-algebra/","title":"부울 대수"},{"content":"윈도우 패키지 매니저 Chocolatey 컴퓨터를 포맷한 후 초기 설정할 때 유용하게 사용중인 프로그램이다.\n여태 프로그램을 설치하기 위해선 프로그램을 검색한 후, 사이트에 들어가서 설치 파일을 다운받은 후, 설치 파일을 실행시켜 일일히 Next를 눌러 가며 설치해야 했다.\nChocolatey를 사용하면 이 과정을 choco install python -y 와 같이 명령어 한 줄로 설치할 수 있다.\n미리 설치 명령어가 담긴 배치 파일을 만들어 둔다면, 포맷 후 한번에 모든 프로그램을 설치할 수 있다.\nchoco upgrade all 명령어를 사용하면 모든 프로그램을 업데이트할 수 있다.\n설치 방법 공식 홈페이지에서 설치 커맨드를 복사한다.\n윈도우 + X 키 를 눌러 Windows 터미널(Windows PowerShell)을 관리자 모드로 열어준 후, 복사한 커맨드를 붙여넣기한 후 엔터를 눌러주면 설치가 진행된다.\n사용 방법 여기서 설치하고 싶은 패키지를 검색한다.\n커맨드를 복사한 후, Windows PowerShell에서 커맨드를 실행한다. 조금만 기다리면 패키지가 설치된다.\n커맨드 패키지 검색 choco search 패키지명\n패키지 설치 choco install 패키지명\n패키지 설치 (프롬프트/버전 포함) choco install -y 패키지명 --version 버전\n설치된 패키지 버전 업그레이드 choco upgrade 패키지명\n패키지 상세 정보 choco info 패키지명\n설치된 패키지 확인 choco list -l\n설치된 패키지 삭제 choco uninstall 패키지명\nchocolatey 버전 업그레이드 choco upgrade chocolatey Chocolatey와 패키지를 한번에 설치하는 스크립트 아래 소스 코드는 본 작성자가 사용하고 있는 스크립트인데, 아래 주석 부분을 수정해서 사용하면 된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 @echo off CLS ECHO ************************************** ECHO * Start Chocolatey Batch ECHO ************************************** NET FILE 1\u0026gt;NUL 2\u0026gt;NUL if \u0026#39;%errorlevel%\u0026#39; == \u0026#39;0\u0026#39; ( goto gotPrivileges ) else ( goto getPrivileges ) :getPrivileges if \u0026#39;%1\u0026#39;==\u0026#39;ELEV\u0026#39; (shift \u0026amp; goto gotPrivileges) ECHO. ECHO ************************************** ECHO * Use UAC, switch to admin ECHO ************************************** setlocal DisableDelayedExpansion set \u0026#34;batchPath=%~0\u0026#34; setlocal EnableDelayedExpansion ECHO Set UAC = CreateObject^(\u0026#34;Shell.Application\u0026#34;^) \u0026gt; \u0026#34;%temp%\\OEgetPrivileges.vbs\u0026#34; ECHO UAC.ShellExecute \u0026#34;!batchPath!\u0026#34;, \u0026#34;ELEV\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;runas\u0026#34;, 1 \u0026gt;\u0026gt; \u0026#34;%temp%\\OEgetPrivileges.vbs\u0026#34; \u0026#34;%temp%\\OEgetPrivileges.vbs\u0026#34; exit /B :gotPrivileges setlocal \u0026amp; pushd . WHERE choco 1\u0026gt;NUL 2\u0026gt;NUL if \u0026#39;%errorlevel%\u0026#39; == \u0026#39;0\u0026#39; ( goto chocoInstalled ) else ( goto chocoMissing ) :chocoMissing ECHO. choice /M \u0026#34;Chocolatey not found. Install now?\u0026#34; IF \u0026#39;%errorlevel%\u0026#39; == \u0026#39;2\u0026#39; exit /B ECHO. ECHO ************************************** ECHO * Chocolatey install ECHO ************************************** @powershell -NoProfile -ExecutionPolicy Bypass -Command \u0026#34;iex ((new-object net.webclient).DownloadString(\u0026#39;https://chocolatey.org/install.ps1\u0026#39;))\u0026#34; \u0026amp;\u0026amp; SET PATH=%PATH%;%ALLUSERSPROFILE%\\chocolatey\\bin :chocoInstalled ECHO. ECHO ************************************** ECHO * Packages install ECHO ************************************** @echo on choco feature enable --name=allowGlobalConfirmation choco upgrade all -y :::::::::::::::::::::::: 이 부분을 변경하세요 :::::::::::::::::::::::: :: 설치할 어플리케이션 목록 set choco_install=choco install -fy %choco_install% bandizip %choco_install% potplayer %choco_install% everything %choco_install% discord %choco_install% leagueoflegends %choco_install% steam %choco_install% notion %choco_install% zoom %choco_install% webex %choco_install% github-desktop %choco_install% jetbrainstoolbox %choco_install% vscode %choco_install% visualstudio2022community %choco_install% qbittorrent %choco_install% obs-studio :: chocolatey에 없는 프로그램들 다운로드 링크 링크 열기 start explorer https://creativecloud.adobe.com/apps/download/creative-cloud?locale=ko# start explorer https://app-pc.kakaocdn.net/talk/win32/KakaoTalk_Setup.exe start explorer https://www.onstove.com/download ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: choco feature disable --name=allowGlobalConfirmation RefreshEnv.cmd pause ","date":"2022-01-15T00:00:00Z","image":"https://gyeongmin.kr/p/chocolatey/image_hu626c632ff351f32c14b2e58c83a851de_69786_120x120_fill_box_smart1_3.png","permalink":"https://gyeongmin.kr/p/chocolatey/","title":"윈도우 패키지 매니저 Chocolatey"},{"content":"C언어로 만드는 리듬게임 1학년 텀프로젝트 과제로 만들었던 리듬게임입니다.\n리듬게임 자체를 만드는건 오래 걸리지 않았던 것 같은데, 노트들의 싱크를 맞춘다고 정말 고생했던 기억이 납니다.\n사용법 D, F, J, K 키를 사용하여 노트를 입력합니다.\n정확하게 노트를 입력하면 Excellent, 조금 느리게 입력하면 Good 판정이 됩니다.\n연속으로 노트를 입력하게 되면 콤보가 쌓입니다.\n콤보와 판정을 통해 점수가 쌓입니다. 점수 계산 식은 다음과 같습니다.\n1 score += (combo / 10 + 1) * 50 설정 메뉴에서 두가지 옵션을 켜고 끌 수 있습니다.\n\u0026lsquo;노트 찍기 모드\u0026rsquo;를 켜면 에디터 모드가 활성화 됩니다. 노래는 나오지만 노트는 내려오지 않습니다. 이 때, 노래에 맞춰 키보드를 입력하면 노트가 \u0026rsquo;note.txt\u0026rsquo; 파일에 저장되게 됩니다.\n\u0026lsquo;디버깅 모드\u0026rsquo;를 켜면 디버깅 모드가 활성화 됩니다. 화면이 새로고침 되는 딜레이를 확인할 수 있습니다.\n설명 시작 부분의 간단한 텍스트 애니메이션은 for문을 통해 구현하였습니다. 이차원 char 배열로 텍스트를 저장해둔 뒤, 한 줄씩 출력/지우기를 반복하는 방식입니다.\n노래를 선택하게 되면 노트가 저장된 텍스트 파일에서 char 배열로 버퍼를 받습니다. 0이면 노트가 없는 것이고, 1이면 노트가 있는 것으로 판단합니다.\n노래는 PlaySound 함수를 사용하여 재생하였습니다.\n노트는 텍스트 색상을 바꾼 뒤, 공백을 출력하여 리듬게임의 노트인 것처럼 구현하였습니다. SetConsoleTextAttribute 함수를 통해 텍스트의 색상을 바꾸어 주었습니다.\n한 화면의 딜레이는 노래의 BPM에 맞춰주었습니다. 노트가 빠르게 출력되어야 하기에 필연적으로 딜레이에 편차가 생기게 됩니다. 반복문의 시작과 끝에 시간을 측정하여, 딜레이를 균일하게 보정해 주었습니다.\n더블 버퍼링을 응용하여 프론트 버퍼와 백 버퍼가 바뀌는 부분만 공백으로 지운 후, 출력하는 방식으로 구현하였습니다.\n키 입력을 받는 것은 GetAsyncKeyState 함수를 사용하였습니다. 멀티 쓰레드를 위해 _beginthreadex 함수를 사용하였습니다. 여러 키를 동시에 입력받을 수 있게 해 줍니다.\n판정선에 노트가 들어왔을 때, 화면 버퍼와 키 입력 상태를 비교하여 노트 입력 판정을 구분합니다. 프론트 버퍼와 백 버퍼의 상태를 비교하여 롱 노트와 숏 노트를 구분합니다. 노트를 정확하게 눌렀다면 Excellent, 빠르게 눌렀다면 Good, 놓쳤다면 Miss로 구분합니다. key_state 변수를 만들어 키를 꾹 눌러 점수를 얻는 것을 방지하였습니다.\n게임 중 Excellent, Good, Miss 판정을 기록합니다. 기록된 판정의 비율에 따라 랭크가 부여됩니다. 게임이 끝나게 되면 판정들을 보여주고, 비율에 따라 그래프를 그려 성취도를 직관적으로 보여줍니다.\n","date":"2021-12-15T00:00:00Z","image":"https://gyeongmin.kr/p/rhythm-game-in-c/image_hu3ff4f112a1159188cba29d5e373d96a1_132087_120x120_fill_box_smart1_3.png","permalink":"https://gyeongmin.kr/p/rhythm-game-in-c/","title":"C언어로 만드는 리듬게임"}]