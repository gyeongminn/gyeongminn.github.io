<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>자연어 처리 on Gyeongmin의 개발 블로그</title><link>https://gyeongmin.kr/tags/%EC%9E%90%EC%97%B0%EC%96%B4-%EC%B2%98%EB%A6%AC/</link><description>Recent content in 자연어 처리 on Gyeongmin의 개발 블로그</description><generator>Hugo -- gohugo.io</generator><language>ko</language><copyright>Gyeongmin Lee</copyright><lastBuildDate>Thu, 21 Mar 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://gyeongmin.kr/tags/%EC%9E%90%EC%97%B0%EC%96%B4-%EC%B2%98%EB%A6%AC/index.xml" rel="self" type="application/rss+xml"/><item><title>언어 모델</title><link>https://gyeongmin.kr/p/language-model/</link><pubDate>Thu, 21 Mar 2024 00:00:00 +0000</pubDate><guid>https://gyeongmin.kr/p/language-model/</guid><description>&lt;img src="https://gyeongmin.kr/images/pytorch-transformer-nlp-computer-vision.png" alt="Featured image of post 언어 모델" />&lt;h2 id="언어-모델">언어 모델&lt;/h2>
&lt;p>언어 모델 (Language Model)은 입력된 문장을 기반으로 각 문장이 생성될 확률을 계산하는 모델이다. 문맥을 이해하고, 문장 구조를 예측하는 역할을 한다. 자동 번역, 음성 인식, 텍스트 요약 등에 활용된다.&lt;/p>
&lt;p>문장 전체를 완벽히 예측하는 것은 어렵기 때문에 조건부 확률을 이용한 방식이 필요하다.&lt;/p>
&lt;h2 id="자기회귀-언어-모델">자기회귀 언어 모델&lt;/h2>
&lt;p>자기회귀 언어 모델 (Autoregressive Language Model)은 이전 단어들의 조건부 확률을 이용하여 다음 단어를 예측하는 모델이다. 문장 내의 모든 단어 시퀀스를 고려하여 다음 단어가 등장할 확률을 계산한다.&lt;/p>
&lt;p>자기회귀 모델에서는 문장의 확률을 다음과 같이 표현할 수 있다.&lt;/p>
&lt;p>$$
P(W_1, W_2, &amp;hellip;, W_n) = P(W_1) \ P(W_2 | W_1) \ P(W_3 | W_1, W_2) \ &amp;hellip; \ P(W_n | W_1, &amp;hellip;, W_{n-1})
$$&lt;/p>
&lt;p>즉, 전체 문장의 확률은 각 단어의 조건부 확률의 곱으로 나타낼 수 있다. 이는 연쇄법칙(Chain Rule)을 적용한 결과로, 각 단어의 확률을 이전 단어들의 조건부 확률을 이용해 순차적으로 계산하는 방식이다.&lt;/p>
&lt;h2 id="통계적-언어-모델">통계적 언어 모델&lt;/h2>
&lt;p>통계적 언어 모델 (Statistical Language Model)은 언어의 통계적 구조를 분석하여 단어 시퀀스를 생성하거나 분석하는 기법이다. 가장 기본적인 방식은 마르코프 체인(Markov Chain)을 활용하여 확률을 예측하는 것이다.&lt;/p>
&lt;p>문장에서 특정 단어가 등장할 확률을 단어 빈도수를 기반으로 계산할 수 있다.&lt;/p>
&lt;p>예를 들어, 말뭉치에서 &lt;code>안녕하세요&lt;/code>라는 단어가 1000번 등장했고, 그다음 &lt;code>만나서&lt;/code>가 700번 등장했다면 아래와 같이 계산할 수 있다.&lt;/p>
&lt;p>$$
P(만나서 | 안녕하세요) = \frac{P(안녕하세요 만나서)}{P(안녕하세요)} = \frac{700}{1000} = 0.7
$$&lt;/p>
&lt;p>하지만, 한 번도 등장한 적이 없는 단어에 대해서는 확률을 정확하게 예측할 수 없는 문제(데이터 희소성, Data Sparsity) 가 발생할 수 있다.&lt;/p>
&lt;h2 id="n-gram-언어-모델">N-gram 언어 모델&lt;/h2>
&lt;p>N-gram 모델은 연속된 N개의 단어를 하나의 단위로 취급하여 문장을 분석하는 방식이다. N의 값에 따라 다음과 같이 구분할 수 있다.&lt;/p>
&lt;ul>
&lt;li>Unigram (N=1): 단어 하나씩 독립적으로 분석&lt;/li>
&lt;li>Bigram (N=2): 두 개의 연속된 단어를 분석&lt;/li>
&lt;li>Trigram (N=3): 세 개의 연속된 단어를 분석&lt;/li>
&lt;/ul>
&lt;p>N-gram 모델에서 특정 단어의 확률은 이전 (N-1)개의 단어를 고려하여 계산된다.&lt;/p>
&lt;p>$$
P(W_t | W_{t-1}, W_{t-2}, &amp;hellip;, W_{t-N+1})
$$&lt;/p>
&lt;p>즉, 이전 N-1개 단어의 조합을 기반으로 다음 단어의 확률을 예측하는 방식이다.&lt;/p>
&lt;p>&lt;code>nltk&lt;/code>을 사용하여 N-gram을 간단히 사용할 수 있다.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">nltk&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">ngrams&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sentence&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">words&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">sentence&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">split&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ngrams&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">zip&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">words&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">:]&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">)])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ngrams&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">sentence&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;안녕하세요 만나서 진심으로 반가워요&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">unigram&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ngrams&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sentence&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">bigram&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ngrams&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sentence&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">trigram&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ngrams&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sentence&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">unigram&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">bigram&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">trigram&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">unigram&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nltk&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ngrams&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sentence&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">split&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">bigram&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nltk&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ngrams&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sentence&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">split&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">trigram&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nltk&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ngrams&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sentence&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">split&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">list&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">unigram&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">list&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">bigram&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">list&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">trigram&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">[(&amp;#39;안녕하세요&amp;#39;,), (&amp;#39;만나서&amp;#39;,), (&amp;#39;진심으로&amp;#39;,), (&amp;#39;반가워요&amp;#39;,)]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">[(&amp;#39;안녕하세요&amp;#39;, &amp;#39;만나서&amp;#39;), (&amp;#39;만나서&amp;#39;, &amp;#39;진심으로&amp;#39;), (&amp;#39;진심으로&amp;#39;, &amp;#39;반가워요&amp;#39;)]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">[(&amp;#39;안녕하세요&amp;#39;, &amp;#39;만나서&amp;#39;, &amp;#39;진심으로&amp;#39;), (&amp;#39;만나서&amp;#39;, &amp;#39;진심으로&amp;#39;, &amp;#39;반가워요&amp;#39;)]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">[(&amp;#39;안녕하세요&amp;#39;,), (&amp;#39;만나서&amp;#39;,), (&amp;#39;진심으로&amp;#39;,), (&amp;#39;반가워요&amp;#39;,)]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">[(&amp;#39;안녕하세요&amp;#39;, &amp;#39;만나서&amp;#39;), (&amp;#39;만나서&amp;#39;, &amp;#39;진심으로&amp;#39;), (&amp;#39;진심으로&amp;#39;, &amp;#39;반가워요&amp;#39;)]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">[(&amp;#39;안녕하세요&amp;#39;, &amp;#39;만나서&amp;#39;, &amp;#39;진심으로&amp;#39;), (&amp;#39;만나서&amp;#39;, &amp;#39;진심으로&amp;#39;, &amp;#39;반가워요&amp;#39;)]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item></channel></rss>