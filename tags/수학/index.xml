<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>수학 on Gyeongmin의 개발 블로그</title><link>https://gyeongmin.kr/tags/%EC%88%98%ED%95%99/</link><description>Recent content in 수학 on Gyeongmin의 개발 블로그</description><generator>Hugo -- gohugo.io</generator><language>ko</language><copyright>Gyeongmin Lee</copyright><lastBuildDate>Mon, 26 Feb 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://gyeongmin.kr/tags/%EC%88%98%ED%95%99/index.xml" rel="self" type="application/rss+xml"/><item><title>정칙화</title><link>https://gyeongmin.kr/p/regularization/</link><pubDate>Mon, 26 Feb 2024 00:00:00 +0000</pubDate><guid>https://gyeongmin.kr/p/regularization/</guid><description>&lt;img src="https://gyeongmin.kr/images/pytorch-transformer-nlp-computer-vision.png" alt="Featured image of post 정칙화" />&lt;h2 id="정칙화regularization">정칙화(Regularization)&lt;/h2>
&lt;p>머신러닝과 딥러닝 모델을 학습시킬 때, 오버피팅은 모델이 훈련 데이터에 지나치게 적합되어 새로운 데이터에 대한 예측 성능이 저하되는 오버피팅(overfitting) 문제는 흔히 접해보았을 것이다. 이러한 문제를 해결하기 위해 사용되는 기술이 바로 정칙화(Regularization)이다.&lt;/p>
&lt;p>정칙화는 모델이 암기(Memorization)가 아니라 일반화(Generalization)할 수 있도록 손실 함수에 규제(Penalty)를 가하는 방식이다.&lt;/p>
&lt;p>정칙화를 적용하면 학습 데이터들이 갖고 있는 작은 차이점에 대해 덜 민감해져 모델의 분산 값이 낮아진다. 그러므로 정칙화는 모델이 데이터를 학습할 때 의존하는 특징의 수를 줄임으로써 모델의 추론 능
력을 개선한다.&lt;/p>
&lt;h2 id="오버피팅과-일반화">오버피팅과 일반화&lt;/h2>
&lt;p>&lt;img src="https://gyeongmin.kr/p/regularization/image.png"
width="1400"
height="609"
srcset="https://gyeongmin.kr/p/regularization/image_hu66396dc1fab62456da9622b86cbec07b_591451_480x0_resize_box_3.png 480w, https://gyeongmin.kr/p/regularization/image_hu66396dc1fab62456da9622b86cbec07b_591451_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="Example of underfitting and overfitting"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="551px"
>&lt;/p>
&lt;p>오버피팅은 모델이 훈련 데이터의 노이즈나 특정 패턴을 학습하여, 실제 데이터의 일반적인 패턴을 파악하지 못할 때 발생한다. 이로 인해 모델은 훈련 데이터에서는 높은 성능을 보이지만, 새로운 데이터에서는 성능이 급격히 떨어진다. 반면, 일반화는 모델이 새로운 데이터에서도 정확한 예측을 수행할 수 있는 능력을 의미한다.&lt;/p>
&lt;p>즉, 모델이 데이터의 일반적인 패턴을 학습하여 노이즈에 의존하지 않고, 다양한 데이터에 대해 일관된 성능을 유지하는 것이다.&lt;/p>
&lt;h2 id="정칙화의-종류">정칙화의 종류&lt;/h2>
&lt;h3 id="l1-정칙화">L1 정칙화&lt;/h3>
&lt;p>L1 정칙화는 라쏘 정칙화(Lasso Regularization)라고도 불리며, 가중치의 절댓값 합을 손실 함수에 추가하여 오버피팅을 방지한다.&lt;/p>
&lt;p>이 방식은 모델이 불필요한 피처의 가중치를 0으로 수렴시키는 특징이 있어, 자동으로 특징 선택(feature selection)의 효과를 제공한다. 그러나 L1 정칙화는 하이퍼파라미터인 규제 강도(lambda)를 적절히 조절해야 하며, 과도한 규제는 정보의 손실을 초래할 수 있다. 주로 선형 회귀 모델에서 활용되며, 계산 복잡도가 다소 높을 수 있다는 단점이 있다.&lt;/p>
&lt;p>$$
\text{Loss}&lt;em>\text{L1} = \text{Loss}&lt;/em>{\text{original}} + \lambda \sum_{i=1}^{n} |w_i|
$$&lt;/p>
&lt;ul>
&lt;li>$ \text{Loss}_{L1} $: L1 정칙화가 적용된 전체 손실 함수&lt;/li>
&lt;li>$ \text{Loss}_{\text{original}} $: 원래의 손실 함수 (ex. 평균 제곱 오차)&lt;/li>
&lt;li>$ \lambda $: 규제 강도 하이퍼파라미터&lt;/li>
&lt;li>$ w_i $: 각 가중치 파라미터&lt;/li>
&lt;li>$ n $: 가중치의 총 개수&lt;/li>
&lt;/ul>
&lt;h3 id="l2-정칙화">L2 정칙화&lt;/h3>
&lt;p>L2 정칙화는 릿지 정칙화(Ridge Regularization)라고도 하며, 가중치 제곱의 합을 손실 함수에 추가하여 오버피팅을 방지한다.&lt;/p>
&lt;p>L2 정칙화는 가중치를 0에 가깝게 유지하므로, 모델의 가중치가 균일하게 분포되도록 도와준다. 이는 모델의 복잡도를 조정하여 일반화 성능을 향상시키는 데 기여한다. L1 정칙화와 달리, L2 정칙화는 모든 가중치를 조금씩 줄이는 경향이 있어, 희소성을 제공하지 않는다. 주로 심층 신경망 모델에서 많이 사용되며, 하이퍼파라미터 조정이 필요하다.&lt;/p>
&lt;p>$$
\text{Loss}&lt;em>\text{L2} = \text{Loss}&lt;/em>{\text{original}} + \lambda \sum_{i=1}^{n} w_i^2
$$&lt;/p>
&lt;ul>
&lt;li>$ \text{Loss}_{L2} $: L2 정칙화가 적용된 전체 손실 함수&lt;/li>
&lt;/ul>
&lt;h3 id="가중치-감쇠">가중치 감쇠&lt;/h3>
&lt;p>가중치 감쇠는 L2 정칙화와 유사하게, 손실 함수에 규제 항을 추가하여 모델의 가중치를 작게 유지하는 기법이다.&lt;/p>
&lt;p>딥러닝 라이브러리에서는 종종 최적화 함수에 weight_decay 파라미터로 구현되며, L2 정칙화와 동일한 효과를 가진다. 가중치 감쇠는 모델의 일반화 성능을 향상시키기 위해 사용되며, 다른 정칙화 기법과 함께 적용할 때 더욱 효과적일 수 있다.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="n">optimizer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">optim&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">SGD&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">parametersO&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.01&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">weight_decay&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.01&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="엘라스틱-넷">엘라스틱 넷&lt;/h3>
&lt;p>엘라스틱 넷(Elastic-Net)은 L1 정칙화와 L2 정칙화를 결합한 방식으로, 두 정칙화의 장점을 동시에 활용한다.&lt;/p>
&lt;p>이는 모델이 희소성과 작은 가중치의 균형을 맞추도록 도와주며, 특히 피처의 수가 샘플의 수보다 많을 때 유의미한 성능 향상을 제공한다. 혼합 비율을 조절하여 두 정칙화의 영향을 조절할 수 있으나, 새로운 하이퍼파라미터가 추가되므로 튜닝이 필요하다.&lt;/p>
&lt;p>$$
\text{Loss}&lt;em>\text{ElasticNet} = \text{Loss}&lt;/em>{\text{original}} + \lambda_1 \sum_{i=1}^{n} |w_i| + \lambda_2 \sum_{i=1}^{n} w_i^2
$$&lt;/p>
&lt;ul>
&lt;li>$ \lambda_1 $: L1 정칙화의 규제 강도&lt;/li>
&lt;li>$ \lambda_2 $: L2 정칙화의 규제 강도&lt;/li>
&lt;/ul>
&lt;h3 id="드롭아웃">드롭아웃&lt;/h3>
&lt;p>&lt;img src="https://gyeongmin.kr/p/regularization/image-1.png"
width="848"
height="462"
srcset="https://gyeongmin.kr/p/regularization/image-1_hu1d510e10439ac30aa1a3a4f0b21d87ec_84154_480x0_resize_box_3.png 480w, https://gyeongmin.kr/p/regularization/image-1_hu1d510e10439ac30aa1a3a4f0b21d87ec_84154_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="Dropout before/after"
class="gallery-image"
data-flex-grow="183"
data-flex-basis="440px"
>&lt;/p>
&lt;p>드롭아웃(Dropout)은 신경망의 훈련 과정에서 일부 노드를 임의로 제거하거나 0으로 설정하여 오버피팅을 방지하는 기법이다.&lt;/p>
&lt;p>이는 노드 간의 동조화(co-adaptation)를 억제하여 모델이 특정 노드에 지나치게 의존하지 않도록 한다. 드롭아웃은 모델의 일반화 성능을 향상시키는 동시에 모델 평균화 효과를 제공하지만, 충분한 데이터와 깊은 모델에 적용할 때 더욱 효과적이다. 배치 정규화와 함께 사용할 때는 신중하게 조합해야 한다.&lt;/p>
&lt;p>$$
y = \begin{cases}
0 &amp;amp; \text{with probability } p \
\frac{y}{1-p} &amp;amp; \text{with probability } 1-p
\end{cases}
$$&lt;/p>
&lt;ul>
&lt;li>$ y $: 뉴런의 출력값&lt;/li>
&lt;li>$ p $: 뉴런을 제거할 확률 (드롭아웃 비율)&lt;/li>
&lt;li>$ 1-p $: 뉴런을 유지할 확률&lt;/li>
&lt;li>출력값을 $ \frac{1}{1-p} $로 스케일링하여 훈련 시와 추론 시의 활성화 분포를 일치&lt;/li>
&lt;/ul>
&lt;h3 id="그레이디언트-클리핑">그레이디언트 클리핑&lt;/h3>
&lt;p>그레이디언트 클리핑(Gradient Clipping)은 모델 학습 시 기울기가 너무 커지는 현상을 방지하기 위해 사용되는 기법이다.&lt;/p>
&lt;p>이는 기울기의 크기를 특정 임곗값으로 제한하여, 학습 과정에서 발생할 수 있는 기울기 폭주 문제를 해결한다. 주로 순환 신경망(RNN)이나 LSTM 모델에서 활용되며, 학습률을 조절하는 효과와 유사한 역할을 한다. 그레이디언트 클리핑은 하이퍼파라미터인 최대 임곗값을 신중하게 설정해야 하며, 이를 통해 모델의 안정적인 학습을 도모할 수 있다.&lt;/p>
&lt;p>$$
\text{if } ||g||_2 &amp;gt; r, \quad g \leftarrow \frac{g}{||g||_2} \times r
$$&lt;/p>
&lt;ul>
&lt;li>$ g $: 기울기 벡터&lt;/li>
&lt;li>$ ||g||_2 $: 기울기 벡터의 L2 노름&lt;/li>
&lt;li>$ r $: 설정한 임계값 (threshold)&lt;/li>
&lt;li>기울기의 방향은 유지하면서 크기를 $ r $로 제한&lt;/li>
&lt;/ul></description></item><item><title>활성화 함수</title><link>https://gyeongmin.kr/p/activation-functions/</link><pubDate>Wed, 14 Feb 2024 00:00:00 +0000</pubDate><guid>https://gyeongmin.kr/p/activation-functions/</guid><description>&lt;img src="https://gyeongmin.kr/images/pytorch-transformer-nlp-computer-vision.png" alt="Featured image of post 활성화 함수" />&lt;h2 id="활성화-함수">활성화 함수&lt;/h2>
&lt;p>활성화 함수는 인공 신경망에서 뉴런의 출력을 비선형으로 변환하여 은닉층을 활성화하는 역할을 한다. 이를 통해 네트워크는 데이터의 복잡한 패턴을 학습하고 비선형 문제를 해결할 수 있다.&lt;/p>
&lt;p>각 노드의 전달 보강이 다르므로 입력값에 따라 일부 노드는 활성화(Activate)되고 다른 노드는 비활성화(Deactivate)된다.&lt;/p>
&lt;p>활성화 함수는 비선형 구조를 가지며 미분 가능해야 학습이 가능하고, 입력값을 정규화(Normalization)하는 효과도 수행한다.&lt;/p>
&lt;h2 id="계단-함수-step-function">계단 함수 (Step Function)&lt;/h2>
&lt;p>계단 함수는 입력값이 특정 임곗값(보통 0)을 넘으면 1을 출력하고, 그렇지 않으면 0을 출력하는 함수이다.&lt;/p>
&lt;p>이 함수는 출력이 이산적이므로 단순한 분류 작업에 사용될 수 있지만, 비연속적인 특성으로 인해 기울기(Gradient)가 존재하지 않아 역전파(Backpropagation)를 사용할 수 없다.&lt;/p>
&lt;p>$$
f(x) =
\begin{cases}
1 &amp;amp; \text{if } x \geq 0 \
0 &amp;amp; \text{if } x &amp;lt; 0
\end{cases}
$$&lt;/p>
&lt;pre class="mermaid">xychart-beta
title "Step Function"
y-axis 0 --> 1
x-axis [-4, -3, -2, -1, 0, 0, 1, 2, 3, 4]
line [ 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
&lt;/pre>
&lt;h2 id="임곗값-함수-threshold-function">임곗값 함수 (Threshold Function)&lt;/h2>
&lt;p>임곗값 함수는 계단 함수의 변형으로 특정 임계값을 기준으로 값을 출력한다. 입력값이 임계값 이상이면 1을 출력하고, 그 미만이면 특정 값을 출력한다.&lt;/p>
&lt;p>이 함수는 이진 분류에 사용될 수 있으나, 계단 함수와 마찬가지로 기울기가 없어서 신경망 학습에는 적합하지 않다.&lt;/p>
&lt;p>$$
f(x) =
\begin{cases}
x &amp;amp; \text{if } x &amp;gt; threshold \
value &amp;amp; \text{else } \ \ \ otherwise
\end{cases}
$$&lt;/p>
&lt;pre class="mermaid">xychart-beta
title "Threshold Function (value = -5)"
y-axis -5 --> 4
x-axis [-4, -3, -2, -1, 0, 0, 1, 2, 3, 4]
line [-5, -5, -5, -5, -5, 0, 1, 2, 3, 4]
&lt;/pre>
&lt;h2 id="시그모이드-함수-sigmoid-function">시그모이드 함수 (Sigmoid Function)&lt;/h2>
&lt;p>시그모이드 함수는 입력값을 0과 1 사이의 연속적인 값으로 변환하는 함수로, 출력값을 확률로 해석할 수 있어 이진 분류 문제에서 사용된다.&lt;/p>
&lt;p>함수의 출력이 부드럽게 변하므로 미분이 가능하지만, 큰 입력값에서는 기울기가 0에 가까워지는 &lt;strong>Vanishing Gradient&lt;/strong> 문제가 발생할 수 있어 깊은 신경망에서는 성능이 저하될 수 있다.&lt;/p>
&lt;p>$$
f(x) = \frac{1}{1 + e^{-x}}
$$&lt;/p>
&lt;pre class="mermaid">xychart-beta
title "Sigmoid Function"
y-axis 0 --> 1
x-axis [-5, -4.5, -4, -3.5, -3, -2.5, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5]
line [0.0067, 0.011, 0.018, 0.029, 0.047, 0.076, 0.119, 0.182, 0.269, 0.378, 0.5, 0.622, 0.731, 0.818, 0.881, 0.924, 0.953, 0.971, 0.982, 0.989, 0.993]
&lt;/pre>
&lt;h2 id="하이퍼볼릭-탄젠트-함수-tanh-function">하이퍼볼릭 탄젠트 함수 (Tanh Function)&lt;/h2>
&lt;p>하이퍼볼릭탄젠트 함수는 시그모이드 함수의 확장판으로, 출력 범위가 -1에서 1 사이로 설정되어 있다.&lt;/p>
&lt;p>이 함수는 평균이 0에 가까워져 학습이 비교적 안정적이며, 시그모이드 함수보다 빠르게 수렴할 수 있다. 그러나 여전히 큰 입력값에서는 기울기가 0에 가까워지는 문제를 완전히 해결하지는 못한다.&lt;/p>
&lt;p>$$
f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$&lt;/p>
&lt;pre class="mermaid">xychart-beta
title "Tanh Function"
y-axis -1 --> 1
x-axis [-5, -4.5, -4, -3.5, -3, -2.5, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5]
line [-1, -0.999, -0.999, -0.998, -0.995, -0.986, -0.964, -0.905, -0.761, -0.462, 0, 0.462, 0.761, 0.905, 0.964, 0.986, 0.995, 0.998, 0.999, 0.999, 1]
&lt;/pre>
&lt;h2 id="relu-함수-rectified-linear-unit-function">ReLU 함수 (Rectified Linear Unit Function)&lt;/h2>
&lt;p>ReLU(Rectified Linear Unit) 함수는 입력값이 0 이하이면 0을 출력하고, 그 이상이면 입력값을 그대로 출력하는 함수이다.&lt;/p>
&lt;p>간단한 수식과 계산량 덕분에 학습 속도가 빠르고 효율적이지만, 0 이하의 값에서는 기울기가 0이 되어 뉴런이 더 이상 업데이트되지 않는 죽은 뉴런 문제가 발생할 수 있다.&lt;/p>
&lt;p>$$
f(x) = \max(0, x)
$$&lt;/p>
&lt;pre class="mermaid">xychart-beta
title "ReLU Function"
y-axis -0.5 --> 3
x-axis [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]
line [0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5]
&lt;/pre>
&lt;h2 id="leaky-relu-함수">Leaky ReLU 함수&lt;/h2>
&lt;p>Leaky ReLU 함수는 ReLU 함수의 단점을 개선한 형태로, 입력값이 0 이하일 때도 작은 기울기
α를 곱한 값을 출력한다. 이로 인해 죽은 뉴런 문제를 완화하며 학습이 계속 이루어질 수 있도록 한다.&lt;/p>
&lt;p>기울기 α는 보통 0.01과 같은 작은 값으로 설정되며, ReLU의 비선형성과 계산 효율성을 유지한다.&lt;/p>
&lt;p>$$
f(x) =
\begin{cases}
x &amp;amp; \text{if } x &amp;gt; 0 \
\alpha x &amp;amp; \text{if } x \leq 0
\end{cases}
$$&lt;/p>
&lt;pre class="mermaid">xychart-beta
title "Leaky ReLU Function (α = 0.1)"
y-axis -1 --> 5
x-axis [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]
line [-0.5, -0.4, -0.3, -0.2, -0.1, 0, 1, 2, 3, 4, 5]
&lt;/pre>
&lt;h2 id="prelu-함수-parametric-relu-function">PReLU 함수 (Parametric ReLU Function)&lt;/h2>
&lt;p>PReLU(Parametric ReLU) 함수는 Leaky ReLU의 확장판으로, 0 이하의 기울기 α를 고정된 값이 아닌 학습 가능한 파라미터로 설정한다.&lt;/p>
&lt;p>이로 인해 데이터에 따라 기울기를 최적화할 수 있으므로 네트워크의 성능을 더욱 개선할 수 있다. 다만 학습할 파라미터가 늘어나기 때문에 계산 비용이 조금 증가할 수 있다.&lt;/p>
&lt;h2 id="elu-함수-exponential-linear-unit-function">ELU 함수 (Exponential Linear Unit Function)&lt;/h2>
&lt;p>ELU(Exponential Linear Unit) 함수는 ReLU와 Leaky ReLU의 단점을 보완한 함수로, 입력값이 0 이하일 때 $ e^x - 1 $ 형태의 부드러운 곡선을 갖는다.&lt;/p>
&lt;p>ELU는 0 이하의 출력이 음수 값을 가지기 때문에 평균 출력이 0에 가깝게 유지되어 학습을 더 안정적으로 만들며, 죽은 뉴런 문제도 해결할 수 있다.&lt;/p>
&lt;p>$$
ELU(x) =
\begin{cases}
x &amp;amp; \text{if } x &amp;gt; 0 \
\alpha (e^x - 1) &amp;amp; \text{else } \text{otherwise}
\end{cases}
$$&lt;/p>
&lt;pre class="mermaid">xychart-beta
title "ELU Function (α = 1)"
y-axis -3 --> 5
x-axis [-4, -3.6, -3.2, -2.8, -2.4, -2, -1.6, -1.2, -0.8, -0.4, 0, 0.4, 0.8, 1.2, 1.6, 2, 2.4, 2.8, 3.2, 3.6, 4]
line [-0.9817, -0.9736, -0.9502, -0.9131, -0.8647, -0.7869, -0.6988, -0.6065, -0.4866, -0.3297, 0, 0.4, 0.8, 1.2, 1.6, 2, 2.4, 2.8, 3.2, 3.6, 4]
&lt;/pre>
&lt;h2 id="소프트맥스-함수-softmax-function">소프트맥스 함수 (Softmax Function)&lt;/h2>
&lt;p>소프트맥스 함수는 다중 클래스 분류에서 사용되는 함수로, 입력값을 확률 분포로 변환한다. 각 클래스의 출력값에 대해 지수 함수로 변환한 후, 전체 클래스의 지수값 합으로 나눈다.&lt;/p>
&lt;p>이로 인해 출력값의 합이 항상 1이 되며, 이를 통해 각 클래스에 대한 확률로 해석할 수 있다. 소프트맥스 함수는 주로 신경망의 출력층에서 사용된다.&lt;/p>
&lt;p>$$
p_k = \frac{e^{z_k}}{\sum_{i=1}^n e^{z_i}}
$$&lt;/p></description></item><item><title>최적화 함수의 종류와 발전 과정</title><link>https://gyeongmin.kr/p/optimization-functions/</link><pubDate>Fri, 09 Feb 2024 00:00:00 +0000</pubDate><guid>https://gyeongmin.kr/p/optimization-functions/</guid><description>&lt;img src="https://gyeongmin.kr/images/pytorch-transformer-nlp-computer-vision.png" alt="Featured image of post 최적화 함수의 종류와 발전 과정" />&lt;h2 id="최적화-함수란">최적화 함수란?&lt;/h2>
&lt;p>신경망(neural network)의 학습 목적은 손실 함수(loss function)의 값을 최대한 낮추는 매개변수(parameter)를 찾는 것이다. 이는 곧 매개변수의 최적값을 찾는 문제이며, 이를 최적화 문제(optimization)라고 한다. 최적화 문제를 해결하기 위해 사용하는 도구가 바로 최적화 함수(optimizer)이다.&lt;/p>
&lt;p>최적화 함수는 손실 함수의 값을 최소화하는 방향으로 모델의 매개변수를 조정한다. 손실 함수는 예측값(predicted value)과 실제값(true value) 간의 차이를 나타내며, 최적화 함수는 손실을 점진적으로 줄여 모델의 예측 성능을 향상시킨다.&lt;/p>
&lt;p>&lt;img src="https://gyeongmin.kr/p/optimization-functions/image.png"
width="3193"
height="1536"
srcset="https://gyeongmin.kr/p/optimization-functions/image_hu0eebc20c8c24827b2e90536cf3d5f782_408890_480x0_resize_box_3.png 480w, https://gyeongmin.kr/p/optimization-functions/image_hu0eebc20c8c24827b2e90536cf3d5f782_408890_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="saddle point and local minima"
class="gallery-image"
data-flex-grow="207"
data-flex-basis="498px"
>&lt;/p>
&lt;p>최적화 함수는 기울기(gradient)를 활용하여 손실 함수의 값을 낮추는 방향으로 매개변수를 조정한다. 기울기는 각 지점에서 함수의 값을 줄이는 방향을 나타내는 지표로, 손실 값을 줄이기 위한 업데이트의 핵심 정보를 제공한다. 그러나 딥러닝에서 사용하는 손실 함수는 고차원적이고 복잡한 지형을 가지기 때문에, 기울기가 항상 최적의 방향을 가리킨다고 보장할 수는 없다. 이는 다음과 같은 문제로 이어질 수 있다.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>안장점(Saddle Point)&lt;br>
안장점은 특정 방향에서는 극대값처럼 보이고, 다른 방향에서는 극솟값처럼 보이는 지점이다. 이 지점에서는 기울기가 0에 가까워져 학습이 정체될 수 있다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>지역 최적해(Local Minimum)&lt;br>
지역 최적해는 특정 구역에서 손실 값이 가장 작은 지점이다. 그러나 이는 전역 최솟값(global minimum)과는 거리가 있을 수 있으며, 최적화 과정이 이 지점에 갇히면 더 나은 해를 찾지 못할 위험이 있다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>평탄한 구간(Flat Regions)&lt;br>
손실 함수가 일정하거나 기울기가 매우 작은 평탄한 구간에서는 학습이 느려지거나 멈출 수 있다.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>이러한 문제를 극복하고 손실 값을 줄이기 위해, 최적화 함수는 기울기의 정보를 활용해 적절한 방향으로 이동한다. 이러한 접근 방식의 시초가 되는 알고리즘이 바로 경사하강법(Gradient Descent)이다.&lt;/p>
&lt;h2 id="gradient-descent-gd">Gradient Descent (GD)&lt;/h2>
&lt;p>경사하강법은 손실 함수의 기울기를 계산하여 손실 값을 줄이는 방향으로 매개변수를 업데이트하는 알고리즘이다.&lt;/p>
&lt;p>경사하강법의 기본 수식은 다음과 같다.&lt;/p>
&lt;p>$$
\theta_{t+1} = \theta_t - \eta \nabla_{\theta} L(\theta_t)
$$&lt;/p>
&lt;ul>
&lt;li>$ \theta_t $ : 현재 파라미터,&lt;/li>
&lt;li>$ \eta $ : 학습률(learning rate),&lt;/li>
&lt;li>$ \nabla_{\theta} L(\theta_t) $ : 손실 함수의 기울기.&lt;/li>
&lt;/ul>
&lt;p>이 수식에서 기울기는 손실 값을 줄이는 방향을 가리킨다. 이를 반복적으로 수행하면 모델의 매개변수가 점진적으로 최적값에 근접한다.&lt;/p>
&lt;h2 id="1950s-stochastic-gradient-descent-sgd">1950s, Stochastic Gradient Descent (SGD)&lt;/h2>
&lt;p>SGD는 초기의 Batch Gradient Descent의 비효율성을 해결하기 위해 개발되었다. Batch Gradient Descent는 전체 데이터셋을 사용하여 손실 함수의 기울기를 계산하고 파라미터를 업데이트하기 때문에 데이터셋이 클수록 계산 비용이 급격히 증가한다. 이러한 문제를 해결하기 위해 SGD는 데이터의 일부인 샘플이나 미니배치를 사용하여 손실 함수의 기울기를 계산하고 파라미터를 업데이트한다.&lt;/p>
&lt;p>SGD의 업데이트 방식은 GD와 거의 같으나, 전체 데이터가 아닌 샘플들을 가지고 gradient 를 구하며 parameter 를 업데이트 해가는 방식이다.&lt;/p>
&lt;p>SGD는 전체 데이터셋을 처리하지 않고도 학습을 진행할 수 있어 대규모 데이터 학습에서 계산 비용을 대폭 줄일 수 있다. 또한, 실시간 학습이나 스트리밍 데이터 처리와 같은 환경에서도 효과적으로 사용할 수 있다. 그러나 기울기의 노이즈로 인해 최적점 근처에서 진동(oscillation) 현상이 발생하며, 손실 함수의 평탄한 구간(flat regions)에서는 느리게 수렴한다는 한계가 있다.&lt;/p>
&lt;p>PyTorch에서 SGD는 아래와 같이 구현할 수 있다.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch.optim&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">optim&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">optimizer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">optim&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">SGD&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">parameters&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.01&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="1964-momentum">1964, Momentum&lt;/h2>
&lt;p>Momentum은 SGD의 진동 문제를 해결하기 위해 제안되었다. SGD는 현재 기울기만을 기반으로 업데이트를 수행하기 때문에 손실 함수의 지형이 비대칭인 경우 진동이 발생하여 최적점에 도달하는 데 오랜 시간이 걸릴 수 있다. Momentum은 과거 기울기의 이동 평균을 반영하여 더욱 안정적이고 효율적인 학습 경로를 제공한다.&lt;/p>
&lt;p>Momentum의 업데이트 방식은 아래와 같다.
$$
v_{t+1} = \beta v_t + (1-\beta) \nabla_{\theta_t} L(\theta_t)
$$&lt;/p>
&lt;p>$$
\theta_{t+1} = \theta_t - \eta_t v_{t+1}
$$&lt;/p>
&lt;ul>
&lt;li>$ v_t $ : 이동 평균(모멘텀 변수),&lt;/li>
&lt;li>$ \beta $ : 모멘텀 계수로, 일반적으로 0.9로 설정된다.&lt;/li>
&lt;/ul>
&lt;p>Momentum은 기울기의 이동 평균을 계산하여 진동을 줄이고, 손실 함수가 좁고 깊은 구간에서도 빠르게 수렴할 수 있도록 한다. 그러나 추가적인 하이퍼파라미터 $ \beta $를 적절히 설정해야 한다는 점에서 복잡성이 증가한다.&lt;/p>
&lt;p>PyTorch에서 Momentum은 아래와 같이 구현할 수 있다.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">optimizer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">optim&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">SGD&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">parameters&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.01&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">momentum&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.9&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="1983-nesterov-accelerated-gradient-nag">1983, Nesterov Accelerated Gradient (NAG)&lt;/h2>
&lt;p>NAG는 Momentum의 단점을 보완하기 위해 개발되었다. Momentum 방식은 현재 위치에서 기울기를 계산하므로, 최적점 근처에서 오버슈팅(overshooting)이 발생할 가능성이 있다. NAG는 모멘텀에 의해 예측된 미래의 위치에서 기울기를 계산함으로써 이러한 문제를 해결한다.&lt;/p>
&lt;p>NAG의 업데이트 방식은 아래와 같다.
$$
v_{t+1} = \beta v_t + \nabla_{\theta} L(\theta_t - \eta \beta v_t)
$$&lt;/p>
&lt;p>$$
\theta_{t+1} = \theta_t - \eta v_{t+1}
$$&lt;/p>
&lt;p>NAG는 Momentum이 제공하는 이동 방향에 대해 한 단계 더 정교하게 접근하여, 최적점 근처에서의 오버슈팅 문제를 완화한다. 또한, 수렴 속도를 개선하며 최적점 근처에서도 안정적으로 작동한다. 그러나 기울기 계산이 더 복잡해지고 계산 비용이 증가한다는 한계가 있다.&lt;/p>
&lt;p>NAG는 PyTorch에서 아래와 같이 구현할 수 있다.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">optimizer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">optim&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">SGD&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">parameters&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.01&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">momentum&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.9&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">nesterov&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="2011-adagrad">2011, Adagrad&lt;/h2>
&lt;p>Adagrad는 SGD와 Momentum이 모든 파라미터에 동일한 학습률을 적용한다는 문제를 해결하기 위해 도입되었다. 데이터셋에 드문 특징(sparse feature)이 포함되어 있는 경우, 이러한 방식은 드문 특징을 학습하는 데 비효율적이다. Adagrad는 각 파라미터의 과거 기울기의 제곱합을 기반으로 학습률을 조정하여 이를 해결한다.&lt;/p>
&lt;p>Adagrad의 업데이트 방식은 아래와 같다.
$$
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \nabla_{\theta} L(\theta_t)
$$&lt;/p>
&lt;ul>
&lt;li>$ G_t $ : 과거 기울기의 제곱합,&lt;/li>
&lt;li>$ \epsilon $ : 0으로 나누는 것을 방지하기 위한 작은 값.&lt;/li>
&lt;/ul>
&lt;p>Adagrad는 희소 데이터에 대한 학습에서 특히 효과적이며, 각 파라미터에 적응적인 학습률을 적용한다는 점에서 큰 장점이 있다. 그러나 기울기의 제곱합이 점진적으로 증가하면서 학습률이 감소하여 장기 학습에는 적합하지 않다는 한계가 있다.&lt;/p>
&lt;p>Adagrad는 PyTorch에서 아래와 같이 구현할 수 있다.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">optimizer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">optim&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Adagrad&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">parameters&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.01&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="2012-rmsprop">2012, RMSprop&lt;/h2>
&lt;p>RMSprop은 Adagrad의 단점을 해결하기 위해 제안되었다. Adagrad는 기울기의 제곱합이 점진적으로 증가하여 학습률이 감소하고, 장기 학습에서는 효과가 떨어진다. RMSprop은 기울기의 제곱 이동 평균(Exponentially Weighted Moving Average)을 사용하여 학습률을 조정함으로써 이러한 문제를 해결한다.&lt;/p>
&lt;p>RMSprop의 업데이트 방식은 아래와 같다.&lt;/p>
&lt;p>$$
E[g^2]_t = \rho E[g^2] _{t-1} + (1-\rho)g_t^2
$$&lt;/p>
&lt;p>$$
\theta_{t+1} = \theta _t - \frac{\eta}{\sqrt{E[g^2] _t + \epsilon}} \nabla _{\theta} L(\theta _t)
$$&lt;/p>
&lt;ul>
&lt;li>$ E[g^2]_t $ : 기울기의 제곱 이동 평균,&lt;/li>
&lt;li>$ \rho $ : 지수 이동 평균의 가중치 계수로, 일반적으로 0.9로 설정된다,&lt;/li>
&lt;li>$ \epsilon $ : 0으로 나누는 것을 방지하기 위한 작은 값.&lt;/li>
&lt;/ul>
&lt;p>RMSprop은 Adagrad와 달리 이동 평균을 사용하여 학습률 감소 문제를 해결하고, 일정한 학습률을 유지하며 안정적으로 작동한다. 특히 RNN(Recurrent Neural Network)과 같은 모델에서 효과적으로 사용된다.&lt;/p>
&lt;p>RMSprop은 PyTorch에서 아래와 같이 구현할 수 있다.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">optimizer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">optim&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">RMSprop&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">parameters&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.01&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="2014-adam">2014, Adam&lt;/h2>
&lt;p>Adam은 Momentum과 RMSprop의 장점을 결합하여 더욱 효율적이고 안정적인 학습을 제공하기 위해 제안되었다. Adam은 1차 모멘텀(기울기의 이동 평균)과 2차 모멘텀(기울기의 제곱 이동 평균)을 모두 사용하여 학습률을 조정한다.&lt;/p>
&lt;p>Adam의 업데이트 방식은 아래와 같다.&lt;/p>
&lt;p>$$
m_t = \beta_1 m_{t-1} + (1 - \beta_1)g_t
$$&lt;/p>
&lt;p>$$
v_t = \beta_2 v_{t-1} + (1 - \beta_2)g_t^2
$$&lt;/p>
&lt;p>$$
\hat{m}_t = \frac{m_t}{1-\beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1-\beta_2^t}
$$&lt;/p>
&lt;p>$$
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
$$&lt;/p>
&lt;ul>
&lt;li>$ m_t $ : 기울기의 1차 모멘텀(평균),&lt;/li>
&lt;li>$ v_t $ : 기울기의 2차 모멘텀(분산),&lt;/li>
&lt;li>$ \hat{m}_t $, $ \hat{v}_t $ : 편향 보정된 모멘텀.&lt;/li>
&lt;/ul>
&lt;p>Adam은 빠르고 안정적인 수렴을 제공하며, 대부분의 딥러닝 모델에서 기본 최적화 알고리즘으로 사용된다. 학습률의 조정이 자동으로 이루어져 하이퍼파라미터 설정의 복잡성이 감소하는 장점이 있다. 그러나 과적합 가능성이 높아질 수 있으며, 일반화 성능이 저하될 위험이 있다.&lt;/p>
&lt;p>Adam은 PyTorch에서 아래와 같이 구현할 수 있다.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">optimizer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">optim&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Adam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">parameters&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.001&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="2017-adamw">2017, AdamW&lt;/h2>
&lt;p>AdamW는 Adam의 일반화 성능 문제를 해결하기 위해 제안되었다. Adam은 L2 정규화를 사용하는 방식이 가중치 감쇠(weight decay)와 동일하지 않다는 문제가 있었고, 이는 과적합 위험을 증가시켰다. AdamW는 가중치 감쇠를 명시적으로 적용하여 이러한 문제를 해결하였다.&lt;/p>
&lt;p>AdamW의 업데이트 방식은 아래와 같다.
$$
\theta_{t+1} = \theta_t - \eta (\hat{m}_t / \sqrt{\hat{v}_t} + \epsilon + \lambda \theta_t)
$$&lt;/p>
&lt;ul>
&lt;li>$ \lambda $ : 가중치 감쇠(weight decay) 계수.&lt;/li>
&lt;/ul>
&lt;p>AdamW는 과적합을 줄이고 일반화 성능을 향상시키는 데 효과적이다. 특히 딥러닝 연구와 최신 모델 개발에서 기본 최적화 알고리즘으로 널리 사용되고 있다.&lt;/p>
&lt;p>AdamW는 PyTorch에서 아래와 같이 구현할 수 있다.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">optimizer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">optim&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">AdamW&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">parameters&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.001&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">weight_decay&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.01&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>레지스터 전송</title><link>https://gyeongmin.kr/p/register-transfer-language/</link><pubDate>Fri, 29 Apr 2022 00:00:00 +0000</pubDate><guid>https://gyeongmin.kr/p/register-transfer-language/</guid><description>&lt;img src="https://gyeongmin.kr/images/computer-system-architecture.png" alt="Featured image of post 레지스터 전송" />&lt;blockquote>
&lt;p>본 포스팅은 &amp;lsquo;Mano의 컴퓨터시스템구조&amp;rsquo; 교재를 참고했습니다.&lt;/p>
&lt;/blockquote>
&lt;h2 id="레지스터-전송-언어">레지스터 전송 언어&lt;/h2>
&lt;p>레지스터에 저장된 데이터를 가지고 실행되는 동작을 마이크로 연산(micro-operation)이라고 한다. 디지털 컴퓨터의 구조를 정의하기 위하여 레지스터의 종류와 기능, 마이크로 연산, 제어 기능 등을 규정해야 하는데, 레지스터 간의 마이크로 연산 전송을 보다 간단하고 명료하게 표시하기 위해 사용하는 기호를 레지스터 전송 언어(register transfer language)라고 한다.&lt;/p>
&lt;h2 id="레지스터-전송">레지스터 전송&lt;/h2>
&lt;p>레지스터는 머리글자를 대문자로 표시한다. 메모리 주소 레지스터는 MAR, 프로그램 카운터는 PC, 명령어 레지스터는 IR을 나타낸다.&lt;/p>
&lt;p>레지스터들 사이의 정보 전송은 아래와 같이 나타낼 수 있다.&lt;/p>
&lt;p>$$
\text{R2} \leftarrow \text{R1}
$$&lt;/p>
&lt;p>병렬 로드 기능과 같이 제어 조건이 발생할 때만 레지스터 전송이 이루어 진다면 아래와 같이 나타낼 수 있다.&lt;/p>
&lt;p>$$
\text{if} \ (P=1) \ \text{then} \ (\text{R2} \leftarrow \text{R1})
$$&lt;/p>
&lt;p>위 문장을 제어 함수를 사용하여 아래와 같이 나타낼 수도 있다.&lt;/p>
&lt;p>$$
P:\text{R2} \leftarrow \text{R1}
$$&lt;/p>
&lt;p>$T=1$일 때, 두 레지스터의 내용이 교체된다면 아래와 같이 나타낼 수 있다.&lt;/p>
&lt;p>$$
T:\text{R2} \leftarrow \text{R1}, \ \text{R1} \leftarrow \text{R2}
$$&lt;/p>
&lt;p>정리하자면, 레지스터는 영어 대문자+숫자 로 표현된다. 괄호는 레지스터의 일부분을 나타내고, 화살표는 전송의 방향을 나타내며, 쉼표는 동시에 일어나는 여러 동작을 나타낸다.&lt;/p>
&lt;h2 id="버스-시스템">버스 시스템&lt;/h2>
&lt;p>각각의 레지스터가 모두 독립된 전송 라인을 사용한다면 그 숫자가 너무 많아지기 때문에, 공통의 버스(bus)을 두어 효율적인 방법을 사용한다.&lt;/p>
&lt;p>&lt;img src="https://gyeongmin.kr/p/register-transfer-language/image.png"
width="925"
height="633"
srcset="https://gyeongmin.kr/p/register-transfer-language/image_hu8ee1cf73cbd6ec5dbbe18c611b3ed814_174878_480x0_resize_box_3.png 480w, https://gyeongmin.kr/p/register-transfer-language/image_hu8ee1cf73cbd6ec5dbbe18c611b3ed814_174878_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="4개의 레지스터에 대한 버스 시스템"
class="gallery-image"
data-flex-grow="146"
data-flex-basis="350px"
>&lt;/p>
&lt;p>공통 버스를 구성하는 방법으로 위 그림처럼 멀티플렉서를 이용하는 것이 있다.
각 레지스터에서 동일한 위치의 비트들은 모두 하나의 멀티플렉서에 연결되어 하나의 버스 라인을 형성한다.&lt;/p>
&lt;p>일반적으로 $n$ 비트의 $k$ 레지스터를 멀티플렉스하여 $n$라인의 공통 버스를 만드는 버스 시스템에서는 $n$개의 $k \times 1$ 멀티플렉서가 필요하다.&lt;/p>
&lt;p>버스 정보를 많은 목적 레지스터 중에서 하나로 전송하기 위해서는 버스라인을 모든 레지스터에 연결하고 로드 제어를 통해 특정한 목적 레지스터를 선택하면 된다.&lt;/p>
&lt;p>$$
BUS \leftarrow C, \ R1 \leftarrow BUS
$$&lt;/p>
&lt;p>버스가 시스템에 존재한다고 가정하면, $R1 \leftarrow C$ 와 같이 간소화할 수 있다.&lt;/p>
&lt;h2 id="메모리-전송">메모리 전송&lt;/h2>
&lt;p>메모리 워드는 $M$으로 나타내며, 주소는 $M$ 다음의 대괄호 안에 표시한다.&lt;/p>
&lt;p>메모리 주소를 주소 레지스터(AR)로부터 받고, 데이터를 데이터 레지스터(DR)에 나타내면 다음과 같다.&lt;/p>
&lt;p>$$
\text{Read}: DR \leftarrow M[AR]
$$&lt;/p>
&lt;p>반대로 R1에 있는 데이터를 AR에 지정된 메모리주소로 전송하는 쓰기 동작은 다음과 같다.&lt;/p>
&lt;p>$$
\text{Write}: M[AR] \leftarrow DR
$$&lt;/p></description></item><item><title>고정 소수점과 부동 소수점 표현</title><link>https://gyeongmin.kr/p/fixed-point-and-floating-point/</link><pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate><guid>https://gyeongmin.kr/p/fixed-point-and-floating-point/</guid><description>&lt;img src="https://gyeongmin.kr/images/computer-system-architecture.png" alt="Featured image of post 고정 소수점과 부동 소수점 표현" />&lt;h2 id="고정-소수점-표현">고정 소수점 표현&lt;/h2>
&lt;p>고정 소수점 방식에서는 정수 부분과 소수 부분을 이진수로 변환한 후 각각 고정된 위치에 표현한다. 아래 그림과 같이 첫번째 비트는 부호를, 그 다음 16비트는 정수를, 우측 15비트는 소수를 나타낸다.&lt;/p>
&lt;p>&lt;img src="https://gyeongmin.kr/p/fixed-point-and-floating-point/image-1.png"
width="995"
height="291"
srcset="https://gyeongmin.kr/p/fixed-point-and-floating-point/image-1_hu9a5a13c4fb56aedbefd94a9fc224620b_19438_480x0_resize_box_3.png 480w, https://gyeongmin.kr/p/fixed-point-and-floating-point/image-1_hu9a5a13c4fb56aedbefd94a9fc224620b_19438_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="고정 소수점 표현"
class="gallery-image"
data-flex-grow="341"
data-flex-basis="820px"
>&lt;/p>
&lt;p>십진 실수를 고정 소수점으로 표현하는 과정은 다음과 같다.&lt;/p>
&lt;ol>
&lt;li>부호(sign)에 양수면 0, 음수면 1을 넣는다.&lt;/li>
&lt;li>정수부를 이진화하여, 정수부에 넣는다. 남는 부분은 0으로 채운다.&lt;/li>
&lt;li>소수부의 근사치를 이진화하여 소수부에 넣는다. 뒷 부분은 잘라내거나, 남는 부분은 0으로 채운다.&lt;/li>
&lt;/ol>
&lt;p>고정 소수점은 부동소수점에 비해 빠르고 간단하다는 장점이 있지만, 정수부로 사용 가능한 비트 수는 정해져 있기 때문에 큰 실수를 표현할 수 없다는 단점이 있다.&lt;/p>
&lt;h2 id="부동-소수점-표현">부동 소수점 표현&lt;/h2>
&lt;p>부동소수점 수는 가수(mantissa)와 지수(exponent)로 나누어 표현한다.
아래 그림과 같이 첫번째 비트는 부호를, 그 다음 8비트는 지수를, 우측 23비트는 가수를 나타낸다.&lt;/p>
&lt;p>&lt;img src="https://gyeongmin.kr/p/fixed-point-and-floating-point/image.png"
width="1016"
height="287"
srcset="https://gyeongmin.kr/p/fixed-point-and-floating-point/image_hu8bfd104357f64d35172f20f6f2a20922_19327_480x0_resize_box_3.png 480w, https://gyeongmin.kr/p/fixed-point-and-floating-point/image_hu8bfd104357f64d35172f20f6f2a20922_19327_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="부동 소수점 예시"
class="gallery-image"
data-flex-grow="354"
data-flex-basis="849px"
>&lt;/p>
&lt;p>십진 실수를 부동 소수점으로 표현하는 과정은 다음과 같다.&lt;/p>
&lt;ol>
&lt;li>부호(sign)에 양수면 0, 음수면 1을 넣는다.&lt;/li>
&lt;li>$1.m \times 2^{n}$ 형태로 수를 정규화한다. $m$은 가수, $n$은 지수이다.&lt;/li>
&lt;li>정규화된 수의 소수부를 이진화하여 가수에 넣는다. 뒷 부분은 잘라내거나, 0으로 채운다.&lt;/li>
&lt;li>지수에 편항(bias) 127을 더해 지수 부분에 담는다. 남는 부분은 0으로 채운다.&lt;/li>
&lt;/ol>
&lt;p>부동 소수점은 고정 소수점에 비해 더 큰 실수를 표현할 수 있기에 대부분 부동 소수점을 기본적으로 채택한다.&lt;/p>
&lt;h3 id="정규화">정규화&lt;/h3>
&lt;p>부동소수점은 $1.m \times 2^{e-\text{bias}}$ 꼴로 표현되는데, 여기서 정규화란 가수를 $1.m$ 형태로 맞추는 과정이다. 정규화를 통해 동일한 크기의 비트로 더 넓은 범위의 실수를 다룰 수 있다.&lt;/p>
&lt;ol>
&lt;li>실수의 가장 왼쪽에 위치한 1이 가수의 첫 부분에 오도록 위치를 조정한다.&lt;/li>
&lt;li>지수 $e$는 가수를 정규화한 결과에 맞게 증가하거나 감소한다.&lt;/li>
&lt;/ol>
&lt;p>예를 들어, 10진수 12.75를 부동 소수점으로 변환한다면, 아래와 같다.&lt;/p>
&lt;ol>
&lt;li>12.75를 이진수로 변환하면 $1100.11_2$이다.&lt;/li>
&lt;li>정규화하면 $1.10011_2 \times 2^3$이 된다.&lt;/li>
&lt;li>여기서 $1.10011_2$는 가수, $3$은 지수가 된다.&lt;/li>
&lt;/ol>
&lt;p>정규화를 진행하면 항상 정수부가 1이기 때문에, 1은 굳이 저장하지 않고 소수부만 저장한다. 마찬가지로 $2^{e-\text{bias}}$ 에서 $e$만 가수에 저장한다.&lt;/p>
&lt;h3 id="편향-bias">편향 (Bias)&lt;/h3>
&lt;p>부호 없는 정수 형태로 지수를 표현하기 위해, 실제 지수 값에 특정 값(편향)을 더해 저장한다. 일반적으로 IEEE 754 표준에서는 편향 값으로 $2^{n-1} - 1$을 사용한다. 32비트 단정밀도는 지수 비트수가 8이므로 편향 값은 $127$이다. 계산할 땐 다시 편향 값을 더해 복구한다.&lt;/p>
&lt;p>양수와 음수 지수를 모두 다룰 수 있고, 하드웨어 구현이 더 간단해지기 때문에 편향을 이용하여 표현한다.
또한, 0을 정확히 표현할 수 있고, &lt;code>infinity&lt;/code>와 &lt;code>NaN&lt;/code>을 표현할 수 있다.&lt;/p>
&lt;h3 id="부동소수점-오차">부동소수점 오차&lt;/h3>
&lt;p>부동소수점의 가장 큰 한계 중 하나는 바로 오차이다.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>근사 오차 (Rounding Error)&lt;br>
부동소수점은 유한한 비트로 실수를 표현하기 때문에, 일부 값은 근사치로 표현될 수밖에 없다. 예를 들어, 10진수 0.1은 2진수로 정확히 표현할 수 없기 때문에 근사치로 저장된다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>연산 오차 (Arithmetic Error)&lt;br>
두 개 이상의 부동소수점 수를 연산할 때, 소수점 아래 비트가 잘리거나 반올림되면서 오차가 발생한다. 특히, 값의 크기 차이가 클수록 오차가 커질 수 있다.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>특히, 부동소수점을 &lt;code>==&lt;/code> 비교하는 것은 자제하는 것이 옳다.&lt;/p>
&lt;h2 id="실수를-더-정확하게-연산하는-방법">실수를 더 정확하게 연산하는 방법&lt;/h2>
&lt;h3 id="정수-연산-치환">정수 연산 치환&lt;/h3>
&lt;p>항상 소수부가 2자리라면, 100을 곱해 정수로 치환한 뒤, 계산한 후 다시 소수로 변환하는 방법이 있다.&lt;/p>
&lt;h3 id="보다-큰-자료형-선택">보다 큰 자료형 선택&lt;/h3>
&lt;p>더 많은 비트를 사용하여, 더욱 정밀도를 높일 수 있다. &lt;code>double&lt;/code> 대신 &lt;code>long double&lt;/code>, &lt;code>__int128&lt;/code>을 사용하면 된다.&lt;/p>
&lt;h3 id="분수-클래스-사용">분수 클래스 사용&lt;/h3>
&lt;p>파이썬의 &lt;code>Fraction&lt;/code>과 같이, 모든 수를 분수 형태로 표현하여 연산한다면 정확한 실수 연산이 가능하다.&lt;/p>
&lt;h3 id="고정-소수점-사용">고정 소수점 사용&lt;/h3>
&lt;p>부동소수점 대신 고정 소수점을 사용하면 일부 환경에서는 오차를 줄일 수 있다. 특히, 값의 범위가 작고 정밀도가 중요한 경우에 적합하다.&lt;/p></description></item><item><title>진법과 보수</title><link>https://gyeongmin.kr/p/notation-and-complement/</link><pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate><guid>https://gyeongmin.kr/p/notation-and-complement/</guid><description>&lt;img src="https://gyeongmin.kr/images/computer-system-architecture.png" alt="Featured image of post 진법과 보수" />&lt;blockquote>
&lt;p>본 포스팅은 &amp;lsquo;Mano의 컴퓨터시스템구조&amp;rsquo; 교재를 참고했습니다.&lt;/p>
&lt;/blockquote>
&lt;h2 id="진법">진법&lt;/h2>
&lt;p>N진법은 수를 셀 때 자릿수가 올라가는 단위를 기준으로 하는 셈법으로, 위치적 기수법이라고도 한다.&lt;/p>
&lt;p>우리가 일반적으로 수를 셀 때는 10진법을 사용한다. 시계에서 시간은 12진법을, 분은 60진법을 사용한다.
진법은 분명히 표시하기 위해 다음과 같이 첨자를 붙이기도 한다.&lt;/p>
&lt;p>$$
(101101)&lt;em>2 = (45)&lt;/em>{10}
$$&lt;/p>
&lt;h3 id="2진법-binary">2진법 (Binary)&lt;/h3>
&lt;p>2진법은 0과 1이라는 두개의 숫자만을 사용하여 수를 나타내는 것이다. 2가 되는 순간 자리올림이 발생한다.&lt;/p>
&lt;p>$$
(1011)_2=\mathtt{0b1011}
$$&lt;/p>
&lt;h3 id="10진법-decimal">10진법 (Decimal)&lt;/h3>
&lt;p>우리가 가장 일반적으로 사용하고 있는 기수법으로, 한 자리에 0~9의 숫자로 나타낸다. 9를 넘어서면 자리올림이 발생한다.&lt;/p>
&lt;p>$$
724.5 = 7 \times 10^2 + 2 \times 10^1 + 4 \times 10^0 + 5 \times 1010^{-1}
$$&lt;/p>
&lt;h3 id="16진법-hexadecimal">16진법 (Hexadecimal)&lt;/h3>
&lt;p>0~F까지 사용한다. 컴퓨터 분야에서 1바이트의 크기를 쉽게 표현할 수 있어 많이 사용된다.&lt;/p>
&lt;p>$$
(5A)_{16}=\mathtt{0x5A}
$$&lt;/p>
&lt;p>컴퓨터는 십진수를 &lt;a class="link" href="https://ko.wikipedia.org/wiki/%EC%9D%B4%EC%A7%84%ED%99%94_%EC%8B%AD%EC%A7%84%EB%B2%95" target="_blank" rel="noopener"
>이진화 집진법(BCD)&lt;/a>의 형태로 저장하고 표현한다.&lt;/p>
&lt;h2 id="진법의-변환">진법의 변환&lt;/h2>
&lt;h3 id="10진수---n진수">10진수 -&amp;gt; N진수&lt;/h3>
&lt;ol>
&lt;li>10진수를 N으로 나누고, 나머지를 기록한다.&lt;/li>
&lt;li>나머지를 기록한다. (뒤에서 앞으로)&lt;/li>
&lt;li>나눈 몫이 N보다 작으면 멈춘다.&lt;/li>
&lt;li>마지막 몫을 기록한다.&lt;/li>
&lt;/ol>
&lt;h3 id="n진수---10진수">N진수 -&amp;gt; 10진수&lt;/h3>
&lt;p>1의 자리수부터 N의 0승, N의 1승, &amp;hellip; 이렇게 차례대로 곱하여 더해주면 된다.&lt;/p>
&lt;p>$$
(1011)_2 = 1 \times 2^3 + 0 \times 2^2 + 1 \times 2^1 + 1 \times 2^0 = 11
$$&lt;/p>
&lt;p>이를 수식으로 일반화하면 아래와 같다.&lt;/p>
&lt;p>$$
(a_k a_{k-1} \cdots a_1 a_0)&lt;em>N = \sum&lt;/em>{i=0}^{k} a_i \times N^i = (x)_{10}
$$&lt;/p>
&lt;h3 id="진법-변환-c-구현">진법 변환 C++ 구현&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-cpp" data-lang="cpp">&lt;span class="line">&lt;span class="cl">&lt;span class="kt">int&lt;/span> &lt;span class="nf">charToInt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">char&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">c&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="sc">&amp;#39;0&amp;#39;&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">c&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="sc">&amp;#39;9&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">c&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="sc">&amp;#39;0&amp;#39;&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">c&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="sc">&amp;#39;A&amp;#39;&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">10&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kt">char&lt;/span> &lt;span class="nf">intToChar&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">num&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">num&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">num&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="mi">9&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="kt">char&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sc">&amp;#39;0&amp;#39;&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">num&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="kt">char&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sc">&amp;#39;A&amp;#39;&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">num&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">10&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kt">int&lt;/span> &lt;span class="nf">convertToDecimal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="n">string&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">num&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">base&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">decimal&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">num&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">length&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">decimal&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">charToInt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">pow&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">base&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">length&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">decimal&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">string&lt;/span> &lt;span class="nf">convertFromDecimal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">num&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">base&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">string&lt;/span> &lt;span class="n">ret&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">num&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ret&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">intToChar&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">base&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">num&lt;/span> &lt;span class="o">/=&lt;/span> &lt;span class="n">base&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">reverse&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ret&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">ret&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">());&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">ret&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="보수">보수&lt;/h2>
&lt;p>보수(compleement)는 디지털 컴퓨터에서 뺄셈 연산과 논리 계산에 사용된다. $r$진법에는 $r$의 보수와 $(r-1)$의 보수가 있다.&lt;/p>
&lt;h3 id="r-1의-보수">(r-1)의 보수&lt;/h3>
&lt;p>일반적으로 $r$진법의 $n$자리수의 수 $N$에 대하여, $(r-1)$의 보수는 $(r^n-1)-N$으로 정의된다. 예를 들어 10진수 $546700$에 대한 9의 보수는 $999999 - 546700 = 453299$이다. 이진수에서 1의 보수는 각 자리 수를 뒤집는 것이다.&lt;/p>
&lt;h3 id="r의-보수">r의 보수&lt;/h3>
&lt;p>일반적으로 $r$진법의 $n$자리수의 수 $N$에 대하여, $r$의 보수는 $N \neq 0$일 때 $r^n-N$이고, $N=0$일 때는 0으로 정의된다. $r$의 보수는 $(r-1)$의 보수에 1을 더한 것과 같다.&lt;/p>
&lt;h3 id="보수의-대칭">보수의 대칭&lt;/h3>
&lt;p>어떤 수에 대한 보수를 다시 보수화하면 원래 수가 된다.&lt;/p>
&lt;p>$r^n-(r^n-N)=N$이고, $(r^n-1)-((r^n-1)-N)=N$ 이므로, 보수의 대칭성을 만족하는 것을 확인할 수 있다.&lt;/p>
&lt;h3 id="부호-없는-숫자의-뺄셈">부호 없는 숫자의 뺄셈&lt;/h3>
&lt;p>$r$진수 부호 없는 두 $n$자리수 사이의 뺼셈 $M-N(N \neq 0)$은 다음과 같이 계산된다.&lt;/p>
&lt;ol>
&lt;li>피감수 $M$에 감수 $N$에 대한 $r$의 보수를 더한다. $M+(r^n-N)=M-N+r^n$&lt;/li>
&lt;li>$M \geq N$이라면, 위의 값은 end캐리 $r^n$을 만들어내고, 이를 무시하면 $M-N$을 얻을 수 있다.&lt;/li>
&lt;li>$M &amp;lt; N$이라면, 위의 값은 end캐리를 만들어내지 않고 그 값은 $r^n-(N-M)$이다. 이것은 $(N-M)$에 대한 $r$보수이므로, 이것에 대한 $r$의 보수를 취하고 앞에 뺄셈 기호를 붙여 뺼셈을 할 수 있다.&lt;/li>
&lt;/ol></description></item><item><title>집적 회로, 디코더, 인코더, 멀티플렉서</title><link>https://gyeongmin.kr/p/ic-and-decoder-and-encoder-and-mux/</link><pubDate>Sat, 09 Apr 2022 00:00:00 +0000</pubDate><guid>https://gyeongmin.kr/p/ic-and-decoder-and-encoder-and-mux/</guid><description>&lt;img src="https://gyeongmin.kr/images/computer-system-architecture.png" alt="Featured image of post 집적 회로, 디코더, 인코더, 멀티플렉서" />&lt;blockquote>
&lt;p>본 포스팅은 &amp;lsquo;Mano의 컴퓨터시스템구조&amp;rsquo; 교재를 참고했습니다.&lt;/p>
&lt;/blockquote>
&lt;h2 id="집적-회로">집적 회로&lt;/h2>
&lt;p>집적 회로(IC)는 디지털 게이트를 구성하는 전자 부품들을 포함하는 실리콘 반도체(chip)이다.
기술이 발전함에 따라 칩 안에 담을 수 있는 게이트의 개수가 급격히 증가하였고, 집적된 정도에 따라 아래와 같이 소규모, 중규모, 대규모 집적 장치로 분류한다.&lt;/p>
&lt;ul>
&lt;li>소규모 집적 장치(SSI)는 10개 이하의 독립적인 게이트가 하나의 칩에 들어가 있고, 게이트의 입출력이 바로 외부 핀으로 연결된다.&lt;/li>
&lt;li>중규모 집적 장치(MSI)는 10개에서 200개의 게이트가 들어가 있고, 디코더나 가산기, 레지스터와 같은 디지털 장치를 구현한다.&lt;/li>
&lt;li>대규모 집적 장치(LSI)는 200에서 1000개의 게이트를 집적하고 프로세서나 메모리 칩과 같은 디지털 시스템을 형성한다.&lt;/li>
&lt;li>초대규모 집적 장치(VLSI)는 수천 개의 게이트를 하나의 칩에 집적하여 대형 메모리나 복잡한 마이크로 컴퓨터 칩을 형성한다.&lt;/li>
&lt;/ul>
&lt;p>디지털 회로는 구현하는 데 적용된 기술에 따라 디지털 논리군으로 분류된다. 대표적으로 아래와 같은 것들이 있다.&lt;/p>
&lt;ul>
&lt;li>TTL: 트랜지스터-트랜지스터 논리&lt;/li>
&lt;li>ECL: 에미터-결합 논리&lt;/li>
&lt;li>MOS: 금속-산화물 반도체&lt;/li>
&lt;li>CMOS: 상보 금속-산화물 반도체&lt;/li>
&lt;/ul>
&lt;p>TTL은 가장 많이 사용되고 있는 논리군이고, ECL은 고속도가 요구되는 시스템에 사용되며, MOS는 부품의 밀도가 높은 집적 회로에, CMOS는 적은 전력 소비가 요구되는 시스템에 많이 사용된다.&lt;/p>
&lt;h2 id="디코더">디코더&lt;/h2>
&lt;p>$n$비트의 이진 코드는 서로 다른 $2^n$개의 원소 정보를 나타낼 수 있다. 디코더는 $n$비트로 코딩된 이진 정보를 최대 $2^n$개의 서로 다른 출력으로 바꾸어 주는 조합 회로이다. $n$개의 입력과 $m(m&amp;lt;2^n)$개의 출력을 가지는 디코더를 $n$대 $m$ 라인 디코더 혹은 $n \times m$ 디코더라고 한다.&lt;/p>
&lt;p>&lt;img src="https://gyeongmin.kr/p/ic-and-decoder-and-encoder-and-mux/image.png"
width="684"
height="747"
srcset="https://gyeongmin.kr/p/ic-and-decoder-and-encoder-and-mux/image_hu65b10ef8cb90c6cc557c07e07bb7a70e_200920_480x0_resize_box_3.png 480w, https://gyeongmin.kr/p/ic-and-decoder-and-encoder-and-mux/image_hu65b10ef8cb90c6cc557c07e07bb7a70e_200920_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="3X8 디코더의 진리표와 회로도"
class="gallery-image"
data-flex-grow="91"
data-flex-basis="219px"
>&lt;/p>
&lt;p>E가 0일때 모든 출력은 항상 0이고, E가 1일때만 정상적으로 동작한다. 각 출력은 다른 일곱 개의 입력 조합에 대해서는 0이고, 오직 하나의 조합에 대해서만 1인 출력값을 가진다. 이것이 입력 이진수에 해당한는 8진수 값이라고 할 수 있다.&lt;/p>
&lt;p>&lt;img src="https://gyeongmin.kr/p/ic-and-decoder-and-encoder-and-mux/image-1.png"
width="972"
height="413"
srcset="https://gyeongmin.kr/p/ic-and-decoder-and-encoder-and-mux/image-1_huadcdd09f88ffc26b1bd8c3c45fb0d645_114561_480x0_resize_box_3.png 480w, https://gyeongmin.kr/p/ic-and-decoder-and-encoder-and-mux/image-1_huadcdd09f88ffc26b1bd8c3c45fb0d645_114561_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="2X4 NAND 게이트 디코더"
class="gallery-image"
data-flex-grow="235"
data-flex-basis="564px"
>&lt;/p>
&lt;p>보수화된 형태로 출력을 만드는 것이 더 경제적이기 때문에, NAND 게이트로 디코드를 형성하기도 한다.&lt;/p>
&lt;p>&lt;img src="https://gyeongmin.kr/p/ic-and-decoder-and-encoder-and-mux/image-2.png"
width="731"
height="540"
srcset="https://gyeongmin.kr/p/ic-and-decoder-and-encoder-and-mux/image-2_hua1d5171524807b662f78c69edb7a342c_97000_480x0_resize_box_3.png 480w, https://gyeongmin.kr/p/ic-and-decoder-and-encoder-and-mux/image-2_hua1d5171524807b662f78c69edb7a342c_97000_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="두 개의 2X4 디코더로 만든 3X8 디코더"
class="gallery-image"
data-flex-grow="135"
data-flex-basis="324px"
>&lt;/p>
&lt;p>두 개 이상의 디코더를 동일한 인에이블 입력에 연결해 하나의 커다란 디코더를 구성할 수 있다.
즉 $4 \times 16$ 디코더 네 개로 $16 \times 64$ 디코더를 만들 수 있다.&lt;/p>
&lt;h2 id="인코더">인코더&lt;/h2>
&lt;p>인코더는 디코더와 반대되는 동작을 수행하는 디지털 회로로서 $2^n$개 입력값에 대해 $n$개의 이진 코드를 출력한다.&lt;/p>
&lt;p>&lt;img src="https://gyeongmin.kr/p/ic-and-decoder-and-encoder-and-mux/image-3.png"
width="660"
height="264"
srcset="https://gyeongmin.kr/p/ic-and-decoder-and-encoder-and-mux/image-3_hu23f218bedbcf696e77c31ef2ab503697_92035_480x0_resize_box_3.png 480w, https://gyeongmin.kr/p/ic-and-decoder-and-encoder-and-mux/image-3_hu23f218bedbcf696e77c31ef2ab503697_92035_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="8진 대 이진 인코더에 대한 진리표"
class="gallery-image"
data-flex-grow="250"
data-flex-basis="600px"
>&lt;/p>
&lt;p>이 인코더는 진리표에 따라 세 개의 $\text{OR}$ 게이트들로 구현할 수 있으며, 각 출력에 대한 부울식은 다음과 같다.&lt;/p>
&lt;p>$$
A_0 = D_1 + D_3 + D_5 + D_7 \
A_1 = D_2 + D_3 + D_6 + D_7 \
A_2 = D_4 + D_5 + D_6 + D_7
$$&lt;/p>
&lt;h2 id="멀티플렉서">멀티플렉서&lt;/h2>
&lt;p>멀티플렉서는 $n$개의 선택 입력에 따라 $2^n$개의 출력을 하나의 출력에 선택적으로 연결시켜 주는 조합 회로이다.
멀티플렉서는 흔히 데이터 선택기(data selector)라고도 하며, 줄여서 MUX라고 쓴다.&lt;/p>
&lt;p>&lt;img src="https://gyeongmin.kr/p/ic-and-decoder-and-encoder-and-mux/image1.png"
width="920"
height="565"
srcset="https://gyeongmin.kr/p/ic-and-decoder-and-encoder-and-mux/image1_hu038882f24ea1dd95b3bea07487f1f548_93413_480x0_resize_box_3.png 480w, https://gyeongmin.kr/p/ic-and-decoder-and-encoder-and-mux/image1_hu038882f24ea1dd95b3bea07487f1f548_93413_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="4X1 멀티플렉서"
class="gallery-image"
data-flex-grow="162"
data-flex-basis="390px"
>&lt;/p>
&lt;p>디코더에 $2^n$개의 입력 라인을 더하게 되면 $2^n$대 1 멀티플렉서를 구현할 수 있다. 디코더처럼 멀티플렉서도 동작을 제어하거나 확장을 위해 인에이블 입력을 가질 수 있다.&lt;/p>
&lt;p>&lt;img src="https://gyeongmin.kr/p/ic-and-decoder-and-encoder-and-mux/image1-1.png"
width="679"
height="384"
srcset="https://gyeongmin.kr/p/ic-and-decoder-and-encoder-and-mux/image1-1_hu2989effcc0d85358fe3d749e80fd3fb7_118765_480x0_resize_box_3.png 480w, https://gyeongmin.kr/p/ic-and-decoder-and-encoder-and-mux/image1-1_hu2989effcc0d85358fe3d749e80fd3fb7_118765_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="4X1 멀티플렉서에 대한 함수표"
class="gallery-image"
data-flex-grow="176"
data-flex-basis="424px"
>&lt;/p>
&lt;p>위 멀티플렉서는 여섯 개의 입력을 가지기 때문에, $2^6=64$줄의 진리표가 필요하다. 하지만 위와 같이 함수표를 이용하면 간단하게 나타낼 수 있다.&lt;/p>
&lt;p>&lt;img src="https://gyeongmin.kr/p/ic-and-decoder-and-encoder-and-mux/image1-2.png"
width="869"
height="529"
srcset="https://gyeongmin.kr/p/ic-and-decoder-and-encoder-and-mux/image1-2_hu22efaea6e62fac6e30b6d3477db7bbdf_52211_480x0_resize_box_3.png 480w, https://gyeongmin.kr/p/ic-and-decoder-and-encoder-and-mux/image1-2_hu22efaea6e62fac6e30b6d3477db7bbdf_52211_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="Quadruple 2X1 멀티플렉서"
class="gallery-image"
data-flex-grow="164"
data-flex-basis="394px"
>&lt;/p>
&lt;p>보통 하나의 IC칩에는 여러 개의 멀티플렉서가 포함된다. 이 회로는 함수표와 같이 두 개의 4비트 데이터를 선택적으로 출력해주는 멀티플렉서로 동작한다.&lt;/p></description></item><item><title>부울 대수</title><link>https://gyeongmin.kr/p/boolean-algebra/</link><pubDate>Wed, 26 Jan 2022 00:00:00 +0000</pubDate><guid>https://gyeongmin.kr/p/boolean-algebra/</guid><description>&lt;img src="https://gyeongmin.kr/images/computer-system-architecture.png" alt="Featured image of post 부울 대수" />&lt;blockquote>
&lt;p>본 포스팅은 &amp;lsquo;Mano의 컴퓨터시스템구조&amp;rsquo; 교재를 참고했습니다.&lt;/p>
&lt;/blockquote>
&lt;h2 id="2진법과-논리-게이트">2진법과 논리 게이트&lt;/h2>
&lt;p>2진법은 0과 1이라는 두개의 숫자만을 사용하여 수를 나타내는 것이다.&lt;/p>
&lt;p>디지털 컴퓨터는 0과 1 두개의 숫자만을 사용하는데, 하나의 이진 숫자를 bit라고 부른다. 컴퓨터는 전압 신호를 이용하여 0과 1로 표현한다.&lt;/p>
&lt;p>이전 정보의 처리는 게이트라 불리는 논리 회로에서 이루어진다. 아래 표는 각 게이트의 이름, 대수 표현식, 진리표를 나타낸 것이다.&lt;/p>
&lt;table style="max-width: 800px;">
&lt;tr>
&lt;th style="text-align: center; vertical-align: middle;">Name&lt;/th>
&lt;th style="text-align: center; vertical-align: middle;">Algebraic function&lt;/th>
&lt;th style="text-align: center; vertical-align: middle;">Truth table&lt;/th>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: center; vertical-align: middle; font-size: 22px">$\text{AND}$&lt;/td>
&lt;td style="text-align: center; vertical-align: middle; font-size: 20px;">$x = A \cdot B$&lt;/td>
&lt;td>
&lt;table>
&lt;tr>&lt;th>A&lt;/th>&lt;th>B&lt;/th>&lt;th>x&lt;/th>&lt;/tr>
&lt;tr>&lt;td>0&lt;/td>&lt;td>0&lt;/td>&lt;td>0&lt;/td>&lt;/tr>
&lt;tr>&lt;td>0&lt;/td>&lt;td>1&lt;/td>&lt;td>0&lt;/td>&lt;/tr>
&lt;tr>&lt;td>1&lt;/td>&lt;td>0&lt;/td>&lt;td>0&lt;/td>&lt;/tr>
&lt;tr>&lt;td>1&lt;/td>&lt;td>1&lt;/td>&lt;td>1&lt;/td>&lt;/tr>
&lt;/table>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: center; vertical-align: middle; font-size: 22px;">$\text{OR}$&lt;/td>
&lt;td style="text-align: center; vertical-align: middle; font-size: 20px;">$x = A + B$&lt;/td>
&lt;td>
&lt;table>
&lt;tr>&lt;th>A&lt;/th>&lt;th>B&lt;/th>&lt;th>x&lt;/th>&lt;/tr>
&lt;tr>&lt;td>0&lt;/td>&lt;td>0&lt;/td>&lt;td>0&lt;/td>&lt;/tr>
&lt;tr>&lt;td>0&lt;/td>&lt;td>1&lt;/td>&lt;td>1&lt;/td>&lt;/tr>
&lt;tr>&lt;td>1&lt;/td>&lt;td>0&lt;/td>&lt;td>1&lt;/td>&lt;/tr>
&lt;tr>&lt;td>1&lt;/td>&lt;td>1&lt;/td>&lt;td>1&lt;/td>&lt;/tr>
&lt;/table>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: center; vertical-align: middle; font-size: 22px;">$\text{Inverter}$&lt;/td>
&lt;td style="text-align: center; vertical-align: middle; font-size: 20px;">$x = A^\prime$&lt;/td>
&lt;td>
&lt;table>
&lt;tr>&lt;th>A&lt;/th>&lt;th>x&lt;/th>&lt;/tr>
&lt;tr>&lt;td>0&lt;/td>&lt;td>1&lt;/td>&lt;/tr>
&lt;tr>&lt;td>1&lt;/td>&lt;td>0&lt;/td>&lt;/tr>
&lt;/table>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: center; vertical-align: middle; font-size: 22px;">$\text{Buffer}$&lt;/td>
&lt;td style="text-align: center; vertical-align: middle; font-size: 20px;">$x = A$&lt;/td>
&lt;td>
&lt;table>
&lt;tr>&lt;th>A&lt;/th>&lt;th>x&lt;/th>&lt;/tr>
&lt;tr>&lt;td>0&lt;/td>&lt;td>0&lt;/td>&lt;/tr>
&lt;tr>&lt;td>1&lt;/td>&lt;td>1&lt;/td>&lt;/tr>
&lt;/table>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: center; vertical-align: middle; font-size: 22px;">$\text{NAND}$&lt;/td>
&lt;td style="text-align: center; vertical-align: middle; font-size: 20px;">$x = (A · B)^\prime$&lt;/td>
&lt;td>
&lt;table>
&lt;tr>&lt;th>A&lt;/th>&lt;th>B&lt;/th>&lt;th>x&lt;/th>&lt;/tr>
&lt;tr>&lt;td>0&lt;/td>&lt;td>0&lt;/td>&lt;td>1&lt;/td>&lt;/tr>
&lt;tr>&lt;td>0&lt;/td>&lt;td>1&lt;/td>&lt;td>1&lt;/td>&lt;/tr>
&lt;tr>&lt;td>1&lt;/td>&lt;td>0&lt;/td>&lt;td>1&lt;/td>&lt;/tr>
&lt;tr>&lt;td>1&lt;/td>&lt;td>1&lt;/td>&lt;td>0&lt;/td>&lt;/tr>
&lt;/table>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: center; vertical-align: middle; font-size: 22px;">$\text{NOR}$&lt;/td>
&lt;td style="text-align: center; vertical-align: middle; font-size: 20px;">$x = (A + B)^\prime$&lt;/td>
&lt;td>
&lt;table>
&lt;tr>&lt;th>A&lt;/th>&lt;th>B&lt;/th>&lt;th>x&lt;/th>&lt;/tr>
&lt;tr>&lt;td>0&lt;/td>&lt;td>0&lt;/td>&lt;td>1&lt;/td>&lt;/tr>
&lt;tr>&lt;td>0&lt;/td>&lt;td>1&lt;/td>&lt;td>0&lt;/td>&lt;/tr>
&lt;tr>&lt;td>1&lt;/td>&lt;td>0&lt;/td>&lt;td>0&lt;/td>&lt;/tr>
&lt;tr>&lt;td>1&lt;/td>&lt;td>1&lt;/td>&lt;td>0&lt;/td>&lt;/tr>
&lt;/table>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: center; vertical-align: middle; font-size: 22px;">$\text{XOR}$&lt;/td>
&lt;td style="text-align: center; vertical-align: middle; font-size: 20px;">$x = A ⊕ B$&lt;/td>
&lt;td>
&lt;table>
&lt;tr>&lt;th>A&lt;/th>&lt;th>B&lt;/th>&lt;th>x&lt;/th>&lt;/tr>
&lt;tr>&lt;td>0&lt;/td>&lt;td>0&lt;/td>&lt;td>0&lt;/td>&lt;/tr>
&lt;tr>&lt;td>0&lt;/td>&lt;td>1&lt;/td>&lt;td>1&lt;/td>&lt;/tr>
&lt;tr>&lt;td>1&lt;/td>&lt;td>0&lt;/td>&lt;td>1&lt;/td>&lt;/tr>
&lt;tr>&lt;td>1&lt;/td>&lt;td>1&lt;/td>&lt;td>0&lt;/td>&lt;/tr>
&lt;/table>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: center; vertical-align: middle; font-size: 22px;">$\text{XNOR}$&lt;/td>
&lt;td style="text-align: center; vertical-align: middle; font-size: 20px;">$x = (A ⊕ B)^\prime$&lt;/td>
&lt;td>
&lt;table>
&lt;tr>&lt;th>A&lt;/th>&lt;th>B&lt;/th>&lt;th>x&lt;/th>&lt;/tr>
&lt;tr>&lt;td>0&lt;/td>&lt;td>0&lt;/td>&lt;td>1&lt;/td>&lt;/tr>
&lt;tr>&lt;td>0&lt;/td>&lt;td>1&lt;/td>&lt;td>0&lt;/td>&lt;/tr>
&lt;tr>&lt;td>1&lt;/td>&lt;td>0&lt;/td>&lt;td>0&lt;/td>&lt;/tr>
&lt;tr>&lt;td>1&lt;/td>&lt;td>1&lt;/td>&lt;td>1&lt;/td>&lt;/tr>
&lt;/table>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;h2 id="부울-대수">부울 대수&lt;/h2>
&lt;p>부울 대수는 19세기 중반 조지 불이 만든 대수 체계로, 디지털 회로의 해석과 설계를 쉽게 하는 것이 목적이다.&lt;/p>
&lt;p>부울 대수의 기본 관계는 아래와 같다. 각각은 모두 진리표로 증명할 수 있다.&lt;/p>
&lt;h3 id="부울-대수의-기본-관계">부울 대수의 기본 관계&lt;/h3>
&lt;ol>
&lt;li>$x + 0 = x$&lt;/li>
&lt;li>$x \cdot 0 = 0$&lt;/li>
&lt;li>$x + 1 = 1$&lt;/li>
&lt;li>$x \cdot 1 = x$&lt;/li>
&lt;li>$x + x = x$&lt;/li>
&lt;li>$x \cdot x = x$&lt;/li>
&lt;li>$x + x^\prime= 1$&lt;/li>
&lt;li>$x \cdot x^\prime 0$&lt;/li>
&lt;li>$x + y = y + x$&lt;/li>
&lt;li>$x \cdot y = y \cdot x$&lt;/li>
&lt;li>$x + (y + z) = (x + y) + z$&lt;/li>
&lt;li>$x \cdot (y \cdot z) = (x \cdot y) \cdot z$&lt;/li>
&lt;li>$x (y + z) = xy + xz$&lt;/li>
&lt;li>$x + yz = (x + y)(x + z)$&lt;/li>
&lt;li>$(x + y)^\prime= x^\prime \cdot y^\prime $&lt;/li>
&lt;li>$(x \cdot y)^\prime = x^\prime + y^\prime $&lt;/li>
&lt;li>$(x^\prime)^\prime = x$&lt;/li>
&lt;/ol>
&lt;p>위 식은 부울 대수의 기본적인 관계를 나타낸다. 부울대수는 교환 법칙과 결합 법칙이 성립한다. 또한 식 15번과 16번은 드 모르간의 정리이다.&lt;/p>
&lt;h3 id="수식의-보수">수식의 보수&lt;/h3>
&lt;p>드 모르간의 정리는 모든 $\text{OR}$ 연산은 $\text{AND}$ 로, 모든 $\text{AND}$ 연산은 $\text{OR}$ 로 바꾸어 주고, 각 변수를 보수화하면 간단히 적용할 수 있다.&lt;/p>
&lt;p>예들들어 다음과 같이 수식의 보수를 만들 수 있다.&lt;/p>
&lt;p>$$
F=AB+C^\prime D^\prime +B^\prime D
$$
$$
F^\prime =(A^\prime +B^\prime )(C+D)(B+D^\prime )
$$&lt;/p>
&lt;h3 id="부울-대수의-활용">부울 대수의 활용&lt;/h3>
&lt;p>부울 대수를 통해 디지털 회로를 간단히 하는 데 사용할 수 있다. 아래와 같은 회로 $F$가 있다고 가정해 보자.&lt;/p>
&lt;p>$$
F=ABC+ABC^\prime +A^\prime C
$$&lt;/p>
&lt;p>부울 대수를 적용하면, $(C+C)^\prime =1$이고, $AB \cdot 1=AB$ 이므로,&lt;/p>
&lt;p>$$
F=ABC+ABC^\prime +A^\prime C=AB(C+C^\prime )+A^\prime C=AB+A^\prime C
$$&lt;/p>
&lt;p>이다. 따라서 4개의 게이트만 사용하여 효율적인 회로를 설계할 수 있는 것이다.&lt;/p>
&lt;h3 id="부울-대수-문제">부울 대수 문제&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>$A \cdot (A + B) = A$ 임을 보여라.&lt;/p>
&lt;details>
&lt;summary>풀이&lt;/summary>
&lt;div markdown="1">
$$
\begin{aligned}
AA + AB &amp;= A + AB \\
&amp;= A (1 + B) \\
&amp;= A \cdot 1 \\
&amp;= A
\end{aligned}
$$
&lt;/div>
&lt;/details>
&lt;/li>
&lt;li>
&lt;p>$ (A + B) \cdot (A + B^\prime ) = A $ 임을 보여라.&lt;/p>
&lt;details>
&lt;summary>풀이&lt;/summary>
&lt;div markdown="1">
$$
\begin{aligned}
AA + AB{^\prime} + AB + BB{^\prime} &amp;= AA + AB{^\prime} + AB + BB{^\prime} \\
&amp;= A + A(B{^\prime} + B) \\
&amp;= A + A \cdot 1 \\
&amp;= A
\end{aligned}
$$
&lt;/div>
&lt;/details>
&lt;/li>
&lt;/ol></description></item></channel></rss>