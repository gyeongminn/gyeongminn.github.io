<!doctype html><html lang=ko dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="신경망의 학습과정에서 손실 함수, 행렬 미분, 오차역전파법에 대한 설명과 구현"><title>신경망의 학습</title><link rel=canonical href=https://gyeongmin.kr/p/neural-network-trainning/><link rel=stylesheet href=/scss/style.min.2325ab3f1c53340b74d88bd622b7f5bcc047e55d78831b42a905dd43de9993e0.css><meta property="og:title" content="신경망의 학습"><meta property="og:description" content="신경망의 학습과정에서 손실 함수, 행렬 미분, 오차역전파법에 대한 설명과 구현"><meta property="og:url" content="https://gyeongmin.kr/p/neural-network-trainning/"><meta property="og:site_name" content="Gyeongmin의 개발 블로그"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="딥러닝"><meta property="article:tag" content="Python"><meta property="article:published_time" content="2023-11-21T00:00:00+00:00"><meta property="article:modified_time" content="2023-11-21T00:00:00+00:00"><meta property="og:image" content="https://gyeongmin.kr/images/deep-learning-from-scratch.jpeg"><meta name=twitter:title content="신경망의 학습"><meta name=twitter:description content="신경망의 학습과정에서 손실 함수, 행렬 미분, 오차역전파법에 대한 설명과 구현"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gyeongmin.kr/images/deep-learning-from-scratch.jpeg"><link rel="shortcut icon" href=/logo.ico/favicon.ico><script async src="https://www.googletagmanager.com/gtag/js?id=G-RBG63ZJRZM"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-RBG63ZJRZM",{anonymize_ip:!1})}</script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2100464285092061" crossorigin=anonymous></script>
<link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color=#757575><meta name=msapplication-TileColor content="#da532c"><meta name=theme-color content="#000000"><link rel=icon href=/logo.ico/favicon.ico><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2100464285092061" crossorigin=anonymous></script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="메뉴 여닫기">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/profile_hu5d202280e416202dba72f1bb8473638c_1285338_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>Gyeongmin의 개발 블로그</a></h1><h2 class=site-description>Computer Vision Engineer</h2></div></header><ol class=social-menu><li><a href=https://github.com/gyeongminn target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="22" height="22" viewBox="1 -2 19 25" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path d="M0 0h22v22H0z" stroke="none"/><path d="M8.25 17.416c-3.942 1.284-3.942-2.292-5.5-2.75m11 4.584v-3.208c0-.916.092-1.284-.458-1.834 2.566-.274 5.042-1.284 5.042-5.5a4.216 4.216.0 00-1.192-2.934A3.85 3.85.0 0017.05 2.84s-1.008-.274-3.208 1.192a11.274 11.274.0 00-5.684.0C5.958 2.566 4.95 2.841 4.95 2.841a3.85 3.85.0 00-.092 2.934A4.216 4.216.0 003.666 8.708c0 4.216 2.474 5.226 5.042 5.5-.55.55-.55 1.1-.458 1.834v3.208"/></svg></a></li><li><a href=https://www.instagram.com/gyeongminx/ target=_blank title=instagram rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-instagram" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 4m0 4a4 4 0 014-4h8a4 4 0 014 4v8a4 4 0 01-4 4H8a4 4 0 01-4-4z"/><path d="M12 12m-3 0a3 3 0 106 0 3 3 0 10-6 0"/><path d="M16.5 7.5v.01"/></svg></a></li><li><a href=https://www.linkedin.com/in/gyeongmin-lee-865448256/ target=_blank title=Linkedin rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-linkedin" width="44" height="44" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 4m0 2a2 2 0 012-2h12a2 2 0 012 2v12a2 2 0 01-2 2H6a2 2 0 01-2-2z"/><path d="M8 11v5"/><path d="M8 8v.01"/><path d="M12 16v-5"/><path d="M16 16v-3a2 2 0 00-4 0"/></svg></a></li><li><a href=mailto:gyeongmin@hansung.ac.kr target=_blank title=mail rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-mail" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path d="M0 0h24v24H0z" stroke="none"/><path d="M3 7a2 2 0 012-2h14a2 2 0 012 2v10a2 2 0 01-2 2H5a2 2 0 01-2-2V7z"/><path d="m3 7 9 6 9-6"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg><span>About</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg><span>Archives</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>Search</span></a></li><li><a href=/links/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg><span>Links</span></a></li><div class=menu-bottom-section><li id=i18n-switch><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg><select name=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://gyeongmin.kr/ selected>한국어</option><option value=https://gyeongmin.kr/en/>English</option></select></li><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>다크 모드</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">목차</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#손실-함수>손실 함수</a></li><li><a href=#소프트맥스>소프트맥스</a></li><li><a href=#교차-엔트로피-오차>교차 엔트로피 오차</a></li><li><a href=#행렬의-미분>행렬의 미분</a></li><li><a href=#연쇄-법칙>연쇄 법칙</a></li><li><a href=#가중치-갱신>가중치 갱신</a></li><li><a href=#확률적경사하강법>확률적경사하강법</a></li><li><a href=#신경망-예제>신경망 예제</a></li><li><a href=#sigmoid의-역전파-구현>Sigmoid의 역전파 구현</a></li><li><a href=#affine의-역전파-구현>Affine의 역전파 구현</a></li><li><a href=#twolayernet-구현>TwoLayerNet 구현</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/neural-network-trainning/><img src=/../images/deep-learning-from-scratch.jpeg loading=lazy alt="Featured image of post 신경망의 학습"></a></div><div class=article-details><header class=article-category><a href=/categories/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-2/>밑바닥부터 시작하는 딥러닝 2</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/neural-network-trainning/>신경망의 학습</a></h2><h3 class=article-subtitle>신경망의 학습과정에서 손실 함수, 행렬 미분, 오차역전파법에 대한 설명과 구현</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Nov 21, 2023</time></div></footer></div></header><section class=article-content><blockquote><p>본 포스팅은 &lsquo;밑바닥부터 시작하는 딥러닝 2&rsquo; 교재를 참고했습니다.</p></blockquote><h1 id=신경망의-학습>신경망의 학습</h1><blockquote><p>학습되지 않은 신경망은 좋은 추론을 할 수 없다. 따라서 학습을 먼저 수행하고, 학습된 매개변수를 이용해 추론을 수행해야 한다.</p></blockquote><h2 id=손실-함수>손실 함수</h2><blockquote><p>신경망 학습에는 학습이 얼마나 잘 되고 있는지를 알기 위한 척도가 필요하다.</p></blockquote><p><strong>손실</strong>이란 신경망이 예측한 결과를 비교하여 예측이 얼마나 나쁜가를 산출한 <strong>스칼라 값</strong>으로, 성능을 나타내는 척도이다.</p><p>이것을 구하는 것이 바로 <strong>손실 함수</strong>이다.</p><p>우리는 소프트맥스와 교차 엔트로피 오차를 통해 손실 함수를 구현할 것이다.</p><h2 id=소프트맥스>소프트맥스</h2><p>$$
p_k = \frac{\exp{(s_k)}}{\sum_{k=1}^{n}{\exp{(s_k)}}} \quad for \ k=1,2,\dots,k
\tag{1}
$$</p><p>[식 1]에서 소프트맥스 함수의 출력의 각 원소 $p_k$는 $0 \leq p_k \leq 1,\space p_k \in \mathbb{R}$ 이다.</p><p>따라서 소프트맥스의 출력은 <strong>확률</strong>로 해석할 수 있다. 우리는 이것을 교차 엔트로피 오차에 입력할 것이다.</p><h2 id=교차-엔트로피-오차>교차 엔트로피 오차</h2><p>$$
Loss = - \sum_{k}t_k \log{p_k}
\tag{2}
$$</p><p>[식 2]에서 $t_k$는 $k$번째 클래스의 정답 레이블이다. $t = \begin{bmatrix} 0, 1, 1 \end{bmatrix}$ 과 같이 one-hot vector로 표기한다.</p><blockquote><p>one-hot vector는 단 하나의 원소만 1 이고 그 외에는 0인 벡터이다.</p></blockquote><p>미니 배치를 고려하면 교차 엔트로피 오차의 식은 아래와 같이 바뀌게 된다.</p><p>$$
Loss = - \frac{1}{N} \sum_{n} \sum_{k}t_{nk} \log{p_{nk}}
\tag{3}
$$</p><p>[식 3]에서 $N$은 미니 배치의 개수, $t_{nk}$는 $n$번째 데이터의 $k$차원째의 값,
$p_{nk}$는 신경망의 출력, $t_{nk}$는 정답 레이블이다.</p><p>이는 N으로 나눠서 1 개당의 <strong>평균 손실 함수</strong>를 구하는 것이다. 미니배치의 크기에 관계없이 항상 일관된 척도를 얻을 수 있다.</p><h2 id=행렬의-미분>행렬의 미분</h2><blockquote><p>신경망 학습의 목표는 손실을 최소화하는 매개변수를 찾는 것이다. 이때 중요한 것이 바로 미분과 기울기이다.</p></blockquote><p>행렬을 입력이나 출력으로 가지는 함수를 미분하는 것을 <strong>행렬 미분</strong>이라고 한다. (정확하게는 편미분이다.)</p><p>또한 행렬미분에는 분자중심 표현법과 분모중심 표현법 두 가지가 있는데, 본 포스팅에서는 분모중심 표현법으로 서술하겠다.</p><blockquote><p>행렬 미분에 대한 상세한 정의는 <a class=link href=https://geniewishescometrue.tistory.com/entry/%EC%84%A0%ED%98%95%EB%8C%80%EC%88%98%ED%95%99-%ED%96%89%EB%A0%AC%EB%AF%B8%EB%B6%84-Matrix-Calculus target=_blank rel=noopener>여기</a>를 참고하기 바란다.</p></blockquote><p>$$
\frac{\partial L}{\partial \mathbf{x}}=
\begin{pmatrix} \frac{\partial L}{\partial x_1} & \frac{\partial L}{\partial x_2} & \cdots & \frac{\partial L}{\partial x_n}
\end{pmatrix}
\tag{4}
$$</p><p>$L$은 스칼라, $x$는 벡터인 함수 $L=f(x)$가 있을 때, $x_i$에 대한 $L$의 미분은 $\frac{\partial y}{\partial x_i}$로 쓸 수 있으며, 이를 정리하면 [식 4]와 같다.</p><p>$$
\frac{\partial L}{\partial \mathbf{W}}=
\begin{pmatrix}
\frac{\partial L}{\partial W_{11}} & \cdots & \frac{\partial L}{\partial W_{1n}}
\\ \vdots & \ddots
\\ \frac{\partial L}{\partial W_{m1}} & & \frac{\partial L}{\partial W_{mn}}
\end{pmatrix}
\tag{5}
$$</p><p>$\mathbf{W}$가 $m \times n$ 행렬이라면, $L = g(\mathbf{W})$ 함수의 기울기는 [식 5] 같이 쓸 수 있다.</p><p>여기서 중요한 점은 $\mathbf{W}$와 $\frac{\partial L}{\partial \mathbf{x}}$의 형상이 같다는 것이다. 이 성질을 이용하면 매개변수 갱신과 연쇄 법칙을 쉽게
구현할 수 있다.</p><h2 id=연쇄-법칙>연쇄 법칙</h2><blockquote><p>우리는 신경망의 학습을 위해 각 매개변수에 대한 손실의 기울기를 구해 매개변수를 갱신할 것이다. 신경망의 기울기는 오차역전파법 (back-propagation)을 통해 구할 수 있으며, 이 때 필요한 것이 연쇄 법칙이다.</p></blockquote><p>$y=f(x)$와 $z=g(y)$라는 두 함수가 있을 때, $z=g(f(x))$이다.</p><p>$$
\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \frac{\partial y}{\partial x}
\tag{6}
$$</p><p>$x$에 대한 $z$의 미분은 [식 6]과 같이 $y=f(x)$의 미분과 $z=g(y)$의 미분을 곱해 구할 수 있다. 이것이 바로 연쇄 법칙이다.</p><p>즉, 함수가 아무리 복잡하더라도 개별 함수들의 미분을 통해 효율적인 계산을 할 수 있다는 것이다.</p><h2 id=가중치-갱신>가중치 갱신</h2><p>신경망의 학습은 다음 순서로 수행된다.</p><pre class=mermaid>graph LR
    a((미니배치))-->b((기울기 계산))-->c((매개변수 갱신))-->a
  </pre><p>우선 미니배치에서 데이터를 선택하고, 이어서 오차역전파법으로 가중치의 기울기를 얻는다. 이 기울기는 현재의 가중치 매개변수에서 손실을 가장 크게 하는 방향을 가리킨다. 따라서 매개변수를 그 기울기와 반대 방향으로 갱신하면 손실을 줄일 수 있다. 이것이 바로 <strong>경사하강법</strong>이다.</p><h2 id=확률적경사하강법>확률적경사하강법</h2><p>확률적 경사하강법 (Stochastic Gradient Descent)은 무작위로 선택된 데이터(미니배치)에 대한 기울기를 이용하여, 현재의 가중치를 기울기 방향으로 일정한 거리만큼 갱신한다.</p><p>$$
W \gets {W} - \eta {\frac{\partial {L}}{\partial {W}}}
\tag{7}
$$</p><p>[식 7]에서 갱신하는 가중치 매개변수는 $\mathbf{W}$
이고, $\mathbf{W}$에 대한 손실 함수의 기울기는 $\frac{\partial {L}}{\partial {W}}$이다. $\eta$는 학습률 (learning rate)을 나타내고, 0.01이나 0.001 같은 값을 미리 정해서 사용한다.</p><p>이것을 파이썬으로 구현하면 아래와 같다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>class</span> <span class=nc>SGD</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>lr</span> <span class=o>=</span> <span class=n>lr</span>  <span class=c1># 학습률</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>update</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>params</span><span class=p>,</span> <span class=n>grads</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>params</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>            <span class=n>params</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>-=</span> <span class=bp>self</span><span class=o>.</span><span class=n>lr</span> <span class=o>*</span> <span class=n>grads</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=신경망-예제>신경망 예제</h2><p>이전 포스팅에서 설계한 신경망에 Softmax 레이어와 Cross Entropy Error 레이어를 새로 추가해 보자.</p><pre class=mermaid>graph LR
    x(x)==> A(Affine)
    A==> B(Sigmoid)
    B==> C(Affine)
    C==>D(Softmax)
    t(t)==>E
    D==>E(Cross Entropy Error)
    E==>F(L)
  </pre><p>$\textbf{x}$는 입력 데이터, $\textbf{t}$는 정답 레이블. $L$은 손실을 나타낸다.</p><h2 id=sigmoid의-역전파-구현>Sigmoid의 역전파 구현</h2><p>Sigmoid의 수식은 $y=\frac{1}{1+\exp{(-x)}}$이다. 그 미분은 아래 [식 7]과 같다.</p><p>$$
\frac{\partial y}{\partial x} = y(1-y)
\tag{8}
$$</p><p>이를 파이썬으로 구현하면 아래와 같다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>class</span> <span class=nc>Sigmoid</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span> 
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>params</span> <span class=o>=</span> <span class=p>[]</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=mi>1</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>backward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dout</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>dout</span> <span class=o>*</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>out</span><span class=p>)</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>out</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=affine의-역전파-구현>Affine의 역전파 구현</h2><p>Affine의 역전파는 MatMul 노드와 Repeat 노드의 역전파를 수행하면 구할 수 있다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>class</span> <span class=nc>Affine</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>W</span><span class=p>,</span> <span class=n>b</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>params</span> <span class=o>=</span> <span class=p>[</span><span class=n>W</span><span class=p>,</span> <span class=n>b</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>grads</span> <span class=o>=</span> <span class=p>[</span><span class=n>np</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>W</span><span class=p>),</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>b</span><span class=p>)]</span> 
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>x</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>W</span><span class=p>,</span> <span class=n>b</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>params</span> 
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>xz</span> <span class=n>W</span><span class=p>)</span> <span class=o>+</span> <span class=n>b</span> 
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>x</span> <span class=o>=</span> <span class=n>x</span> 
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>out</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>backward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dout</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>W</span><span class=p>,</span> <span class=n>b</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>params</span>
</span></span><span class=line><span class=cl>        <span class=n>dx</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>doutz</span> <span class=n>W</span><span class=o>.</span><span class=n>T</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>dW</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>x</span><span class=o>.</span><span class=n>T</span><span class=p>,</span> <span class=n>dout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>db</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dout</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>grads</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=o>...</span><span class=p>]</span> <span class=o>=</span> <span class=n>dW</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>grads</span><span class=p>[</span><span class=mi>1</span><span class=p>][</span><span class=o>...</span><span class=p>]</span> <span class=o>=</span> <span class=n>db</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>dx</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=twolayernet-구현>TwoLayerNet 구현</h2><p>Sigmoid와 Affine 레이어의 back-propagation을 구현했으니, 이제 <code>TwoLayerNet</code> 클래스를 완성해 보자.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>class</span> <span class=nc>SoftmaxWithLoss</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>params</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>grads</span> <span class=o>=</span> <span class=p>[],</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>y</span> <span class=o>=</span> <span class=kc>None</span>  <span class=c1># softmax의 출력</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>t</span> <span class=o>=</span> <span class=kc>None</span>  <span class=c1># 정답 레이블</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>t</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>t</span> <span class=o>=</span> <span class=n>t</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>y</span> <span class=o>=</span> <span class=n>softmax</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 정답 레이블이 원핫 벡터일 경우 정답의 인덱스로 변환</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>t</span><span class=o>.</span><span class=n>size</span> <span class=o>==</span> <span class=bp>self</span><span class=o>.</span><span class=n>y</span><span class=o>.</span><span class=n>size</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>t</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>t</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>cross_entropy_error</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>y</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>loss</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>TwoLayerNet</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>output_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>I</span><span class=p>,</span> <span class=n>H</span><span class=p>,</span> <span class=n>O</span> <span class=o>=</span> <span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>output_size</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>W1</span> <span class=o>=</span> <span class=mf>0.01</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>I</span><span class=p>,</span> <span class=n>H</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>b1</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>H</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>W2</span> <span class=o>=</span> <span class=mf>0.01</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>H</span><span class=p>,</span> <span class=n>O</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>b2</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>O</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>layers</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=n>Affine</span><span class=p>(</span><span class=n>W1</span><span class=p>,</span> <span class=n>b1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>Sigmoid</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>Affine</span><span class=p>(</span><span class=n>W2</span><span class=p>,</span> <span class=n>b2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>loss_layer</span> <span class=o>=</span> <span class=n>SoftmaxWithLoss</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>params</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>grads</span> <span class=o>=</span> <span class=p>[],</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>params</span> <span class=o>+=</span> <span class=n>layer</span><span class=o>.</span><span class=n>params</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>grads</span> <span class=o>+=</span> <span class=n>layer</span><span class=o>.</span><span class=n>grads</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>layer</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>t</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>score</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>loss_layer</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>score</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>loss</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>backward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dout</span><span class=o>=</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>dout</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>loss_layer</span><span class=o>.</span><span class=n>backward</span><span class=p>(</span><span class=n>dout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=nb>reversed</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>dout</span> <span class=o>=</span> <span class=n>layer</span><span class=o>.</span><span class=n>backward</span><span class=p>(</span><span class=n>dout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>dout</span>
</span></span></code></pre></td></tr></table></div></div></section><script type=module>
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';
    mermaid.initialize({ startOnLoad: true });
    if (document.documentElement.dataset.scheme == 'dark') {
      mermaid.initialize({theme: 'dark'})
    } else {
      mermaid.initialize({theme: 'light', themeVariables: {xyChart: {plotColorPalette: '#3366FF', backgroundColor: '#FAFAFA'}}})
    }
  </script><footer class=article-footer><section class=article-tags><a href=/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D/>딥러닝</a>
<a href=/tags/python/>Python</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.css integrity="sha256-J+iAE0sgH8QSz9hpcDxXIftnj65JEZgNhGcgReTTK9s=" crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.js integrity="sha256-InsNdER1b2xUewP+pKCUJpkhiqwHgqiPXDlIk7GzBu4=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/contrib/auto-render.min.js integrity="sha256-y39Mpg7V3D4lhBX4x6O0bUqTV4pSrfgwEfGKfxkOdgI=" crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".article-content");if(e){let t=e.innerHTML;t=t.replace(/(\$\$)([\s\S]+?)(\$\$)/g,(e,t,n,s)=>{const o=n.replace(/<em>/g,"_").replace(/<\/em>/g,"_");return o=o.replace(/\\big[oO]/g,"\\mathcal{O}"),`<div markdown="katex">
${t}
${o.trim()}
${s}
</div>`}),e.innerHTML=t,renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],ignoredClasses:["gist"]})}})</script></article><aside class=related-content--wrapper><h2 class=section-title>관련 글</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/word2vec-2/><div class=article-image><img src=/../images/deep-learning-from-scratch.jpeg loading=lazy data-key=word2vec-2 data-hash=/../images/deep-learning-from-scratch.jpeg></div><div class=article-details><h2 class=article-title>Word2Vec의 최적화</h2></div></a></article><article class=has-image><a href=/p/word2vec/><div class=article-image><img src=/../images/deep-learning-from-scratch.jpeg loading=lazy data-key=word2vec data-hash=/../images/deep-learning-from-scratch.jpeg></div><div class=article-details><h2 class=article-title>word2vec을 이용한 단어 임베딩</h2></div></a></article><article class=has-image><a href=/p/word-distributed-representation/><div class=article-image><img src=/../images/deep-learning-from-scratch.jpeg loading=lazy data-key=word-distributed-representation data-hash=/../images/deep-learning-from-scratch.jpeg></div><div class=article-details><h2 class=article-title>단어의 분산 표현</h2></div></a></article><article class=has-image><a href=/p/neural-network-inference/><div class=article-image><img src=/../images/deep-learning-from-scratch.jpeg loading=lazy data-key=neural-network-inference data-hash=/../images/deep-learning-from-scratch.jpeg></div><div class=article-details><h2 class=article-title>신경망의 추론</h2></div></a></article><article class=has-image><a href=/p/basic-linear-algebra/><div class=article-image><img src=/../images/deep-learning-from-scratch.jpeg loading=lazy data-key=basic-linear-algebra data-hash=/../images/deep-learning-from-scratch.jpeg></div><div class=article-details><h2 class=article-title>기초 선형대수 - 스칼라, 벡터, 행렬</h2></div></a></article></div></div></aside><script src=https://giscus.app/client.js data-repo=gyeongminn/gyeongminn.github.io data-repo-id=R_kgDOKsf9nw data-category=Announcements data-category-id=DIC_kwDOKsf9n84Ca_6Y data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=light data-lang=ko crossorigin=anonymous async></script>
<script>function setGiscusTheme(e){let t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:{setConfig:{theme:e}}},"https://giscus.app")}(function(){addEventListener("message",t=>{if(event.origin!=="https://giscus.app")return;e()}),window.addEventListener("onColorSchemeChange",e);function e(){setGiscusTheme(document.documentElement.dataset.scheme==="light"?"light":"noborder_gray")}})()</script><footer class=site-footer><section class=copyright>&copy;
2023 -
2025 Gyeongmin Lee</section><section class=powerby><br></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://cdn.jsdelivr.net/gh/orioncactus/pretendard@v1.3.9/dist/web/static/pretendard.min.css",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>