<!doctype html><html lang=ko dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="추론 기반 기법과 단순한 word2vec의 구현"><title>word2vec을 이용한 단어 임베딩</title><link rel=canonical href=https://gyeongmin.kr/p/word2vec/><link rel=stylesheet href=/scss/style.min.2325ab3f1c53340b74d88bd622b7f5bcc047e55d78831b42a905dd43de9993e0.css><meta property="og:title" content="word2vec을 이용한 단어 임베딩"><meta property="og:description" content="추론 기반 기법과 단순한 word2vec의 구현"><meta property="og:url" content="https://gyeongmin.kr/p/word2vec/"><meta property="og:site_name" content="Gyeongmin의 개발 블로그"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="딥러닝"><meta property="article:tag" content="Python"><meta property="article:published_time" content="2023-12-19T00:00:00+00:00"><meta property="article:modified_time" content="2023-12-19T00:00:00+00:00"><meta property="og:image" content="https://gyeongmin.kr/images/deep-learning-from-scratch.jpeg"><meta name=twitter:title content="word2vec을 이용한 단어 임베딩"><meta name=twitter:description content="추론 기반 기법과 단순한 word2vec의 구현"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gyeongmin.kr/images/deep-learning-from-scratch.jpeg"><link rel="shortcut icon" href=/logo.ico/favicon.ico><script async src="https://www.googletagmanager.com/gtag/js?id=G-RBG63ZJRZM"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-RBG63ZJRZM",{anonymize_ip:!1})}</script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2100464285092061" crossorigin=anonymous></script>
<link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color=#757575><meta name=msapplication-TileColor content="#da532c"><meta name=theme-color content="#000000"><link rel=icon href=/logo.ico/favicon.ico><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2100464285092061" crossorigin=anonymous></script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="메뉴 여닫기">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/profile_hu5d202280e416202dba72f1bb8473638c_1285338_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>Gyeongmin의 개발 블로그</a></h1><h2 class=site-description>Vision Engineer</h2></div></header><ol class=social-menu><li><a href=https://github.com/gyeongminn target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="22" height="22" viewBox="1 -2 19 25" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path d="M0 0h22v22H0z" stroke="none"/><path d="M8.25 17.416c-3.942 1.284-3.942-2.292-5.5-2.75m11 4.584v-3.208c0-.916.092-1.284-.458-1.834 2.566-.274 5.042-1.284 5.042-5.5a4.216 4.216.0 00-1.192-2.934A3.85 3.85.0 0017.05 2.84s-1.008-.274-3.208 1.192a11.274 11.274.0 00-5.684.0C5.958 2.566 4.95 2.841 4.95 2.841a3.85 3.85.0 00-.092 2.934A4.216 4.216.0 003.666 8.708c0 4.216 2.474 5.226 5.042 5.5-.55.55-.55 1.1-.458 1.834v3.208"/></svg></a></li><li><a href=https://www.instagram.com/gyeongminx/ target=_blank title=instagram rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-instagram" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 4m0 4a4 4 0 014-4h8a4 4 0 014 4v8a4 4 0 01-4 4H8a4 4 0 01-4-4z"/><path d="M12 12m-3 0a3 3 0 106 0 3 3 0 10-6 0"/><path d="M16.5 7.5v.01"/></svg></a></li><li><a href=https://www.linkedin.com/in/gyeongmin-lee-865448256/ target=_blank title=Linkedin rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-linkedin" width="44" height="44" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 4m0 2a2 2 0 012-2h12a2 2 0 012 2v12a2 2 0 01-2 2H6a2 2 0 01-2-2z"/><path d="M8 11v5"/><path d="M8 8v.01"/><path d="M12 16v-5"/><path d="M16 16v-3a2 2 0 00-4 0"/></svg></a></li><li><a href=mailto:gyeongmin@hansung.ac.kr target=_blank title=mail rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-mail" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path d="M0 0h24v24H0z" stroke="none"/><path d="M3 7a2 2 0 012-2h14a2 2 0 012 2v10a2 2 0 01-2 2H5a2 2 0 01-2-2V7z"/><path d="m3 7 9 6 9-6"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg><span>About</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg><span>Archives</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>Search</span></a></li><li><a href=/links/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg><span>Links</span></a></li><div class=menu-bottom-section><li id=i18n-switch><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg><select name=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://gyeongmin.kr/ selected>한국어</option><option value=https://gyeongmin.kr/en/>English</option></select></li><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>다크 모드</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">목차</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#추론-기반-기법과-신경망>추론 기반 기법과 신경망</a><ol><li><a href=#통계-기반-기법의-문제점>통계 기반 기법의 문제점</a></li><li><a href=#추론-기반-기법의-개요>추론 기반 기법의 개요</a></li><li><a href=#신경망에서의-단어-처리>신경망에서의 단어 처리</a></li></ol></li><li><a href=#cbow>CBOW</a><ol><li><a href=#cbow-모델의-추론-처리>CBOW 모델의 추론 처리</a></li><li><a href=#cbow-모델의-학습>CBOW 모델의 학습</a></li><li><a href=#word2vec의-가중치와-분산-표현>word2vec의 가중치와 분산 표현</a></li></ol></li><li><a href=#학습-데이터-준비>학습 데이터 준비</a><ol><li><a href=#맥락과-타깃>맥락과 타깃</a></li><li><a href=#원핫-벡터로-변환>원핫 벡터로 변환</a></li></ol></li><li><a href=#cbow-모델-구현>CBOW 모델 구현</a><ol><li><a href=#학습코드구현>학습코드구현</a></li></ol></li><li><a href=#word2vec-보충>word2vec 보충</a><ol><li><a href=#cbow-모델과-확률>CBOW 모델과 확률</a></li><li><a href=#skip-gram-모델>skip-gram 모델</a></li><li><a href=#통계-기반-vs-추론-기반>통계 기반 vs 추론 기반</a></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/word2vec/><img src=/../images/deep-learning-from-scratch.jpeg loading=lazy alt="Featured image of post word2vec을 이용한 단어 임베딩"></a></div><div class=article-details><header class=article-category><a href=/categories/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-2/>밑바닥부터 시작하는 딥러닝 2</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/word2vec/>word2vec을 이용한 단어 임베딩</a></h2><h3 class=article-subtitle>추론 기반 기법과 단순한 word2vec의 구현</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Dec 19, 2023</time></div></footer></div></header><section class=article-content><blockquote><p>본 포스팅은 &lsquo;밑바닥부터 시작하는 딥러닝 2&rsquo; 교재를 참고했습니다.</p></blockquote><h2 id=추론-기반-기법과-신경망>추론 기반 기법과 신경망</h2><p>단어를 벡터로 표현하는 방법은 통계 기반 기법과 추론 기반 기법이 있다. 둘 모두 <a class=link href=https://gyeongminn.github.io/p/word-distributed-representation/#%eb%b6%84%ed%8f%ac-%ea%b0%80%ec%84%a4%ea%b3%bc-%eb%b6%84%ec%82%b0-%ed%91%9c%ed%98%84 target=_blank rel=noopener>분포 가설</a>을 기반으로 한다.</p><h3 id=통계-기반-기법의-문제점>통계 기반 기법의 문제점</h3><p>통계 기반 기법은 주변 단어의 빈도를 기초로 단어를 표현한다. 단어 수가 $N$개일 때, $N \times N$이라는 거대한 행렬을 만들게 된다. 영어 어휘만 해도 100만 개에 가까운데, 그렇다면 1조개의 원소를 가진 행렬이 필요하다는 것이다.</p><p>이렇게 통계 기반 기법처럼 학습 데이터를 한번에 처리하지 말고, <strong>데이터를 작게 나눠 순차적으로 학습</strong>시키는 (미니배치 학습) 방법이 필요하다.</p><h3 id=추론-기반-기법의-개요>추론 기반 기법의 개요</h3><p>추론이란 <strong>주변 단어</strong> (맥락)가 주어졌을 때, 무슨 단어가 들어갈 지 단어를 <strong>유추</strong>하는 것이다.</p><p>맥락 정보를 입력받아 출현할 수 있는 단어들의 확률분포를 나타내는 모델을 만들고, 학습의 결과로 분산 표현을 얻는 것이 추론 기반 기법이다.</p><h3 id=신경망에서의-단어-처리>신경망에서의 단어 처리</h3><p>신경망은 단어를 그대로 처리할 수 없기 때문에, 단어를 고정된 길이의 벡터로 변환해야 한다. 이를 위해 가장 대표적으로 사용되는 방법이 <strong>원핫 인코딩</strong>(one-hot encoding)이다.</p><div class=table-wrapper><table><thead><tr><th style=text-align:center>단어(텍스트)</th><th style=text-align:center>단어 ID</th><th style=text-align:center>원핫 표현</th></tr></thead><tbody><tr><td style=text-align:center>you</td><td style=text-align:center>0</td><td style=text-align:center>(1, 0, 0, 0, 0, 0, 0, 0)</td></tr><tr><td style=text-align:center>goodbye</td><td style=text-align:center>2</td><td style=text-align:center>(0, 0, 1, 0, 0, 0, 0, 0)</td></tr></tbody></table></div><p>위와 같이 단어는 텍스트, 단어 ID, 원핫 표현으로 나타낼 수 있다. 단어를 고정 크기의 원핫 표현으로 나타내게 되면 뉴런의 수를 고정할 수 있다.</p><p>신경망을 구성하는 계층들이 벡터를 처리할 수 있으므로, 이제 단어를 신경망으로 처리할 수 있을 것이다.</p><p><img src=/p/word2vec/image.png width=562 height=459 srcset="/p/word2vec/image_hu3f7b4f5201a3d844f9498b9d3ce51d55_90389_480x0_resize_box_3.png 480w, /p/word2vec/image_hu3f7b4f5201a3d844f9498b9d3ce51d55_90389_1024x0_resize_box_3.png 1024w" loading=lazy alt="완전연결계층에 의한 변환을 단순화한 그림" class=gallery-image data-flex-grow=122 data-flex-basis=293px></p><p>완전연결계층의 계산은 행렬 곱으로 수행할 수 있고. 행렬 곱은 넘파이의 <code>np.matmul()</code>로 할 수 있다.</p><h2 id=cbow>CBOW</h2><h3 id=cbow-모델의-추론-처리>CBOW 모델의 추론 처리</h3><p>CBOW 모델은 맥락으로부터 타깃을 추측하는 용도의 신경망이다. (타깃은 중앙 단어, 맥락은 주변 단어를 의미한다)</p><p><img src=/p/word2vec/image-1.png width=853 height=829 srcset="/p/word2vec/image-1_hub4bfc2fe41a6c706de102652017462d2_200069_480x0_resize_box_3.png 480w, /p/word2vec/image-1_hub4bfc2fe41a6c706de102652017462d2_200069_1024x0_resize_box_3.png 1024w" loading=lazy alt="CBOW 모델의 신경망 구조" class=gallery-image data-flex-grow=102 data-flex-basis=246px></p><p>입력층이 2개 있고, 은닉층을 거쳐 출력층에 도달한다.</p><p>두 입력층에서 은닉층으로의 변환은 완전연결계층이 수행한다. 그리고 은닉층에서 출력층 뉴런으로의 변환은 다른 완전연결계층이 처리한다. 입력층이 여러 개이면 전체를 <strong>평균</strong>하면 된다.</p><p>출력층의 뉴런은 총 7개인데, 이 뉴런 하나하나가 각각의 단어에 대응한다. 출력층 뉴런은 각 단어의 <strong>점수</strong>를 뜻하며, 값이 높을수록 대응 단어의 출현 확률도 높아진다. 이 점수에 소프트맥스 함수를 적용해서, <strong>확률</strong>을 얻을 수 있다.</p><p>학습을 진행할수록 맥락에서 출현하는 단어를 잘 추측하는 방향으로 이 분산 표현들이 갱신된다. 이렇게 얻은 벡터에는 <strong>단어의 의미도 포함되어 있다</strong>.</p><p><img src=/p/word2vec/image-2.png width=1009 height=771 srcset="/p/word2vec/image-2_hu83e02f47960715b8f885eb14234d3d19_138189_480x0_resize_box_3.png 480w, /p/word2vec/image-2_hu83e02f47960715b8f885eb14234d3d19_138189_1024x0_resize_box_3.png 1024w" loading=lazy alt="계층 관점에서 본 CBOW 모델의 신경망 구성" class=gallery-image data-flex-grow=130 data-flex-basis=314px></p><p>이제 CBOW 모델의 추론 처리를 파이썬으로 구현해 보자.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>class</span> <span class=nc>MatMul</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>W</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>params</span> <span class=o>=</span> <span class=p>[</span><span class=n>W</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>grads</span> <span class=o>=</span> <span class=p>[</span><span class=n>np</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>W</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>x</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>W</span><span class=p>,</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>params</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>W</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>x</span> <span class=o>=</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>out</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>backward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dout</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>W</span><span class=p>,</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>params</span>
</span></span><span class=line><span class=cl>        <span class=n>dx</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>dout</span><span class=p>,</span> <span class=n>W</span><span class=o>.</span><span class=n>T</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>dW</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>x</span><span class=o>.</span><span class=n>T</span><span class=p>,</span> <span class=n>dout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>grads</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=o>...</span><span class=p>]</span> <span class=o>=</span> <span class=n>dW</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>dx</span>
</span></span></code></pre></td></tr></table></div></div><p>MatMul 계층은 내부에서 행렬 곱을 계산한다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=c1># 샘플 맥락 데이터</span>
</span></span><span class=line><span class=cl><span class=n>c0</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>]])</span>
</span></span><span class=line><span class=cl><span class=n>c1</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>]])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 가중치 초기화</span>
</span></span><span class=line><span class=cl><span class=n>W_in</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>7</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>W_out</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>7</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 계층 생성</span>
</span></span><span class=line><span class=cl><span class=n>in_layer0</span> <span class=o>=</span> <span class=n>MatMul</span><span class=p>(</span><span class=n>W_in</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>in_layer1</span> <span class=o>=</span> <span class=n>MatMul</span><span class=p>(</span><span class=n>W_in</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>out_layer</span> <span class=o>=</span> <span class=n>MatMul</span><span class=p>(</span><span class=n>W_out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 순전파</span>
</span></span><span class=line><span class=cl><span class=n>h0</span> <span class=o>=</span> <span class=n>in_layer0</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>c0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>h1</span> <span class=o>=</span> <span class=n>in_layer1</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>c1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>h</span> <span class=o>=</span> <span class=mf>0.5</span> <span class=o>*</span> <span class=p>(</span><span class=n>h0</span> <span class=o>+</span> <span class=n>h1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>s</span> <span class=o>=</span> <span class=n>out_layer</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>h</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>s</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>CBOW 모델은 활성화 함수를 사용하지 않는 간단한 구성의 신경망이다.</p><h3 id=cbow-모델의-학습>CBOW 모델의 학습</h3><blockquote><p>모델이 올바른 예측을 할 수 있도록 가중치를 조정해야 한다.</p></blockquote><p>소프트맥스 함수를 이용 해 점수를 확률로 변환하고, 그 확률과 정답 레이블로부터 교차 엔트로피 오차를 구한 후, 그 값을 손실로 사용해 학습을 진행한다.</p><p><img src=/p/word2vec/image-3.png width=995 height=596 srcset="/p/word2vec/image-3_hu42c86eda2c2eb22ccbe976f58bc00797_127466_480x0_resize_box_3.png 480w, /p/word2vec/image-3_hu42c86eda2c2eb22ccbe976f58bc00797_127466_1024x0_resize_box_3.png 1024w" loading=lazy alt="CBOW 모델의 학습 시 신경망 구성" class=gallery-image data-flex-grow=166 data-flex-basis=400px></p><p>앞서 구현한 추론 처리를 수행하는 CBOW 모델에 <code>Softmax</code> 계층과 <code>Cross Entropy Error</code> 계층을 추가하기만 하면 된다.</p><h3 id=word2vec의-가중치와-분산-표현>word2vec의 가중치와 분산 표현</h3><p>word2vec에서 사용되는 신경망에는 두 가지 가중치가 존재한다.</p><ol><li><p>입력 측 가중치 $ W_\text{in} $: 입력 측 완전연결계층의 가중치로, 각 행은 해당 단어의 분산 표현을 나타낸다.</p></li><li><p>출력 측 가중치 $ W_\text{out} $: 출력 측 완전연결계층의 가중치로, 단어의 의미가 인코딩된 벡터가 각 열에 저장된다.</p></li></ol><p>일반적으로 word2vec에서는 입력 측 가중치 $ W_\text{in} $만을 최종 단어의 분산 표현으로 사용한다. 출력 측 가중치는 대부분의 연구에서 버려진다.</p><p><img src=/p/word2vec/image-4.png width=904 height=497 srcset="/p/word2vec/image-4_hu1af831d405d8a936ea25b48a045ffece_189259_480x0_resize_box_3.png 480w, /p/word2vec/image-4_hu1af831d405d8a936ea25b48a045ffece_189259_1024x0_resize_box_3.png 1024w" loading=lazy alt="각 단어의 분산 표현" class=gallery-image data-flex-grow=181 data-flex-basis=436px></p><p>두 가중치는 하나만 이용할 수도 있고, 둘을 합쳐서 사용할 수도 있다.</p><p>word2vec의 skip-gram 등 <strong>많은 연구에서는 입력 측의 가중치만 사용</strong>하고, GloVe에서는 두 가중치를 더하여 사용한다.</p><h2 id=학습-데이터-준비>학습 데이터 준비</h2><p>&ldquo;You say goodbye and I say hello&rdquo; 문장을 이용해 학습을 진행해 보자.</p><p>문장을 전처리하는 <code>preprocess</code> 함수는 <a class=link href=https://gyeongminn.github.io/p/word-distributed-representation/#%eb%a7%90%eb%ad%89%ec%b9%98-%ec%a0%84%ec%b2%98%eb%a6%ac target=_blank rel=noopener>여기</a>를 참고하자.</p><h3 id=맥락과-타깃>맥락과 타깃</h3><p>word2vec에서 이용하는 신경망의 입력은 <strong>맥락</strong>이다. 그 정답 레이블은 중앙 단어인 <strong>타깃</strong> 이다. 우리는 맥락을 입력했을 때, 타깃을 출력할 확률이 높아지도록 학습시키면 된다.</p><p><img src=/p/word2vec/image-5.png width=789 height=312 srcset="/p/word2vec/image-5_hu9e7d1c407f3b0c8c4571a4e51b18e111_176184_480x0_resize_box_3.png 480w, /p/word2vec/image-5_hu9e7d1c407f3b0c8c4571a4e51b18e111_176184_1024x0_resize_box_3.png 1024w" loading=lazy alt="맥락과 타깃의 예시" class=gallery-image data-flex-grow=252 data-flex-basis=606px></p><p>맥락과 타깃을 만드는 함수를 구현해 보자.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>def</span> <span class=nf>create_contexts_target</span><span class=p>(</span><span class=n>corpus</span><span class=p>,</span> <span class=n>window_size</span><span class=o>=</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>target</span> <span class=o>=</span> <span class=n>corpus</span><span class=p>[</span><span class=n>window_size</span><span class=p>:</span><span class=o>-</span><span class=n>window_size</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>contexts</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>idx</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>window_size</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>corpus</span><span class=p>)</span><span class=o>-</span><span class=n>window_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>cs</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=o>-</span><span class=n>window_size</span><span class=p>,</span> <span class=n>window_size</span> <span class=o>+</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>t</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=k>continue</span>
</span></span><span class=line><span class=cl>            <span class=n>cs</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>corpus</span><span class=p>[</span><span class=n>idx</span> <span class=o>+</span> <span class=n>t</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>contexts</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>cs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>contexts</span><span class=p>),</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>target</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=원핫-벡터로-변환>원핫 벡터로 변환</h3><p>맥락과 타깃을 단어 ID에서 원핫 표현으로 변환하기 위해, 원핫 벡터로 변환하는 함수를 구현해 보자.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>def</span> <span class=nf>convert_one_hot</span><span class=p>(</span><span class=n>corpus</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>N</span> <span class=o>=</span> <span class=n>corpus</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>corpus</span><span class=o>.</span><span class=n>ndim</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>one_hot</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>N</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>idx</span><span class=p>,</span> <span class=n>word_id</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>corpus</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>one_hot</span><span class=p>[</span><span class=n>idx</span><span class=p>,</span> <span class=n>word_id</span><span class=p>]</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>elif</span> <span class=n>corpus</span><span class=o>.</span><span class=n>ndim</span> <span class=o>==</span> <span class=mi>2</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>C</span> <span class=o>=</span> <span class=n>corpus</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>one_hot</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>N</span><span class=p>,</span> <span class=n>C</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>idx_0</span><span class=p>,</span> <span class=n>word_ids</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>corpus</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>idx_1</span><span class=p>,</span> <span class=n>word_id</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>word_ids</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=n>one_hot</span><span class=p>[</span><span class=n>idx_0</span><span class=p>,</span> <span class=n>idx_1</span><span class=p>,</span> <span class=n>word_id</span><span class=p>]</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>one_hot</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=cbow-모델-구현>CBOW 모델 구현</h2><p>그럼 이제 모델을 구현해 보자.</p><p><img src=/p/word2vec/image-6.png width=978 height=664 srcset="/p/word2vec/image-6_hud00d83d8669da602ae7fdcc74b583914_117555_480x0_resize_box_3.png 480w, /p/word2vec/image-6_hud00d83d8669da602ae7fdcc74b583914_117555_1024x0_resize_box_3.png 1024w" loading=lazy alt="CBOW 모델의신경망 구성" class=gallery-image data-flex-grow=147 data-flex-basis=353px></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>class</span> <span class=nc>SimpleCBOW</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>V</span><span class=p>,</span> <span class=n>H</span> <span class=o>=</span> <span class=n>vocab_size</span><span class=p>,</span> <span class=n>hidden_size</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 가중치 초기화</span>
</span></span><span class=line><span class=cl>        <span class=n>W_in</span> <span class=o>=</span> <span class=mf>0.01</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>V</span><span class=p>,</span> <span class=n>H</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=s1>&#39;f&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>W_out</span> <span class=o>=</span> <span class=mf>0.01</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>H</span><span class=p>,</span> <span class=n>V</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=s1>&#39;f&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 계층 생성</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>in_layer0</span> <span class=o>=</span> <span class=n>MatMul</span><span class=p>(</span><span class=n>W_in</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>in_layer1</span> <span class=o>=</span> <span class=n>MatMul</span><span class=p>(</span><span class=n>W_in</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>out_layer</span> <span class=o>=</span> <span class=n>MatMul</span><span class=p>(</span><span class=n>W_out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>loss_layer</span> <span class=o>=</span> <span class=n>SoftmaxWithLoss</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 모든 가중치와 기울기를 리스트에 모은다.</span>
</span></span><span class=line><span class=cl>        <span class=n>layers</span> <span class=o>=</span> <span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>in_layer0</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>in_layer1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>out_layer</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>params</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>grads</span> <span class=o>=</span> <span class=p>[],</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=n>layers</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>params</span> <span class=o>+=</span> <span class=n>layer</span><span class=o>.</span><span class=n>params</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>grads</span> <span class=o>+=</span> <span class=n>layer</span><span class=o>.</span><span class=n>grads</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 인스턴스 변수에 단어의 분산 표현을 저장한다.</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>word_vecs</span> <span class=o>=</span> <span class=n>W_in</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>contexts</span><span class=p>,</span> <span class=n>target</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>h0</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>in_layer0</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>contexts</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>h1</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>in_layer1</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>contexts</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>h</span> <span class=o>=</span> <span class=p>(</span><span class=n>h0</span> <span class=o>+</span> <span class=n>h1</span><span class=p>)</span> <span class=o>*</span> <span class=mf>0.5</span>
</span></span><span class=line><span class=cl>        <span class=n>score</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>out_layer</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>h</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>loss_layer</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>score</span><span class=p>,</span> <span class=n>target</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>loss</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>backward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dout</span><span class=o>=</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>ds</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>loss_layer</span><span class=o>.</span><span class=n>backward</span><span class=p>(</span><span class=n>dout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>da</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>out_layer</span><span class=o>.</span><span class=n>backward</span><span class=p>(</span><span class=n>ds</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>da</span> <span class=o>*=</span> <span class=mf>0.5</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>in_layer1</span><span class=o>.</span><span class=n>backward</span><span class=p>(</span><span class=n>da</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>in_layer0</span><span class=o>.</span><span class=n>backward</span><span class=p>(</span><span class=n>da</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=kc>None</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=학습코드구현>학습코드구현</h3><p>학습 데이터를 준비해 신경망에 입력한 다음, 기울기를 구하고 가중치 매개변수를 순서대로 갱신해보자.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=n>window_size</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=n>hidden_size</span> <span class=o>=</span> <span class=mi>5</span>
</span></span><span class=line><span class=cl><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>3</span>
</span></span><span class=line><span class=cl><span class=n>max_epoch</span> <span class=o>=</span> <span class=mi>1000</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=s1>&#39;You say goodbye and I say hello.&#39;</span>
</span></span><span class=line><span class=cl><span class=n>corpus</span><span class=p>,</span> <span class=n>word_to_id</span><span class=p>,</span> <span class=n>id_to_word</span> <span class=o>=</span> <span class=n>preprocess</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>vocab_size</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>word_to_id</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>contexts</span><span class=p>,</span> <span class=n>target</span> <span class=o>=</span> <span class=n>create_contexts_target</span><span class=p>(</span><span class=n>corpus</span><span class=p>,</span> <span class=n>window_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>target</span> <span class=o>=</span> <span class=n>convert_one_hot</span><span class=p>(</span><span class=n>target</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>contexts</span> <span class=o>=</span> <span class=n>convert_one_hot</span><span class=p>(</span><span class=n>contexts</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>SimpleCBOW</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>Adam</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>trainer</span> <span class=o>=</span> <span class=n>Trainer</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>trainer</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>contexts</span><span class=p>,</span> <span class=n>target</span><span class=p>,</span> <span class=n>max_epoch</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>trainer</span><span class=o>.</span><span class=n>plot</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>word_vecs</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>word_vecs</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>word_id</span><span class=p>,</span> <span class=n>word</span> <span class=ow>in</span> <span class=n>id_to_word</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=n>word_vecs</span><span class=p>[</span><span class=n>word_id</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><p>Optimizer는 Adam을 사용해서 학습시켰다.</p><p><img src=/p/word2vec/image-7.png width=640 height=480 srcset="/p/word2vec/image-7_hu7d4630c01d55ea2ccca985100a1b63c6_27676_480x0_resize_box_3.png 480w, /p/word2vec/image-7_hu7d4630c01d55ea2ccca985100a1b63c6_27676_1024x0_resize_box_3.png 1024w" loading=lazy alt="학습 경과 그래프" class=gallery-image data-flex-grow=133 data-flex-basis=320px></p><p>학습을 거듭할수록 손실이 줄어들고 있다. 그럼 이제 학습이 끝난 후의 가중치 매개변수를 확인해 보자.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>you [ 1.001226   1.0100921 -1.0480953 -1.1371888  1.4559563]
</span></span><span class=line><span class=cl>say [-1.1566567  -1.176362    1.1436371   1.10196     0.29608265]
</span></span><span class=line><span class=cl>goodbye [ 0.93248147  0.8733082  -0.8807387  -0.79100204  0.47581592]
</span></span><span class=line><span class=cl>and [-0.77025014 -0.7765116   0.7260802   0.86010003  1.9209603 ]
</span></span><span class=line><span class=cl>i [ 0.9177614   0.86838746 -0.8880995  -0.7853987   0.48234403]
</span></span><span class=line><span class=cl>hello [ 0.97636664  1.0043367  -1.0315654  -1.1388433   1.4567573 ]
</span></span><span class=line><span class=cl>. [-1.2583737 -1.2487313  1.2197953  1.20546   -1.6672388]
</span></span></code></pre></td></tr></table></div></div><p>이제 드디어 단어를 밀집벡터로 나타낼 수 있게 되었다. 이 밀집 벡터가 바로 단어의 분산 표현이다.</p><p>학습이 잘 이루어졌으니, 이 분산 표현은 단어의 실제 의미를 담고 있을 것이다.</p><p>여태 구현한 CBOW 모델은 처리 효율 면에서 문제가 있다. 이제 그걸 개선해 보자.</p><h2 id=word2vec-보충>word2vec 보충</h2><h3 id=cbow-모델과-확률>CBOW 모델과 확률</h3><p>사건 A가 일어날 확률은 $P(A)$와 같이 표기하고, A와 B가 동시에 일어날 확률은 $P(A, B)$와 같이 표기한다. 사건 B가 일어났을 때 사건 A가 일어날 확률은 $P(A|B)$와 같이 표기한다.</p><p>맥락으로 $w_{t-1}$과 $w_{t+1}$ 이 주어졌을 때 $w_t$가 일어날 확률은</p><p>$$
P(w_t | w_{t-1}, w_{t+1})
\tag 1
$$</p><p>식 1과 같이 쓸 수 있다. 이는 CBOW를 모델링 하는 식이다.</p><p>교차 엔트로피 오차 식은 $L = - \sum_k t_k \log y_k$이다. $y_k$는 $k$번째에 해당하는 사건이 일어날 확률을 의미하고, $t_k$는 정답 레이블로 원핫 벡터로 표현된다. 여기서 문제의 정답은 $w_i$가 발생하는 것이므로 $w_i$에 해당하는 원소만 1이고 나머지는 0이 된다. 이 점을 활용하여, 다음 식을 유도할 수 있다.</p><p>$$
L = - \log P(w_t \ | \ w_{t-1}, \ w_{t+1})
\tag 2
$$</p><p>식 2는 <strong>음의 로그 가능도</strong>라고 부른다. 이처럼 CBOW 모델의 손실 함수는 식 1의 확률에 $\log$를 취한 다음 마이너스를 붙인 것이다. 이는 샘플 데이터 하나에 대한 손실 함수이고, 이를 corpus 전체로 확장시키면 아래 식 3과 같다.</p><p>$$
L = - \frac{1}{T} \sum_{t=1}^T \log P(w_t \ | \ w_{t-1}, \ w_{t+1})
\tag 3
$$</p><p>CBOW 모델의 학습이 수행하는 일은 이 손실 함수의 값을 가능한 한 작게 만드는 것이다.</p><h3 id=skip-gram-모델>skip-gram 모델</h3><p>skip-gram은 CBOW에서 다루는 맥락과 타깃을 역전시킨 모델이다.</p><p><img src=/p/word2vec/image-8.png width=833 height=147 srcset="/p/word2vec/image-8_hu2916d9d77f1323c25fd727aaf35c33dd_75662_480x0_resize_box_3.png 480w, /p/word2vec/image-8_hu2916d9d77f1323c25fd727aaf35c33dd_75662_1024x0_resize_box_3.png 1024w" loading=lazy alt="CBOW 모델과 skip-gram 모델이 다루는 문제" class=gallery-image data-flex-grow=566 data-flex-basis=1360px></p><p>CBOW 모델은 맥락이 여러 개 있고, 그 여러 맥락으로부터 타깃을 추측한다. 반면에 skip-gmm 모델은 중앙의 타깃으로부터 주변의 맥락을 추측한다.</p><p><img src=/p/word2vec/image-9.png width=811 height=660 srcset="/p/word2vec/image-9_huab099de2643e368e2fc9c09000b2c279_217229_480x0_resize_box_3.png 480w, /p/word2vec/image-9_huab099de2643e368e2fc9c09000b2c279_217229_1024x0_resize_box_3.png 1024w" loading=lazy alt="skip-gram 모델의 신경망 구성 예" class=gallery-image data-flex-grow=122 data-flex-basis=294px></p><p>skip-gram 모델을 확률 표기로 나타내면 아래 식 4와 같다.</p><p>$$
P(w_{t-1}, \ w_{t+1} \ | \ w_t)
\tag 4
$$</p><p>조건부 독립이라고 가정하고 (맥락의 단어 사이에 관련성이 없다고 가정하고) 식 4를 아래 식 5와 같이 분해한다.</p><p>$$
P(w_{t-1}, \ w_{t+1} \ | \ w_t) = P(w_{t-1} \ | \ w_{t}) \ P(w_{t+1} \ | \ w_{t})
\tag 5
$$</p><p>위 식을 교차 엔트로피 오차에 적용하고, corpus 전체로 확장시키면 아래 식 6이 된다.</p><p>$$
L = - \frac{1}{T} \sum_{t=1}^T ( \log P(w_{t-1} \ | \ w_{t})+ P(w_{t+1} \ | \ w_{t}))
\tag 6
$$</p><p>skip-gram 모델은 맥락의 수만큼 추측하기 때문에 그 손실 함수는 각 맥락에서 구한 손실의 총합이어야 하는 반면, CBOW 모델은 타깃 하나의 손실을 구한다.</p><p>단어 분산 표현의 정밀도 면에서 skip-gram 모델의 결과가 더 좋은 경우가 많고, corpus가 클 수록 성능 면에서 skip-gram이 뛰어난 경향이 있다.</p><h3 id=통계-기반-vs-추론-기반>통계 기반 vs 추론 기반</h3><p>통계 기반 기법과 추론 기반 기법인 word2vec은 학습과 갱신 방식에서 차이를 보인다.</p><p>통계 기반은 새 단어 추가 시 처음부터 다시 계산해야 하지만, word2vec은 기존 가중치를 활용해 효율적으로 갱신할 수 있다. 단어의 유사성과 복잡한 패턴 인코딩에서도 word2vec이 더 복잡한 관계를 파악할 수 있으며, &lsquo;king - man + woman = queen&rsquo; 같은 유추 문제를 풀 수 있다.</p><p>그러나 실제 유사성 평가에서는 두 기법 간 우열을 가리기 어렵다. 추론 기반과 통계 기반은 서로 관련되어 있으며, 이를 바탕으로 추론 기반과 통계 기반을 융합한 GloVe 기법이 등장하여 두 방법의 장점을 결합했다.</p></section><footer class=article-footer><section class=article-tags><a href=/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D/>딥러닝</a>
<a href=/tags/python/>Python</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.css integrity="sha256-J+iAE0sgH8QSz9hpcDxXIftnj65JEZgNhGcgReTTK9s=" crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.js integrity="sha256-InsNdER1b2xUewP+pKCUJpkhiqwHgqiPXDlIk7GzBu4=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/contrib/auto-render.min.js integrity="sha256-y39Mpg7V3D4lhBX4x6O0bUqTV4pSrfgwEfGKfxkOdgI=" crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".article-content");if(e){let t=e.innerHTML;t=t.replace(/\\big[oO]/g,"\\mathcal{O}"),t=t.replace(/(\$\$)([\s\S]+?)(\$\$)/g,(e,t,n,s)=>{const o=n.replace(/<em>/g,"_").replace(/<\/em>/g,"_");return`<div markdown="katex">
${t}
${o.trim()}
${s}
</div>`}),e.innerHTML=t,renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],ignoredClasses:["gist"]})}})</script></article><aside class=related-content--wrapper><h2 class=section-title>관련 글</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/word2vec-2/><div class=article-image><img src=/../images/deep-learning-from-scratch.jpeg loading=lazy data-key=word2vec-2 data-hash=/../images/deep-learning-from-scratch.jpeg></div><div class=article-details><h2 class=article-title>Word2Vec의 최적화</h2></div></a></article><article class=has-image><a href=/p/word-distributed-representation/><div class=article-image><img src=/../images/deep-learning-from-scratch.jpeg loading=lazy data-key=word-distributed-representation data-hash=/../images/deep-learning-from-scratch.jpeg></div><div class=article-details><h2 class=article-title>단어의 분산 표현</h2></div></a></article><article class=has-image><a href=/p/neural-network-trainning/><div class=article-image><img src=/../images/deep-learning-from-scratch.jpeg loading=lazy data-key=neural-network-trainning data-hash=/../images/deep-learning-from-scratch.jpeg></div><div class=article-details><h2 class=article-title>신경망의 학습</h2></div></a></article><article class=has-image><a href=/p/neural-network-inference/><div class=article-image><img src=/../images/deep-learning-from-scratch.jpeg loading=lazy data-key=neural-network-inference data-hash=/../images/deep-learning-from-scratch.jpeg></div><div class=article-details><h2 class=article-title>신경망의 추론</h2></div></a></article><article class=has-image><a href=/p/rnn-and-lstm/><div class=article-image><img src=/../images/pytorch-transformer-nlp-computer-vision.png loading=lazy data-key=rnn-and-lstm data-hash=/../images/pytorch-transformer-nlp-computer-vision.png></div><div class=article-details><h2 class=article-title>순환 신경망(RNN)과 장단기 메모리(LSTM)</h2></div></a></article></div></div></aside><script src=https://giscus.app/client.js data-repo=gyeongminn/gyeongminn.github.io data-repo-id=R_kgDOKsf9nw data-category=Announcements data-category-id=DIC_kwDOKsf9n84Ca_6Y data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=light data-lang=ko crossorigin=anonymous async></script>
<script>function setGiscusTheme(e){let t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:{setConfig:{theme:e}}},"https://giscus.app")}(function(){addEventListener("message",t=>{if(event.origin!=="https://giscus.app")return;e()}),window.addEventListener("onColorSchemeChange",e);function e(){setGiscusTheme(document.documentElement.dataset.scheme==="light"?"light":"noborder_gray")}})()</script><footer class=site-footer><section class=copyright>&copy;
2021 -
2025 Gyeongmin Lee</section><section class=powerby><br></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://cdn.jsdelivr.net/gh/orioncactus/pretendard@v1.3.9/dist/web/static/pretendard.min.css",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>