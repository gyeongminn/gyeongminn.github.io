<!doctype html><html lang=ko dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Embedding 계층과 네거티브 샘플링 도입으로 인한 word2vec의 최적화와 CBOW 모델의 개선"><title>Word2Vec의 최적화</title><link rel=canonical href=https://gyeongmin.kr/p/word2vec-2/><link rel=stylesheet href=/scss/style.min.2325ab3f1c53340b74d88bd622b7f5bcc047e55d78831b42a905dd43de9993e0.css><meta property="og:title" content="Word2Vec의 최적화"><meta property="og:description" content="Embedding 계층과 네거티브 샘플링 도입으로 인한 word2vec의 최적화와 CBOW 모델의 개선"><meta property="og:url" content="https://gyeongmin.kr/p/word2vec-2/"><meta property="og:site_name" content="Gyeongmin의 개발 블로그"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="딥러닝"><meta property="article:tag" content="Python"><meta property="article:published_time" content="2024-01-03T00:00:00+00:00"><meta property="article:modified_time" content="2024-01-03T00:00:00+00:00"><meta property="og:image" content="https://gyeongmin.kr/images/deep-learning-from-scratch.jpeg"><meta name=twitter:title content="Word2Vec의 최적화"><meta name=twitter:description content="Embedding 계층과 네거티브 샘플링 도입으로 인한 word2vec의 최적화와 CBOW 모델의 개선"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gyeongmin.kr/images/deep-learning-from-scratch.jpeg"><link rel="shortcut icon" href=/logo.ico/favicon.ico><script async src="https://www.googletagmanager.com/gtag/js?id=G-RBG63ZJRZM"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-RBG63ZJRZM",{anonymize_ip:!1})}</script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2100464285092061" crossorigin=anonymous></script>
<link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color=#757575><meta name=msapplication-TileColor content="#da532c"><meta name=theme-color content="#000000"><link rel=icon href=/logo.ico/favicon.ico><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2100464285092061" crossorigin=anonymous></script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="메뉴 여닫기">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/profile_hu5d202280e416202dba72f1bb8473638c_1285338_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>Gyeongmin의 개발 블로그</a></h1><h2 class=site-description>Computer Vision Engineer</h2></div></header><ol class=social-menu><li><a href=https://github.com/gyeongminn target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="22" height="22" viewBox="1 -2 19 25" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path d="M0 0h22v22H0z" stroke="none"/><path d="M8.25 17.416c-3.942 1.284-3.942-2.292-5.5-2.75m11 4.584v-3.208c0-.916.092-1.284-.458-1.834 2.566-.274 5.042-1.284 5.042-5.5a4.216 4.216.0 00-1.192-2.934A3.85 3.85.0 0017.05 2.84s-1.008-.274-3.208 1.192a11.274 11.274.0 00-5.684.0C5.958 2.566 4.95 2.841 4.95 2.841a3.85 3.85.0 00-.092 2.934A4.216 4.216.0 003.666 8.708c0 4.216 2.474 5.226 5.042 5.5-.55.55-.55 1.1-.458 1.834v3.208"/></svg></a></li><li><a href=https://www.instagram.com/gyeongminx/ target=_blank title=instagram rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-instagram" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 4m0 4a4 4 0 014-4h8a4 4 0 014 4v8a4 4 0 01-4 4H8a4 4 0 01-4-4z"/><path d="M12 12m-3 0a3 3 0 106 0 3 3 0 10-6 0"/><path d="M16.5 7.5v.01"/></svg></a></li><li><a href=https://www.linkedin.com/in/gyeongmin-lee-865448256/ target=_blank title=Linkedin rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-linkedin" width="44" height="44" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 4m0 2a2 2 0 012-2h12a2 2 0 012 2v12a2 2 0 01-2 2H6a2 2 0 01-2-2z"/><path d="M8 11v5"/><path d="M8 8v.01"/><path d="M12 16v-5"/><path d="M16 16v-3a2 2 0 00-4 0"/></svg></a></li><li><a href=mailto:gyeongmin@hansung.ac.kr target=_blank title=mail rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-mail" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path d="M0 0h24v24H0z" stroke="none"/><path d="M3 7a2 2 0 012-2h14a2 2 0 012 2v10a2 2 0 01-2 2H5a2 2 0 01-2-2V7z"/><path d="m3 7 9 6 9-6"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg><span>About</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg><span>Archives</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>Search</span></a></li><li><a href=/links/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg><span>Links</span></a></li><div class=menu-bottom-section><li id=i18n-switch><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg><select name=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://gyeongmin.kr/ selected>한국어</option><option value=https://gyeongmin.kr/en/>English</option></select></li><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>다크 모드</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">목차</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#embedding-계층>Embedding 계층</a><ol><li><a href=#배경>배경</a></li><li><a href=#작동-원리>작동 원리</a></li><li><a href=#구현-예시>구현 예시</a></li></ol></li><li><a href=#네거티브-샘플링-기법>네거티브 샘플링 기법</a><ol><li><a href=#배경-1>배경</a></li><li><a href=#다중-분류-문제의-이진-분류-근사>다중 분류 문제의 이진 분류 근사</a></li><li><a href=#긍정적-예와-부정적-예의-선택>긍정적 예와 부정적 예의 선택</a></li><li><a href=#계산-효율성의-증가>계산 효율성의 증가</a></li><li><a href=#구현-예시-1>구현 예시</a></li></ol></li><li><a href=#cbow-모델-구현>CBOW 모델 구현</a></li><li><a href=#결론>결론</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/word2vec-2/><img src=/../images/deep-learning-from-scratch.jpeg loading=lazy alt="Featured image of post Word2Vec의 최적화"></a></div><div class=article-details><header class=article-category><a href=/categories/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-2/>밑바닥부터 시작하는 딥러닝 2</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/word2vec-2/>Word2Vec의 최적화</a></h2><h3 class=article-subtitle>Embedding 계층과 네거티브 샘플링 도입으로 인한 word2vec의 최적화와 CBOW 모델의 개선</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Jan 03, 2024</time></div></footer></div></header><section class=article-content><h1 id=word2vec의-최적화>Word2Vec의 최적화</h1><p><img src=/p/word2vec-2/image-3.png width=1569 height=1191 srcset="/p/word2vec-2/image-3_hu8834c8184300983a591c4f0045bd8abe_487521_480x0_resize_box_3.png 480w, /p/word2vec-2/image-3_hu8834c8184300983a591c4f0045bd8abe_487521_1024x0_resize_box_3.png 1024w" loading=lazy alt="Word2Vec의 구조" class=gallery-image data-flex-grow=131 data-flex-basis=316px></p><p>Word2Vec은 자연어 처리 분야에서 매우 중요한 도구로 자리 잡고 있다. 그러나 Word2Vec은 어휘의 양이 방대해질수록 계산량과 메모리 사용량이 커지는 문제를 가지고 있다. 이러한 문제를 해결하기 위하여 &lsquo;임베딩(Embedding)&rsquo; 계층을 도입하고 &lsquo;네거티브 샘플링(Negative Sampling)&rsquo; 기법을 적용하는 두 가지 주요 개선 방법을 살펴보자.</p><h2 id=embedding-계층>Embedding 계층</h2><p><img src=/p/word2vec-2/image-4.png width=1347 height=661 srcset="/p/word2vec-2/image-4_hu38ab5d6d8c1cfa13035a3656906f9354_254061_480x0_resize_box_3.png 480w, /p/word2vec-2/image-4_hu38ab5d6d8c1cfa13035a3656906f9354_254061_1024x0_resize_box_3.png 1024w" loading=lazy alt="Embedding Layer" class=gallery-image data-flex-grow=203 data-flex-basis=489px></p><h3 id=배경>배경</h3><p>전통적으로 단어를 표현하는 방법 중 하나는 원-핫 인코딩이다. 이 방법은 각 단어를 하나의 긴 벡터로 표현하며, 벡터의 크기는 어휘의 크기와 같다. 벡터에서 단어에 해당하는 위치는 1이고 나머지는 모두 0이다. 이 방식은 직관적이지만, 벡터가 대부분 0으로 채워지는 희소성 문제와 차원이 커질수록 계산 비효율성이 증가하는 문제를 가지고 있다.</p><p>예를 들어, 어휘 사전에 10,000개의 단어가 있다면, 각 단어는 10,000차원의 벡터로 표현된다. 이렇게 고차원 벡터와 가중치 행렬의 곱셈 계산은 많은 계산 자원을 소모한다.</p><p>임베딩 레이어는 각 단어를 고정된 크기의 밀집 벡터로 변환한다. 이 밀집 벡터는 단어의 의미를 수치적으로 포착할 수 있으며, 벡터의 각 요소는 연속된 값으로 이루어져 있다. 이로 인해 희소성 문제를 해결하고, 효율적인 계산이 가능해진다.</p><h3 id=작동-원리>작동 원리</h3><p>임베딩 레이어는 각 단어를 고유한 인덱스에 매핑하고, 이 인덱스를 사용하여 단어의 밀집 벡터를 찾는다. 이 밀집 벡터는 학습 가능한 파라미터로, 모델 학습 과정에서 최적화된다.</p><p>전통적인 원-핫 인코딩 방식은 단어의 인덱스에 해당하는 위치에만 1을 두고 나머지는 0으로 채워 계산을 수행한다. 이에 반해, 임베딩 레이어는 각 단어에 대한 밀집 벡터를 직접 참조하여 계산을 수행하기 때문에, 불필요한 계산을 크게 줄여준다.</p><h3 id=구현-예시>구현 예시</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Embedding</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>W</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>params</span> <span class=o>=</span> <span class=p>[</span><span class=n>W</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>grads</span> <span class=o>=</span> <span class=p>[</span><span class=n>np</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>W</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>idx</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>idx</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>W</span><span class=p>,</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>params</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>idx</span> <span class=o>=</span> <span class=n>idx</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=n>W</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>out</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>backward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dout</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>dW</span><span class=p>,</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>grads</span>
</span></span><span class=line><span class=cl>        <span class=n>dW</span><span class=p>[</span><span class=o>...</span><span class=p>]</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>word_id</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>idx</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>dW</span><span class=p>[</span><span class=n>word_id</span><span class=p>]</span> <span class=o>+=</span> <span class=n>dout</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=kc>None</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=네거티브-샘플링-기법>네거티브 샘플링 기법</h2><p><img src=/p/word2vec-2/image-5.png width=1559 height=772 srcset="/p/word2vec-2/image-5_huf6cb38d7ca8e765b1958c39d9d025593_249082_480x0_resize_box_3.png 480w, /p/word2vec-2/image-5_huf6cb38d7ca8e765b1958c39d9d025593_249082_1024x0_resize_box_3.png 1024w" loading=lazy alt="Negative Sampling" class=gallery-image data-flex-grow=201 data-flex-basis=484px></p><p><strong>네거티브 샘플링</strong>은 다중 분류 문제를 이진 분류로 근사하여 계산량을 대폭 줄이는 기법이다. 이 기법은 특히 대규모 어휘를 다루는 자연어 처리에서 중요한 역할을 한다.</p><h3 id=배경-1>배경</h3><p>Word2Vec과 같은 언어 모델은 단어의 의미를 벡터로 변환하여 수치화하는 과정을 수행한다. 이 과정에서 전체 어휘에 대한 예측을 수행하게 되면, 어휘의 크기가 커질수록 계산량이 매우 늘어나는 문제가 있다. 특히, Softmax 계층에서의 계산은 모든 어휘에 대해 수행되어야 하므로, 어휘 수에 비례하여 계산량이 급격히 증가한다. 네거티브 샘플링은 이러한 문제를 효과적으로 해결하는 방법으로 제안되었다.</p><h3 id=다중-분류-문제의-이진-분류-근사>다중 분류 문제의 이진 분류 근사</h3><p>네거티브 샘플링의 핵심 아이디어는 다중 분류 문제를 이진 분류 문제로 근사하는 것이다. 전체 어휘에 대한 예측 대신, 모델이 특정 단어를 정답으로 예측하는지 여부만을 판단하게 한다. 즉, &lsquo;이 단어가 맞는가? 아닌가?&lsquo;라는 간단한 질문에 답하는 형식으로 문제를 단순화한다.</p><h3 id=긍정적-예와-부정적-예의-선택>긍정적 예와 부정적 예의 선택</h3><ul><li><p>긍정적 예(Positive Samples): 모델이 맞추어야 하는 실제 단어. 예를 들어 문맥이 &ldquo;The cat sits on the&rdquo; 일 때, 실제 다음 단어인 &ldquo;mat&rdquo; 이 긍정적 예가 된다.</p></li><li><p>부정적 예(Negative Samples): 무작위로 선택된 단어들로, 모델이 이 단어들을 정답으로 선택하지 않도록 학습한다. 이들은 긍정적 예와 구분되어야 할 대상들이다.</p></li></ul><h3 id=계산-효율성의-증가>계산 효율성의 증가</h3><p>네거티브 샘플링을 사용하면 모델이 전체 어휘에 대한 Softmax 계산을 수행할 필요가 없어진다. 대신, 긍정적 예에 대해서는 확률을 높이고, 선택된 부정적 예에 대해서는 확률을 낮추는 방식으로 학습이 이루어진다. 이 과정은 계산량을 현저히 줄여주며, 특히 어휘의 크기가 큰 경우에 매우 효과적이다.</p><h3 id=구현-예시-1>구현 예시</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>class</span> <span class=nc>NegativeSamplingLoss</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>W</span><span class=p>,</span> <span class=n>corpus</span><span class=p>,</span> <span class=n>power</span><span class=o>=</span><span class=mf>0.75</span><span class=p>,</span> <span class=n>sample_size</span><span class=o>=</span><span class=mi>5</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>sample_size</span> <span class=o>=</span> <span class=n>sample_size</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>sampler</span> <span class=o>=</span> <span class=n>UnigramSampler</span><span class=p>(</span><span class=n>corpus</span><span class=p>,</span> <span class=n>power</span><span class=p>,</span> <span class=n>sample_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>loss_layers</span> <span class=o>=</span> <span class=p>[</span><span class=n>SigmoidWithLoss</span><span class=p>()</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>sample_size</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>embed_dot_layers</span> <span class=o>=</span> <span class=p>[</span><span class=n>EmbeddingDot</span><span class=p>(</span><span class=n>W</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>sample_size</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>params</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>grads</span> <span class=o>=</span><span class=p>[],</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>embed_dot_layers</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>params</span> <span class=o>+=</span> <span class=n>layer</span><span class=o>.</span><span class=n>params</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>grads</span> <span class=o>+=</span> <span class=n>layer</span><span class=o>.</span><span class=n>grads</span>
</span></span><span class=line><span class=cl>            
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>h</span><span class=p>,</span> <span class=n>target</span><span class=p>):</span> 
</span></span><span class=line><span class=cl>        <span class=n>batch_size</span> <span class=o>=</span> <span class=n>target</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>negative_sample</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>sampler</span><span class=o>.</span><span class=n>get</span> <span class=n>_negative_sample</span><span class=p>(</span><span class=n>target</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 긍정적 예 순전파</span>
</span></span><span class=line><span class=cl>        <span class=n>score</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embed_dot_layers</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>h</span><span class=p>,</span> <span class=n>target</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>correct_label</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>loss_layers</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>score</span><span class=p>,</span> <span class=n>correct_label</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 부정적 예 순전파</span>
</span></span><span class=line><span class=cl>        <span class=n>negative_label</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>sample</span><span class=o>.</span><span class=n>size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>negative_target</span> <span class=o>=</span> <span class=n>negative_sample</span><span class=p>[:,</span> <span class=n>i</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>score</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embed_dot_layers</span><span class=p>[</span><span class=mi>1</span> <span class=o>+</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>h</span><span class=p>,</span> <span class=n>negative_target</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>+=</span> <span class=bp>self</span><span class=o>.</span><span class=n>loss_layers</span><span class=p>[</span><span class=mi>1</span> <span class=o>+</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>score</span><span class=p>,</span> <span class=n>negative_label</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>loss</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>backward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dout</span><span class=o>=</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>dh</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>l0</span><span class=p>,</span> <span class=n>l1</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>loss_layers</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>embed_dot_layers</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>dscore</span> <span class=o>=</span> <span class=mf>10.</span><span class=n>backward</span><span class=p>(</span><span class=n>dout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>dh</span> <span class=o>+=</span> <span class=n>l1</span><span class=o>.</span><span class=n>backward</span><span class=p>(</span><span class=n>dscore</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>dh</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=cbow-모델-구현>CBOW 모델 구현</h2><p>이전 포스팅에서 구현했던 CBOW 모델에 Embedding과 Negative Sampling Loss 계층을 적용해 개선해 보자.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>class</span> <span class=nc>CBOW</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>window_size</span><span class=p>,</span> <span class=n>corpus</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>V</span><span class=p>,</span> <span class=n>H</span> <span class=o>=</span> <span class=n>vocab_size</span><span class=p>,</span> <span class=n>hidden_size</span>
</span></span><span class=line><span class=cl>        <span class=n>W_in</span> <span class=o>=</span> <span class=mf>0.01</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>V</span><span class=p>,</span> <span class=n>H</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=s1>&#39;f&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>W_out</span> <span class=o>=</span> <span class=mf>0.01</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>V</span><span class=p>,</span> <span class=n>H</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=s1>&#39;f&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>in_layers</span> <span class=o>=</span><span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>window_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>layer</span> <span class=o>=</span> <span class=n>Embedding</span><span class=p>(</span><span class=n>W_in</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>in_layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>layer</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ns_loss</span> <span class=o>=</span> <span class=n>NegativeSamplingLoss</span><span class=p>(</span><span class=n>W_out</span><span class=p>,</span> <span class=n>corpus</span><span class=p>,</span> <span class=n>power</span><span class=o>=</span><span class=mf>0.75</span><span class=p>,</span> <span class=n>sample_size</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>layers</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>in_layers</span> <span class=o>+</span> <span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>ns_loss</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>params</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>grads</span> <span class=o>=</span> <span class=p>[],</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=n>layers</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>params</span> <span class=o>+=</span> <span class=n>layer</span><span class=o>.</span><span class=n>params</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>grads</span> <span class=o>+=</span> <span class=n>layer</span><span class=o>.</span><span class=n>grads</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>word_vecs</span> <span class=o>=</span> <span class=n>W_in</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>contexts</span><span class=p>,</span> <span class=n>target</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>h</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>layer</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>in_layers</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>h</span> <span class=o>+=</span> <span class=n>layer</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>contexts</span><span class=p>[:,</span> <span class=n>i</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>h</span> <span class=o>*=</span> <span class=mi>1</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>in_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ns_loss</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>h</span><span class=p>,</span> <span class=n>target</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>loss</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>backward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dout</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span><span class=err>：</span>
</span></span><span class=line><span class=cl>        <span class=n>dout</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ns_loss</span><span class=o>.</span> <span class=n>backward</span><span class=p>(</span><span class=n>dout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>dout</span> <span class=o>*=</span> <span class=mi>1</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>in_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>in_layers</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>layer</span><span class=o>.</span><span class=n>backward</span><span class=p>(</span><span class=n>dout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=kc>None</span>
</span></span></code></pre></td></tr></table></div></div><p><img src=/p/word2vec-2/image.png width=771 height=224 srcset="/p/word2vec-2/image_hu97ada50fdfa2d7dea73986e8a482bada_60348_480x0_resize_box_3.png 480w, /p/word2vec-2/image_hu97ada50fdfa2d7dea73986e8a482bada_60348_1024x0_resize_box_3.png 1024w" loading=lazy alt="맥락과 타깃을 단어 ID로 나타낸 예시" class=gallery-image data-flex-grow=344 data-flex-basis=826px></p><p>단어 ID의 배열이 contexts와 target의 예이다. 맥락은 2차원 배열이고 타겟은 1차원 배열이고, 이러한 데이터가 순전파에에 입력되는 것이다.</p><h2 id=결론>결론</h2><p>임베딩 계층과 네거티브 샘플링 기법은 Word2Vec 모델의 계산량과 메모리 사용량을 현저하게 줄여주는 효과적인 방법이다. 특히, 대규모 어휘를 가진 말뭉치를 다루는 데 있어서 이러한 개선 방법은 필수적이다. 이를 통해 보다 빠르고 효율적으로 자연어 처리 모델을 학습이 가능하다.</p><p>Word2Vec은 자연어 처리에 중요한 역할을 하고 있다. 이것으로 단어들이 고정 길이의 벡터로 변환되며, 이렇게 변환된 단어 벡터는 비슷한 의미를 가진 단어들을 찾는 데 유용하게 사용된다. 또한, 단어의 분산 표현은 전이 학습에도 적용할 수 있는데, 이는 한 분야에서 획득한 지식을 다른 분야에 적용하는 것을 의미한다. 이를 통해 다양한 자연어 처리 문제에 효과적으로 접근할 수 있다.</p><p>자연어 처리 작업에서는 Word2Vec 모델이 주로 큰 말뭉치로 사전 학습된 상태에서 사용된다. 예를 들어, 텍스트 분류, 문서 클러스터링, 감정 분석 등의 작업에서 사전에 학습된 단어 벡터를 활용함으로써 작업의 성능을 향상시킬 수 있다. 이러한 방식은 자연어를 벡터로 변환함으로써 일반적인 머신러닝 기법(신경망, SVM 등등)을 자연어 처리 문제에 적용할 수 있게 해준다. 따라서 Word2Vec의 단어 분산 표현은 자연어 처리 분야에서 높은 정확도와 효율성을 제공하는 핵심적인 요소가 되고 있다.</p></section><footer class=article-footer><section class=article-tags><a href=/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D/>딥러닝</a>
<a href=/tags/python/>Python</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.css integrity="sha256-J+iAE0sgH8QSz9hpcDxXIftnj65JEZgNhGcgReTTK9s=" crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.js integrity="sha256-InsNdER1b2xUewP+pKCUJpkhiqwHgqiPXDlIk7GzBu4=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/contrib/auto-render.min.js integrity="sha256-y39Mpg7V3D4lhBX4x6O0bUqTV4pSrfgwEfGKfxkOdgI=" crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".article-content");if(e){let t=e.innerHTML;t=t.replace(/(\$\$)([\s\S]+?)(\$\$)/g,(e,t,n,s)=>{const o=n.replace(/<em>/g,"_").replace(/<\/em>/g,"_");return`<div markdown="katex">
${t}
${o.trim()}
${s}
</div>`}),e.innerHTML=t,renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],ignoredClasses:["gist"]})}})</script></article><aside class=related-content--wrapper><h2 class=section-title>관련 글</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/word2vec/><div class=article-image><img src=/../images/deep-learning-from-scratch.jpeg loading=lazy data-key=word2vec data-hash=/../images/deep-learning-from-scratch.jpeg></div><div class=article-details><h2 class=article-title>word2vec을 이용한 단어 임베딩</h2></div></a></article><article class=has-image><a href=/p/word-distributed-representation/><div class=article-image><img src=/../images/deep-learning-from-scratch.jpeg loading=lazy data-key=word-distributed-representation data-hash=/../images/deep-learning-from-scratch.jpeg></div><div class=article-details><h2 class=article-title>단어의 분산 표현</h2></div></a></article><article class=has-image><a href=/p/neural-network-trainning/><div class=article-image><img src=/../images/deep-learning-from-scratch.jpeg loading=lazy data-key=neural-network-trainning data-hash=/../images/deep-learning-from-scratch.jpeg></div><div class=article-details><h2 class=article-title>신경망의 학습</h2></div></a></article><article class=has-image><a href=/p/neural-network-inference/><div class=article-image><img src=/../images/deep-learning-from-scratch.jpeg loading=lazy data-key=neural-network-inference data-hash=/../images/deep-learning-from-scratch.jpeg></div><div class=article-details><h2 class=article-title>신경망의 추론</h2></div></a></article><article class=has-image><a href=/p/basic-linear-algebra/><div class=article-image><img src=/../images/deep-learning-from-scratch.jpeg loading=lazy data-key=basic-linear-algebra data-hash=/../images/deep-learning-from-scratch.jpeg></div><div class=article-details><h2 class=article-title>기초 선형대수 - 스칼라, 벡터, 행렬</h2></div></a></article></div></div></aside><script src=https://giscus.app/client.js data-repo=gyeongminn/gyeongminn.github.io data-repo-id=R_kgDOKsf9nw data-category=Announcements data-category-id=DIC_kwDOKsf9n84Ca_6Y data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=light data-lang=ko crossorigin=anonymous async></script>
<script>function setGiscusTheme(e){let t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:{setConfig:{theme:e}}},"https://giscus.app")}(function(){addEventListener("message",t=>{if(event.origin!=="https://giscus.app")return;e()}),window.addEventListener("onColorSchemeChange",e);function e(){setGiscusTheme(document.documentElement.dataset.scheme==="light"?"light":"noborder_gray")}})()</script><footer class=site-footer><section class=copyright>&copy;
2023 -
2025 Gyeongmin Lee</section><section class=powerby><br></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://cdn.jsdelivr.net/gh/orioncactus/pretendard@v1.3.9/dist/web/static/pretendard.min.css",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>