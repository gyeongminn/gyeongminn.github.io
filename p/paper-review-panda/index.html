<!doctype html><html lang=ko dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Anomaly Detection 분야에서 Catastrophic Collapse을 방지하는 방법론 'PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation' 논문 리뷰"><title>[논문리뷰] PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation</title><link rel=canonical href=https://gyeongmin.kr/p/paper-review-panda/><link rel=stylesheet href=/scss/style.min.2325ab3f1c53340b74d88bd622b7f5bcc047e55d78831b42a905dd43de9993e0.css><meta property="og:title" content="[논문리뷰] PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation"><meta property="og:description" content="Anomaly Detection 분야에서 Catastrophic Collapse을 방지하는 방법론 'PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation' 논문 리뷰"><meta property="og:url" content="https://gyeongmin.kr/p/paper-review-panda/"><meta property="og:site_name" content="Gyeongmin의 개발 블로그"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="딥러닝"><meta property="article:published_time" content="2023-12-27T00:00:00+00:00"><meta property="article:modified_time" content="2023-12-27T00:00:00+00:00"><meta property="og:image" content="https://gyeongmin.kr/images/paper-review.png"><meta name=twitter:title content="[논문리뷰] PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation"><meta name=twitter:description content="Anomaly Detection 분야에서 Catastrophic Collapse을 방지하는 방법론 'PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation' 논문 리뷰"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gyeongmin.kr/images/paper-review.png"><link rel="shortcut icon" href=/logo.ico/favicon.ico><script async src="https://www.googletagmanager.com/gtag/js?id=G-RBG63ZJRZM"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-RBG63ZJRZM",{anonymize_ip:!1})}</script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2100464285092061" crossorigin=anonymous></script>
<link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color=#757575><meta name=msapplication-TileColor content="#da532c"><meta name=theme-color content="#000000"><link rel=icon href=/logo.ico/favicon.ico><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2100464285092061" crossorigin=anonymous></script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="메뉴 여닫기">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/profile_hu5d202280e416202dba72f1bb8473638c_1285338_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>Gyeongmin의 개발 블로그</a></h1><h2 class=site-description>Computer Vision Engineer</h2></div></header><ol class=social-menu><li><a href=https://github.com/gyeongminn target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="22" height="22" viewBox="1 -2 19 25" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path d="M0 0h22v22H0z" stroke="none"/><path d="M8.25 17.416c-3.942 1.284-3.942-2.292-5.5-2.75m11 4.584v-3.208c0-.916.092-1.284-.458-1.834 2.566-.274 5.042-1.284 5.042-5.5a4.216 4.216.0 00-1.192-2.934A3.85 3.85.0 0017.05 2.84s-1.008-.274-3.208 1.192a11.274 11.274.0 00-5.684.0C5.958 2.566 4.95 2.841 4.95 2.841a3.85 3.85.0 00-.092 2.934A4.216 4.216.0 003.666 8.708c0 4.216 2.474 5.226 5.042 5.5-.55.55-.55 1.1-.458 1.834v3.208"/></svg></a></li><li><a href=https://www.instagram.com/gyeongminx/ target=_blank title=instagram rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-instagram" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 4m0 4a4 4 0 014-4h8a4 4 0 014 4v8a4 4 0 01-4 4H8a4 4 0 01-4-4z"/><path d="M12 12m-3 0a3 3 0 106 0 3 3 0 10-6 0"/><path d="M16.5 7.5v.01"/></svg></a></li><li><a href=https://www.linkedin.com/in/gyeongmin-lee-865448256/ target=_blank title=Linkedin rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-linkedin" width="44" height="44" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 4m0 2a2 2 0 012-2h12a2 2 0 012 2v12a2 2 0 01-2 2H6a2 2 0 01-2-2z"/><path d="M8 11v5"/><path d="M8 8v.01"/><path d="M12 16v-5"/><path d="M16 16v-3a2 2 0 00-4 0"/></svg></a></li><li><a href=mailto:gyeongmin@hansung.ac.kr target=_blank title=mail rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-mail" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path d="M0 0h24v24H0z" stroke="none"/><path d="M3 7a2 2 0 012-2h14a2 2 0 012 2v10a2 2 0 01-2 2H5a2 2 0 01-2-2V7z"/><path d="m3 7 9 6 9-6"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg><span>About</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg><span>Archives</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>Search</span></a></li><li><a href=/links/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg><span>Links</span></a></li><div class=menu-bottom-section><li id=i18n-switch><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg><select name=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://gyeongmin.kr/ selected>한국어</option><option value=https://gyeongmin.kr/en/>English</option></select></li><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>다크 모드</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">목차</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#introduction>Introduction</a><ol><li><a href=#anomaly-detection>Anomaly Detection</a></li><li><a href=#catastrophic-collapse>Catastrophic Collapse</a></li></ol></li><li><a href=#background>Background</a><ol><li><a href=#3-stage-framework>3-Stage Framework</a></li><li><a href=#deep-nearest-neighbours-dn2>Deep Nearest Neighbours (DN2)</a></li><li><a href=#semantic-pyramid-anomaly-detection-spade>Semantic Pyramid Anomaly Detection (SPADE)</a></li><li><a href=#deep-support-vector-data-description-deepsvdd>Deep Support Vector Data Description (DeepSVDD)</a></li><li><a href=#joint-optimization-jo>Joint Optimization (JO)</a></li></ol></li><li><a href=#method>Method</a><ol><li><a href=#simple-early-stopping-panda-early>Simple Early Stopping (PANDA-Early)</a></li><li><a href=#sample-wise-early-stopping-panda-ses>Sample-Wise Early Stopping (PANDA-SES)</a></li><li><a href=#elastic-regularization-panda-ewc>Elastic Regularization (PANDA-EWC)</a></li><li><a href=#anomaly-scoring>Anomaly Scoring</a></li><li><a href=#outlier-exposure>Outlier Exposure</a></li></ol></li><li><a href=#expreiments>Expreiments</a><ol><li><a href=#dataset>Dataset</a></li></ol></li><li><a href=#results>Results</a></li><li><a href=#conclusion>Conclusion</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/paper-review-panda/><img src=/../images/paper-review.png loading=lazy alt="Featured image of post [논문리뷰] PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation"></a></div><div class=article-details><header class=article-category><a href=/categories/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/>논문리뷰</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/paper-review-panda/>[논문리뷰] PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation</a></h2><h3 class=article-subtitle>Anomaly Detection 분야에서 Catastrophic Collapse을 방지하는 방법론 'PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation' 논문 리뷰</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Dec 27, 2023</time></div></footer></div></header><section class=article-content><h1 id=panda-adapting-pretrained-features-for-anomaly-detection-and-segmentation>PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation</h1><blockquote><p><a class=link href=https://openaccess.thecvf.com/content/CVPR2021/papers/Reiss_PANDA_Adapting_Pretrained_Features_for_Anomaly_Detection_and_Segmentation_CVPR_2021_paper.pdf target=_blank rel=noopener>논문 PDF</a></p></blockquote><h2 id=introduction>Introduction</h2><h3 id=anomaly-detection>Anomaly Detection</h3><p><strong>Anomaly</strong>는 정상의 범주에서 벗어나 있는 모든 것들을 의미한다.</p><p>주어진 데이터셋에서 Anomaly들을 탐지하는 것을 <strong>Anomaly Detection</strong> (이상치 탐지)라 한다. 주로 정상 데이터셋만으로 학습을 진행하며, 주어진 이미지를 Normal / Anomaly로 구분해야 하기에 One-Class Classification (OCC) 라고도 부른다.</p><p>본 논문에서는 Anomaly를 검출하는 세 분야를 다음과 같이 정의한다.</p><ul><li><p><strong>Anomaly Detection</strong>: 훈련 과정에서 오직 정상 이미지만 사용하는 경우</p></li><li><p><strong>Anomaly Segmentation</strong>: 정상 이미지가 주어진 상태에서 Anomaly가 포함된 모든 픽셀을 탐지하는 경우</p></li><li><p><strong>Outlier Exposure</strong>: 이상 현상을 시뮬레이션하는 데에 외부 데이터셋 사용이 가능한 경우</p></li></ul><p>최근 대부분의 이상 탐지 방법들은 제한된 정상 훈련 데이터셋을 사용하여 데이터의 feature를 학습하며, 이것에 의존하고 있다.</p><h3 id=catastrophic-collapse>Catastrophic Collapse</h3><p>특히 Multi-Class Calssifiaction보다 One-Class Classification 분야에서 전이 학습에 대한 연구가 부족한 상황이다. 이미지 전이 학습 분야에서는 <strong>Catastrophic Collapse</strong>가 잘 발생하기 때문이다. Catastrophic Collapse는 Normal과 Anomaly 데이터가 잘 구분되지 않고, feature 공간에서 동일한 지점에 매핑되는 현상을 말한다. 이 현상은 데이터의 패턴을 학습하지 못하거나 학습 과정에서 Overfitting에 빠지는 것에 기인하기 때문에, Early Stopping을 위해 Epoch를 잘 조절한다면 이를 방지할 수 있다.</p><p>본 논문에서는 Catastrophic Collapse를 방지하기 위해 Epoch 수에 의존적이지 않은 <strong>early stopping variant</strong> 방법과 continual learning에서 영감을 받은 <strong>Elastic Regularization</strong> 기법을 제안하는데, 이는 뒤에서 자세히 설명하겠다.</p><h2 id=background>Background</h2><h3 id=3-stage-framework>3-Stage Framework</h3><p>일반적인 프레임워크는 세 단계로 구성된다. 정상 훈련 데이터셋 $D_{train} = \{ x_1, x_2, \dots, x_N \} $이 주어졌다고 가정하자.</p><ol><li><p><strong>Initial feature extractor</strong></p><p>Initial feature extractor인 $\psi_0$는 pre-train을 통해 얻을 수 있으며, 손실 함수는 $L_{pretrain}$이다. auxiliary task (보조 작업)은 외부 데이터셋을 통한 pre-train이나 self-supervised learning일 수 있다.</p></li><li><p><strong>Feature Adaptation</strong></p><p>auxiliary task나 데이터셋을 통해 학습된 feature는 anomaly score를 매기기 전에 적응이 필요할 수 있다. 이는 훈련 데이터셋을 통한 fine-tuning으로, 적응된 feature extractor는 $\psi$로 표기한다.</p></li><li><p><strong>Anomaly Scoring</strong></p><p>Feature를 적응시킨 후, 훈련 데이터셋 샘플들의 특성 $\psi(x_1), \psi(x_2), \dots, \psi(x_N)$ 을 추출한다. 그 다음, Anomaly score를 구하는 함수를 학습하는 과정을 진행한다. 일반적으로 scoring function은 테스트 샘플 $\psi(x)$ 주변의 정상 데이터 밀도를 측정하며 밀도가 낮은 지역에 높은 anomaly score를 할당한다.</p></li></ol><h3 id=deep-nearest-neighbours-dn2>Deep Nearest Neighbours (DN2)</h3><p><a class=link href=https://arxiv.org/pdf/2002.10445.pdf target=_blank rel=noopener>&ldquo;Deep Nearest Neighbor Anomaly Detection&rdquo;</a>에서 제안된 방법으로, <strong>DN2</strong>에서는 ImageNet 데이터셋에서 pre-trained된 <strong>ResNet</strong>을 사용해 feature를 추출하고, <strong>kNN</strong>을 적용해 Normal과 Anomaly를 구분한다. (Normal과 Anomaly 이미지 간의 평균 거리를 Anomaly Score로 사용한다)</p><h3 id=semantic-pyramid-anomaly-detection-spade>Semantic Pyramid Anomaly Detection (SPADE)</h3><p><a class=link href=https://arxiv.org/pdf/2005.02357.pdf target=_blank rel=noopener>&ldquo;Sub-Image Anomaly Detection with Deep Pyramid Correspondences&rdquo;</a>에서 제안된 방법으로, ImageNet으로 사전 훈련된 ResNet을 사용하여 모든 이미지에 대한 픽셀별 특성을 추출한다. 또한 Feature Pyramid를 이용하여 다양한 수준의 feature를 동시에 추출하고 concat해 사용한다. DN2와는 달리 SPADE에서는 Anomaly Segmentation도 가능하다.</p><h3 id=deep-support-vector-data-description-deepsvdd>Deep Support Vector Data Description (DeepSVDD)</h3><p><a class=link href=https://proceedings.mlr.press/v80/ruff18a/ruff18a.pdf target=_blank rel=noopener>&ldquo;Deep One-Class Classification&rdquo;</a>에서 제안된 방법으로, 데이터의 정상적인 패턴을 학습하여 정상 범주에서 크게 벗어난 데이터 포인트를 Anomaly로 식별하는 것이다. CNN을 사용해 이미지 데이터를 저차원의 feature 공간으로 매핑하고, 데이터 포인트가 중심에서 얼마나 떨어져 있는지를 측정하여 Anomaly 여부를 판단한다.</p><h3 id=joint-optimization-jo>Joint Optimization (JO)</h3><p><a class=link href=https://arxiv.org/pdf/1801.05365.pdf target=_blank rel=noopener>&ldquo;Learning Deep Features for One-Class Classification&rdquo;</a>에서 제안된 방법으로, ImageNet 데이터셋에서 객체 분류를 위해 Pretrained된 Feature Extractor를 사용한다. 오분류에 대한 패널티가 없는 경우 trivial한 솔루션을 학습할 수 있기에, compactness loss와 classification loss를 동시에 최적화한다. 이 방법은 메모리를 많이 요구하며, 두 작업을 함께 훈련하는 경우 Anomaly Detection 작업의 정확도가 떨어질 수 있다는 한계점을 가진다.</p><h2 id=method>Method</h2><p>SVDD와 JO와 유사하게 compactness loss를 사용하여 pre-trained된 feature의 분포를 anomaly detection 작업에 적응시킨다. 하지만 구조를 제한하거나 외부 데이터를 사용하지 않고, 직접적으로 Catastrophic Collapse를 다룬다.</p><p>compactness loss의 최적 솔루션이 Collapse로 이어질 수 있다. Collapse가 일어나면 모든 입력을 같은 지점으로 매핑하게 되어, 더이상 구분이 불가능해진다.</p><p>본 논문에서는 Catastrophic Collapse를 방지하기 위해 아래 3가지 방법을 제시하였다.</p><h3 id=simple-early-stopping-panda-early>Simple Early Stopping (PANDA-Early)</h3><p>collapse가 일어나기 전에, <strong>특정 Epoch마다 Early Stopping</strong>을 진행하는 방법이다. 가장 단순하며 강력한 방법이지만, Hyperparameter의 설정이 필요하다는 단점이 있다. 예를 들어, 15 Epoch마다 early stopping을 진행하는 것이다.</p><h3 id=sample-wise-early-stopping-panda-ses>Sample-Wise Early Stopping (PANDA-SES)</h3><p><strong>sample 단위로 early stopping</strong>여부를 결정하는 방법이다. Anomaly 샘플과 중심 간의 거리가 멀고, 정상 샘플과 중심 간의 거리가 짧을 때 anomaly detection의 정확도가 상관관계가 있다. 이를 위해 훈련 과정 중 특정 Epoch마다 (예를 들어 5 Epoch 마다) 평균 거리를 저장하고, 정규화를 거치고 평균 거리의 최대 비율을 anomaly detection score로 사용한다.</p><h3 id=elastic-regularization-panda-ewc>Elastic Regularization (PANDA-EWC)</h3><p>Continual Learning은 adaptive regularization을 이용하는 방법이다.</p><p>continual learning에서 영감을 받아 Elastic Weight Consolidation(EWC)을 사용한 방법이다. Continual Learning이란 이전에 학습한 것을 까먹지 않고 새로운 것을 학습하는 것이다. 여기서 보조 작업에 대해 사전 훈련을 진행하기 위해 100개의 미니배치를 사용한다. 이 과정에서 신경망의 모든 가중치 파라미터에 대한 Fisher 정보 행렬 $F$의 대각선을 계산한다. 이는 사전 훈련 단계가 끝난 후 한 번만 수행한다. 각 가중치 파라미터의 Fisher 행렬 값은 사전 훈련 데이터셋을 통해 주어진 식으로 계산된다.</p><p>연구팀은 Fisher 정보 행렬의 대각선 요소를 사용하여 네트워크의 각 가중치가 사전 훈련된 상태($\psi_0$)에서 미세 조정된 상태($\psi^*$)로 변화하는 거리의 제곱을 가중치로 삼는다. 이는 가중치 함수의 손실 풍경 곡률을 측정하는 방법으로 볼 수 있으며, 값이 크면 곡률이 높고 가중치가 비탄력적임을 의미한다.</p><p>이러한 정규화 방법은 $\lambda$라는 하이퍼파라미터로 가중치가 부여된 compactness loss와 함께 사용된다. 본 연구에서는 $\lambda = 10^4$를 사용했다. 이를 통해 최종적인 손실 함수를 정의하고, 이는 사전 훈련된 가중치와 미세 조정된 가중치 간의 차이를 기반으로 한다. 이 접근법은 네트워크가 새로운 작업을 학습하면서도 이전에 학습한 작업에 대한 정보를 유지할 수 있도록 돕는다.</p><h3 id=anomaly-scoring>Anomaly Scoring</h3><p>전통적인 Anomaly Detection와 같이, Anomaly Score는 밀도 추정을 통해 구할 수 있다. 따라서 본 연구에서도 kNN을 사용해 구현한다.</p><h3 id=outlier-exposure>Outlier Exposure</h3><p>Outlier Exposure는 이미지 이상 탐지 작업을 확장한 것으로, Normal 데이터보다 Anomaly에 더 유사한 Auxiliary 데이터셋 $D_{OE}$가 있다고 가정하고, 데이터셋을 Normal과 Abnormal로 분류한다. 이는 Linear Classification Layer $w$와 Feature $\psi$를 이용해 Logistic Regression Loss를 계산하는 방식으로 이루어진다.</p><h2 id=expreiments>Expreiments</h2><h3 id=dataset>Dataset</h3><p><img src=/p/paper-review-panda/image-1.png width=828 height=383 srcset="/p/paper-review-panda/image-1_hu8fe9d0bb7d2841f520322010f3770529_538886_480x0_resize_box_3.png 480w, /p/paper-review-panda/image-1_hu8fe9d0bb7d2841f520322010f3770529_538886_1024x0_resize_box_3.png 1024w" loading=lazy alt="왼쪽부터 시계 방향으로 CIFAR10, CIFAR100, Fashion MNIST, DogsVsCats, WBC, DIOR, Oxford Flowers, MVTec" class=gallery-image data-flex-grow=216 data-flex-basis=518px></p><div class=table-wrapper><table><thead><tr><th>Dataset</th><th>$N_{classes}$</th><th>$N_{train}$</th><th>$N_{test}$</th></tr></thead><tbody><tr><td>CIFAR10</td><td>10</td><td>5,000</td><td>10,000</td></tr><tr><td>Fashion MNIST</td><td>10</td><td>6,000</td><td>10,000</td></tr><tr><td>CIFAR100</td><td>20</td><td>2,500</td><td>10,000</td></tr><tr><td>Flowers</td><td>102</td><td>10</td><td>7,169</td></tr><tr><td>Birds</td><td>200</td><td>30</td><td>5,794</td></tr><tr><td>CatsVsDogs</td><td>2</td><td>10,000</td><td>5,000</td></tr><tr><td>MVTec</td><td>15</td><td>242</td><td>1,725</td></tr><tr><td>WBC</td><td>4</td><td>59</td><td>62</td></tr><tr><td>DIOR</td><td>19</td><td>649</td><td>9,243</td></tr></tbody></table></div><p>여기서 MVTec을 제외한 데이터셋은 정상 데이터를 한 개의 클래스로 구성하고, 나머지 클래스는 비정상으로 구성한다. 예를 들어, 고양이 이미지는 정상이고 강아지나 토끼 등 다른 이미지는 전부 비정상으로 구성된다.</p><p>MVTec 데이터셋은 Anomaly Detection을 위해 제작된 데이터셋으로, 클래스마다 정상과 비정상 이미지가 라벨링되어 있다.</p><h2 id=results>Results</h2><p><img src=/p/paper-review-panda/image-2.png width=1664 height=435 srcset="/p/paper-review-panda/image-2_huf70542624251d7a6a11badc98e4b0885_133792_480x0_resize_box_3.png 480w, /p/paper-review-panda/image-2_huf70542624251d7a6a11badc98e4b0885_133792_1024x0_resize_box_3.png 1024w" loading=lazy alt="Anomaly detection performance (Average ROC AUC %)" class=gallery-image data-flex-grow=382 data-flex-basis=918px></p><p><img src=/p/paper-review-panda/image-3.png width=1042 height=385 srcset="/p/paper-review-panda/image-3_huc19b42b9821d23f121b64dda0f10e1cd_84052_480x0_resize_box_3.png 480w, /p/paper-review-panda/image-3_huc19b42b9821d23f121b64dda0f10e1cd_84052_1024x0_resize_box_3.png 1024w" loading=lazy alt="Pretrained feature performance on various small datasets (Average ROC AUC %)" class=gallery-image data-flex-grow=270 data-flex-basis=649px></p><p>위 두 결과를 보았을 때, Self-Supervised 방법보다 DN2와 PANDA 방법이 훨씬 높은 성능을 보이는 것을 확인할 수 있다.</p><p><img src=/p/paper-review-panda/image-4.png width=1085 height=111 srcset="/p/paper-review-panda/image-4_hu4ce90baf9558792fb2b91e9a4c4c6d70_30823_480x0_resize_box_3.png 480w, /p/paper-review-panda/image-4_hu4ce90baf9558792fb2b91e9a4c4c6d70_30823_1024x0_resize_box_3.png 1024w" loading=lazy alt="Comparison of anomaly segmentation methods (pixel-level ROCAUC and PRO %)" class=gallery-image data-flex-grow=977 data-flex-basis=2345px></p><p>기존의 Anomaly Segmentation 방법들보다 SPADE 방법이 매우 높은 성능을 보이는 것을 확인할 수 있다.</p><p><img src=/p/paper-review-panda/image-5.png width=492 height=228 srcset="/p/paper-review-panda/image-5_hud409dcc8912bd9d9524595ad3a9d5b84_40928_480x0_resize_box_3.png 480w, /p/paper-review-panda/image-5_hud409dcc8912bd9d9524595ad3a9d5b84_40928_1024x0_resize_box_3.png 1024w" loading=lazy alt="A comparison of different feature adaptation methods (Avg. ROC AUC %)" class=gallery-image data-flex-grow=215 data-flex-basis=517px></p><p>위 표를 보면, 모든 데이터셋에서 PANDA 방법들이 JO 방법보다 높은 성능을 보이는 것을 확인할 수 있다.</p><h2 id=conclusion>Conclusion</h2><p>본 논문에서는 Anomaly Detection과 Anomaly Aegmentation을 위한 간단한 baseline을 제안하였으며, 이 방법은 현재 SOTA 방법들을 능가하며, 한계점을 해결했다. 또한 pre-trained된 Feature을 적응시키고 Catastrophic Collapse를 완화하는 방법을 제안하였다.</p><p>하지만 이 연구의 주요 한계점은 pre-trained된 강력한 Feature Extractor가 필요하다는 것이다.</p></section><footer class=article-footer><section class=article-tags><a href=/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D/>딥러닝</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.css integrity="sha256-J+iAE0sgH8QSz9hpcDxXIftnj65JEZgNhGcgReTTK9s=" crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.js integrity="sha256-InsNdER1b2xUewP+pKCUJpkhiqwHgqiPXDlIk7GzBu4=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/contrib/auto-render.min.js integrity="sha256-y39Mpg7V3D4lhBX4x6O0bUqTV4pSrfgwEfGKfxkOdgI=" crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".article-content");if(e){let t=e.innerHTML;t=t.replace(/\\big[oO]/g,"\\mathcal{O}"),t=t.replace(/(\$\$)([\s\S]+?)(\$\$)/g,(e,t,n,s)=>{const o=n.replace(/<em>/g,"_").replace(/<\/em>/g,"_");return`<div markdown="katex">
${t}
${o.trim()}
${s}
</div>`}),e.innerHTML=t,renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],ignoredClasses:["gist"]})}})</script></article><aside class=related-content--wrapper><h2 class=section-title>관련 글</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/paper-review-visual-attention-ad/><div class=article-image><img src=/../images/paper-review.png loading=lazy data-key=paper-review-visual-attention-ad data-hash=/../images/paper-review.png></div><div class=article-details><h2 class=article-title>[논문리뷰] Visual Attention and Background Subtraction With Adaptive Weight for Hyperspectral Anomaly Detection</h2></div></a></article><article class=has-image><a href=/p/paper-review-hyperspectral-ad/><div class=article-image><img src=/../images/paper-review.png loading=lazy data-key=paper-review-hyperspectral-ad data-hash=/../images/paper-review.png></div><div class=article-details><h2 class=article-title>[논문리뷰] Hyperspectral Anomaly Detection via Background and Potential Anomaly Dictionaries Construction</h2></div></a></article></div></div></aside><script src=https://giscus.app/client.js data-repo=gyeongminn/gyeongminn.github.io data-repo-id=R_kgDOKsf9nw data-category=Announcements data-category-id=DIC_kwDOKsf9n84Ca_6Y data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=light data-lang=ko crossorigin=anonymous async></script>
<script>function setGiscusTheme(e){let t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:{setConfig:{theme:e}}},"https://giscus.app")}(function(){addEventListener("message",t=>{if(event.origin!=="https://giscus.app")return;e()}),window.addEventListener("onColorSchemeChange",e);function e(){setGiscusTheme(document.documentElement.dataset.scheme==="light"?"light":"noborder_gray")}})()</script><footer class=site-footer><section class=copyright>&copy;
2023 -
2025 Gyeongmin Lee</section><section class=powerby><br></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://cdn.jsdelivr.net/gh/orioncactus/pretendard@v1.3.9/dist/web/static/pretendard.min.css",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>